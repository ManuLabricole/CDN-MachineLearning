{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:00:13.824779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources** :\n",
    "\n",
    "*Matrix Factorization techniques for Recommender Systems*, Koren (2009)    \n",
    "https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf\n",
    "\n",
    "Hands on Machine Learning with scikit-learn and tensorflow:             \n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "The movieLens dataset:                                                \n",
    "https://grouplens.org/datasets/movielens/ \n",
    "\n",
    "Keras Functional API doc :                                            \n",
    "https://keras.io/guides/functional_api/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems : collaborative filtering via matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wonder how Netflix is able to recommend you movies despite it doesn't know anything about you but the ratings you gave to the movies you watched ? This is what we are going to explore during this 3 days machine learning module.\n",
    "\n",
    "First off, let's learn about what are recommender system, collaborative filtering and matrix factorization techniques, which are all very well introduced in Koren's 2009 famous article : *Matrix Factorization techniques for Recommender Systems* : https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf . Read the 4 first pages (up to section *adding biases* included). \n",
    "\n",
    "Through this notebook we are going to re-implement the model described in the pages you read, and apply it to a classic movie ratings dataset coming from the website *movieLens*. To do so, we will use a powerful deep learning python library called *Keras*, that makes it easy to train complex models based on linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we are going to use the movieLens dataset, that contains data from the movie recommending website movielens. The data is a subset of ratings from 0 to 5 given by some users of the website to a subset of movies. You can read more about it here (we are using the latest small dataset) : https://grouplens.org/datasets/movielens/ , and in the *README* file that is in the *data/ml-latest-small/* folder.\n",
    "\n",
    "Load the ratings data from the `ratings.csv` file into a dataframe. The userId and movieId provided in the file don't start from 0, and are not contiguous (i.e. there are missing indexes).\n",
    "\n",
    "Re-index the user and movie ids to indexes going from 0 to `nb_users` and 0 to `nb_movies` respectively, by building two dictionnaries `user_ids_map` and `movie_ids_map` that maps the file ids to your new ids. \n",
    "And finally, split the rows of this dataframe in a random 90%/10% train/test sets.\n",
    "\n",
    "To do so, fill the `get_train_test_sets` function below, and respect the returned objects structures that are described in the docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd :  /Users/manulabricole/Documents/CDN/MachineLearning/ML4/notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "path = \"/Users/manulabricole/Documents/CDN/MachineLearning/ML4/data/ml-latest-small/ratings.csv\"\n",
    "print(\"cwd : \",cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1              3\n",
       "2              6\n",
       "3             47\n",
       "4             50\n",
       "           ...  \n",
       "100831    166534\n",
       "100832    168248\n",
       "100833    168250\n",
       "100834    168252\n",
       "100835    170875\n",
       "Name: movieId, Length: 100836, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"movieId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieId_unique = data[\"movieId\"].unique()\n",
    "movie_ids_map = {movie_id: index for index, movie_id in enumerate(movieId_unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId  movieId  rating  timestamp\n",
       "False   False    False   False        100836\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(data_path, train_prop = 0.9, rdmS=42):\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    userId_unique = data[\"userId\"].unique()\n",
    "    movieId_unique = data[\"movieId\"].unique()\n",
    "    \n",
    "    user_ids_map = {user_id: index for index, user_id in enumerate(userId_unique)}\n",
    "    movie_ids_map = {movie_id: index for index, movie_id in enumerate(movieId_unique)}\n",
    "    \n",
    "    data[\"userId\"] = data[\"userId\"].map(user_ids_map)\n",
    "    data[\"movieId\"] = data[\"movieId\"].map(movie_ids_map)\n",
    "    \n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    train, test = train_test_split(data, train_size=train_prop, random_state=rdmS)\n",
    "    \n",
    "    nb_users = len(userId_unique)\n",
    "    nb_movies = len(movieId_unique)\n",
    "    \n",
    "    return train, test, nb_users, nb_movies, user_ids_map, movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9724 movies, 610 users, and 100836 ratings\n"
     ]
    }
   ],
   "source": [
    "ratings_s_path =  '../data/ml-latest-small/ratings.csv'\n",
    "train, test, nb_users, nb_movies, user_ids_map, movie_ids_map = get_train_test_sets(ratings_s_path)\n",
    "dataset = pd.concat((train,test), axis = 0)\n",
    "\n",
    "print(\"There are %i movies, %i users, and %i ratings\" % (nb_movies, nb_users, dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [train[\"userId\"].to_numpy(), train[\"movieId\"].to_numpy()]\n",
    "y_train = train[\"rating\"].to_numpy()\n",
    "\n",
    "X_test = [test[\"userId\"].to_numpy(), test[\"movieId\"].to_numpy()]\n",
    "y_test = test[\"rating\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the ratings distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are a lot of missing values, because not all the user/movie pairs have an associated rating. Indeed, each user rates only a few movies ! The goal of this notebook is to predict (some of) the missing user/movie ratings.\n",
    "\n",
    "Print how many movies each of the 5 first users have rated, and print the percentage of available ratings in the whole dataset (i.e. the ratio between number of ratings and all the possible users/movies combinations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"rating\"].isna().value_counts().values[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--> There are no nan in rating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Id : 1 --> 29 movies rated\n",
      "User Id : 2 --> 39 movies rated\n",
      "User Id : 3 --> 216 movies rated\n",
      "User Id : 4 --> 44 movies rated\n",
      "User Id : 5 --> 314 movies rated\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4, 5]:\n",
    "    films = len(dataset[dataset[\"userId\"] == i])\n",
    "    print(f\"User Id : {i} --> {films} movies rated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage : 1.7\n"
     ]
    }
   ],
   "source": [
    "combination = nb_users * nb_movies\n",
    "ratings_num = len(dataset[\"rating\"])\n",
    "print(f\"Pourcentage : {round(ratings_num/combination*100, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1.7% of ratings that are available, which is normal as each hasn't rated all the movies. To see the dataset in a matrix form with all the missing ratings, use the `Dataframe.pivot()` function, with the `userId` as index, the `movieId` as columns, and the ratings for the `values` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9714</th>\n",
       "      <th>9715</th>\n",
       "      <th>9716</th>\n",
       "      <th>9717</th>\n",
       "      <th>9718</th>\n",
       "      <th>9719</th>\n",
       "      <th>9720</th>\n",
       "      <th>9721</th>\n",
       "      <th>9722</th>\n",
       "      <th>9723</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  0     1     2     3     4     5     6     7     8     9     ...   \n",
       "userId                                                               ...   \n",
       "0         4.0   4.0   4.0   5.0   5.0   3.0   5.0   4.0   5.0   5.0  ...  \\\n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3         NaN   NaN   NaN   2.0   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         4.0   NaN   NaN   NaN   4.0   NaN   NaN   4.0   NaN   NaN  ...   \n",
       "\n",
       "movieId  9714  9715  9716  9717  9718  9719  9720  9721  9722  9723  \n",
       "userId                                                               \n",
       "0         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 9724 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "pivoted_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the ratings of user 1. To do so, use the *movies.csv* file and your `movie_ids_map` dictionnary to find the movie title from the new movie indexes, and print the real movie title associated to each rating of user 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = pd.read_csv(\"../data/ml-latest-small/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user_1 = pivoted_dataset.loc[1,:]\n",
    "movie_user_1 = movie_user_1.dropna()\n",
    "merged_df = pd.merge(movie_user_1, movie_title, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cure, The (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Eat Drink Man Woman (Yin shi nan nu) (1994)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Exotica (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Exit to Eden (1994)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>235</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>French Kiss (1995)</td>\n",
       "      <td>Action|Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>237</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Forget Paris (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>238</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Far From Home: The Adventures of Yellow Dog (1...</td>\n",
       "      <td>Adventure|Children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Goofy Movie, A (1995)</td>\n",
       "      <td>Animation|Children|Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>240</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Hideaway (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>241</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fluke (1995)</td>\n",
       "      <td>Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>242</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Farinelli: il castrato (1994)</td>\n",
       "      <td>Drama|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>243</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gordy (1995)</td>\n",
       "      <td>Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>246</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hoop Dreams (1994)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>247</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Heavenly Creatures (1994)</td>\n",
       "      <td>Crime|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Houseguest (1994)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>249</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Immortal Beloved (1994)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>250</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Heavyweights (Heavy Weights) (1995)</td>\n",
       "      <td>Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>251</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hunted, The (1995)</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I.Q. (1994)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>253</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Interview with the Vampire: The Vampire Chroni...</td>\n",
       "      <td>Drama|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>254</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Jefferson in Paris (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>255</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jerky Boys, The (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>256</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Junior (1994)</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>257</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just Cause (1995)</td>\n",
       "      <td>Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>258</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kid in King Arthur's Court, A (1995)</td>\n",
       "      <td>Adventure|Children|Comedy|Fantasy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId    1                                              title   \n",
       "0        18  4.0                                  Four Rooms (1995)  \\\n",
       "1       219  4.0                                   Cure, The (1995)   \n",
       "2       232  3.0        Eat Drink Man Woman (Yin shi nan nu) (1994)   \n",
       "3       233  4.5                                     Exotica (1994)   \n",
       "4       234  4.0                                Exit to Eden (1994)   \n",
       "5       235  3.5                                     Ed Wood (1994)   \n",
       "6       236  4.0                                 French Kiss (1995)   \n",
       "7       237  4.0                                Forget Paris (1995)   \n",
       "8       238  4.5  Far From Home: The Adventures of Yellow Dog (1...   \n",
       "9       239  5.0                              Goofy Movie, A (1995)   \n",
       "10      240  4.5                                    Hideaway (1995)   \n",
       "11      241  3.0                                       Fluke (1995)   \n",
       "12      242  4.0                      Farinelli: il castrato (1994)   \n",
       "13      243  3.0                                       Gordy (1995)   \n",
       "14      246  5.0                                 Hoop Dreams (1994)   \n",
       "15      247  4.0                          Heavenly Creatures (1994)   \n",
       "16      248  5.0                                  Houseguest (1994)   \n",
       "17      249  3.5                            Immortal Beloved (1994)   \n",
       "18      250  2.5                Heavyweights (Heavy Weights) (1995)   \n",
       "19      251  3.5                                 Hunted, The (1995)   \n",
       "20      252  5.0                                        I.Q. (1994)   \n",
       "21      253  3.0  Interview with the Vampire: The Vampire Chroni...   \n",
       "22      254  4.0                          Jefferson in Paris (1995)   \n",
       "23      255  2.0                             Jerky Boys, The (1995)   \n",
       "24      256  3.5                                      Junior (1994)   \n",
       "25      257  5.0                                  Just Cause (1995)   \n",
       "26      258  5.0               Kid in King Arthur's Court, A (1995)   \n",
       "\n",
       "                                       genres  \n",
       "0                                      Comedy  \n",
       "1                                       Drama  \n",
       "2                        Comedy|Drama|Romance  \n",
       "3                                       Drama  \n",
       "4                                      Comedy  \n",
       "5                                Comedy|Drama  \n",
       "6                       Action|Comedy|Romance  \n",
       "7                              Comedy|Romance  \n",
       "8                          Adventure|Children  \n",
       "9           Animation|Children|Comedy|Romance  \n",
       "10                                   Thriller  \n",
       "11                             Children|Drama  \n",
       "12                              Drama|Musical  \n",
       "13                    Children|Comedy|Fantasy  \n",
       "14                                Documentary  \n",
       "15                                Crime|Drama  \n",
       "16                                     Comedy  \n",
       "17                              Drama|Romance  \n",
       "18                            Children|Comedy  \n",
       "19                                     Action  \n",
       "20                             Comedy|Romance  \n",
       "21                               Drama|Horror  \n",
       "22                                      Drama  \n",
       "23                                     Comedy  \n",
       "24                              Comedy|Sci-Fi  \n",
       "25                           Mystery|Thriller  \n",
       "26  Adventure|Children|Comedy|Fantasy|Romance  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a python library made for easily designing complex models such as deep learning models, in this module we are going to use just a few features from it to implement our simple matrix factorization model, as it makes a good introduction to the library before the next module about deep learning where you will also be using Keras.\n",
    "\n",
    "The following function `get_mf_model` implements the model described in equation (2) in Koren's paper (without the $+\\lambda(\\ldots)$ part for the moment). So it basically tries to find the $p_u \\in \\mathbb{R}^k$ and $q_i \\in \\mathbb{R}^k$ vectors that minimizes the squared loss between their dot product $p_u^Tq_i$, and the observed ratings $r_{ui}$, from random initialization of $p_u$ and $q_i$. In machine learning terms, $p_u$ and $q_i$ are called the *embeddings* of the user $u$ and of the movie $i$ respectively. Their size $k$ is an hyper-parameter of the model, which is called the *rank* of the factorization.\n",
    "\n",
    "To do so, it uses the functional API from Keras (the other API proposed is the sequential one, but is not adapted for this model), you can read about it here : https://keras.io/guides/functional_api/ .\n",
    "\n",
    "Keras, unlike Numpy, uses a different progamming paradigm. Numpy uses an *imperative* programming style (like python in general), meaning that when you execute `x.dot(y)`, the dot product is actually calculated. Keras however, uses a *declarative* (also called *symbolic*) programming style, meaning that when you write `Dot()([x, y])`, you tell Keras than when you will call the *fit* function of your model in the future, you will want to do a dot product between the future values that *x* and *y* will have. And this is what Keras is about, it allows you to build your own model as a sequence of operations, describing each input and output, and then later fit it and predict with it.\n",
    "\n",
    "Let's not get in too many details, but retain that the `get_mf_model` function below is not actually executing the model, it creates it, and returns an object of the class `keras.models.Model` that has been instructed with your model operations, and this object can then be trained with the classic `fit` and `predict` functions. \n",
    "\n",
    "Read carefully the comments in the code of the function to understand the different steps in the model creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS : 2.12.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKERAS : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeras\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorflow : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"KERAS : {keras.__version__}\")\n",
    "print(f\"Tensorflow : {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embedddings_2 = 30\n",
    "p_u = Embedding(nb_users, dim_embedddings_2, name=\"p_u__user_embedding\")(u)\n",
    "p_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape, Dot\n",
    "\n",
    "def get_mf_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a simple matrix factorization model from\n",
    "    the number of user, the number of movies, and the size of the embeddings k.\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization\n",
    "        \n",
    "    \"\"\"\n",
    "    dim_embedddings = k\n",
    "    \n",
    "    #Inputs:\n",
    "    #First we describe the input of the model, that is the training data that we will give it as X\n",
    "    #In our case, the input are just the user index u and the movie index i.\n",
    "    #So we declare two inputs of size one:\n",
    "    \n",
    "    u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    #Then let's declare our variable, the embeddings p and q.\n",
    "    #First with the users, we declare that we have nb_users embeddings, each of size dim_embeddings.\n",
    "    #An embedding object is indexed by calling it with the index parameter like a function,\n",
    "    #so we add a `(u)` at the end to tell keras we want it to be indexed \n",
    "    #by the user ids we will pass at training time as inputs.\n",
    "    \n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    \n",
    "    #Unfortunatly, when indexing an embeddings it keeps [1,k] matrix shape instead\n",
    "    #of just a [k] vector, so we have to tell Keras that we just want a vector by\n",
    "    #redefining its shape:\n",
    "    \n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Same thing for the movie embeddings:\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    #Then the dot product between the two indexed embeddings, \n",
    "    #we'll understand the axes = 1 part later.\n",
    "    r_hat = Dot(axes = 1)([q_i, p_u])\n",
    "\n",
    "    #We define our model by giving its input and outputs, in our case\n",
    "    #the user and movie ids will be the inputs, and the output will be\n",
    "    #the estimated rating r_hat, that is the dot product of the \n",
    "    #corresponding embeddings.\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "\n",
    "r_hat = Dot(axes = 1)([q_i, p_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:10:09.364767: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-11 21:10:09.364830: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "mf_model = get_mf_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows us to have a textual overview of the model we defined with the *summary()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_users * K : 18300\n",
      "nb_movies * K : 291720\n"
     ]
    }
   ],
   "source": [
    "print(f\"nb_users * K : {nb_users*30}\")\n",
    "print(f\"nb_movies * K : {nb_movies*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,020\n",
      "Trainable params: 310,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each of the keras objects we defined in our model is called a *layer*, and we find them in order in the first column. The *Param #* column gives the number of trainable parameters of the layer, in our case these are just the embeddings, and they should be equal to $nb\\_users \\times k$ and $nb\\_movies \\times k$. The *Connected to* column tells for each layer which layers are inputs for this layer (you can safely ignore the `[0][0]` for this module).\n",
    "\n",
    "Finally the *Output Shape* column gives us the shape of the layer, each layer being a *tensor*. A tensor is the generalization of matrices to more than two dimensions. So a matrix is a 2D-tensor and a vector is a 1D-tensor, and each layer can be a matrix, a vector, or a higher-order tensor. The output shape we see is indeed the expected one at each layer, except there is this `None` in first dimension, why is that ?\n",
    "\n",
    "To understand it, we have to get into how Keras is actually minimizing the mean squared loss of our model. In general, when in comes to minimizing error functions on big datasets, a generic method is to use Stocastic Gradient Descent (SGD), briefly described in page 4 of Koren's article. \n",
    "\n",
    "Read about gradient descent, SGD and its variant mini-batch SGD in Chapter 4 of *Hands on ML ...* (pages 111-120):\n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "This is what Keras does when it fits the model, it initializes the $q_i$ and $p_u$ embedding vectors randomly, and then perform mini-batch SGD to find the minimum mean squared error on the training set. Since mini-batching means considering multiple training samples at the same time, Keras keeps the first dimension of each layer to stack the samples of each batch, this is why `None` is written, the actual batch_size being set at training time when calling the `fit` function. This is also why we had to set `axes=1` when calling the `Dot` layer in the `get_mf_model` function, because the first dimension (axe 0) of each layer is kept for the batches. And about the `optimizer='adam'`, it is just a variation of mini-batch SGD that is faster, we'll get into more details about SGD variations in the optional parts of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our matrix factorization model on our movieLens data. The `epochs` parameter controls the number of iterations of the SGD algorithm, that is the number of times it is going to pass on each training rating and update the embeddings accordingly. Let's keep it at 20 for the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:10:11.134653: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 5s 24ms/step - loss: 13.3253 - mse: 13.3253\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 4s 21ms/step - loss: 11.6439 - mse: 11.6439\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 5.7928 - mse: 5.7928\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 2.6657 - mse: 2.6657\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 1.7664 - mse: 1.7664\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 1.3534 - mse: 1.3534\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 1.1157 - mse: 1.1157\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.9626 - mse: 0.9626\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.8576 - mse: 0.8576\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.7814 - mse: 0.7814\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.7241 - mse: 0.7241\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 0.6795 - mse: 0.6795\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.6439 - mse: 0.6439\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 0.6152 - mse: 0.6152\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.5911 - mse: 0.5911\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 0.5704 - mse: 0.5704\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 0.5523 - mse: 0.5523\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.5362 - mse: 0.5362\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 3s 18ms/step - loss: 0.5215 - mse: 0.5215\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 3s 17ms/step - loss: 0.5080 - mse: 0.5080\n"
     ]
    }
   ],
   "source": [
    "history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now try to predict the test ratings, and report our root mean squared error like in other regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50/316 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:11:16.164869: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 3ms/step\n",
      " Test RMSE : 1.1056819974000958 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "y_pred = mf_model.predict(X_test)\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 1.1/1.2 RMSE, we can probably do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the same model on your GPU and on your CPU, and compare the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the `tensorflow` python library for the computation part, if you have installed your GPU drivers and the GPU version of keras, then it will run on your GPU by default. We can force tensorflow to use the cpu instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "------------------------------------------------------------------\n",
      "Num CPUs Available:  1\n",
      "CPUs :  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs : \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"CPUs : \", tf.config.list_physical_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 13.3300 - mse: 13.3300\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 11.8066 - mse: 11.8066\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 5.8620 - mse: 5.8620\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.6520 - mse: 2.6520\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.7626 - mse: 1.7626\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.3604 - mse: 1.3604\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1332 - mse: 1.1332\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9880 - mse: 0.9880\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8883 - mse: 0.8883\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8160 - mse: 0.8160\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7616 - mse: 0.7616\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7197 - mse: 0.7197\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6853 - mse: 0.6853\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6577 - mse: 0.6577\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6338 - mse: 0.6338\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6135 - mse: 0.6135\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5957 - mse: 0.5957\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5794 - mse: 0.5794\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5643 - mse: 0.5643\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5503 - mse: 0.5503\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    mf_model = get_mf_model(nb_users, nb_movies, k)\n",
    "    history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix farctorization model is a really simple model, with not enough operations to parallelize on the GPU, this is why the training time is quite similar for this model. However with deep networks models the training time can be up to 10x times faster on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding user and movie bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enhance our matrix factorization model and add the user and movie biases to the rating estimation function as in equation (4) of Koren's paper ; except we will for the moment forget about the global bias $\\mu$ as it is not so intuitive to implement in Keras. Fill the function below to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add\n",
    "\n",
    "def get_mf_bias_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    \n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # User bias embeddings\n",
    "    bu = Embedding(nb_users, 1, name=\"bu__user_bias\")(u)\n",
    "    bu = Reshape((1,), name=\"bu__user_bias_reshaped\")(bu)\n",
    "    \n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    # Movie bias embeddings\n",
    "    bi = Embedding(nb_movies, 1, name=\"bi__movie_bias\")(i)\n",
    "    bi = Reshape((1,), name=\"bi__movie_bias_reshaped\")(bi)\n",
    "    \n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    # Add user and movie biases\n",
    "    d = Add()([d, bu, bi])\n",
    "    \n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " bu__user_bias (Embedding)      (None, 1, 1)         610         ['u__user_id[0][0]']             \n",
      "                                                                                                  \n",
      " bi__movie_bias (Embedding)     (None, 1, 1)         9724        ['i__movie_id[0][0]']            \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, 1)            0           ['p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " bu__user_bias_reshaped (Reshap  (None, 1)           0           ['bu__user_bias[0][0]']          \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bi__movie_bias_reshaped (Resha  (None, 1)           0           ['bi__movie_bias[0][0]']         \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1)            0           ['dot_2[0][0]',                  \n",
      "                                                                  'bu__user_bias_reshaped[0][0]', \n",
      "                                                                  'bi__movie_bias_reshaped[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 12.6505 - mse: 12.6505\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 9.8028 - mse: 9.8028\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 4.3644 - mse: 4.3644\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.0903 - mse: 2.0903\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.4491 - mse: 1.4491\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1509 - mse: 1.1509\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9809 - mse: 0.9809\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8730 - mse: 0.8730\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7997 - mse: 0.7997\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7471 - mse: 0.7471\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7073 - mse: 0.7073\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6761 - mse: 0.6761\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6508 - mse: 0.6508\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6293 - mse: 0.6293\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6102 - mse: 0.6102\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5933 - mse: 0.5933\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5774 - mse: 0.5774\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5622 - mse: 0.5622\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5475 - mse: 0.5475\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5335 - mse: 0.5335\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)\n",
    "    history = mf_bias_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41/316 [==>...........................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:11:29.484766: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 4ms/step\n",
      " Test RMSE : 1.0308558319698373 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a lower RMSE, about 1.0/1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we have omitted the regularization of the embeddings and bias parameters, as described in equation (5) of Koren's paper. We are now going to add them to the model, have a look at https://keras.io/layers/embeddings/ and https://keras.io/regularizers/ to see how to do this with keras. Fill the function below to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_):\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    \n",
    "    p_u = Embedding(\n",
    "        nb_users, \n",
    "        dim_embedddings, \n",
    "        name=\"p_u__user_embedding\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(u)\n",
    "    \n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # User bias embeddings\n",
    "    bu = Embedding(\n",
    "        nb_users, \n",
    "        1, \n",
    "        name=\"bu__user_bias\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(u)\n",
    "    bu = Reshape((1,), name=\"bu__user_bias_reshaped\")(bu)\n",
    "    \n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    q_i = Embedding(\n",
    "        nb_movies, \n",
    "        dim_embedddings, \n",
    "        name=\"q_i__movie_embedding\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    # Movie bias embeddings\n",
    "    bi = Embedding(\n",
    "        nb_movies, \n",
    "        1, \n",
    "        name=\"bi__movie_bias\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(i)\n",
    "    bi = Reshape((1,), name=\"bi__movie_bias_reshaped\")(bi)\n",
    "    \n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    # Add user and movie biases\n",
    "    d = Add()([d, bu, bi])\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " bu__user_bias (Embedding)      (None, 1, 1)         610         ['u__user_id[0][0]']             \n",
      "                                                                                                  \n",
      " bi__movie_bias (Embedding)     (None, 1, 1)         9724        ['i__movie_id[0][0]']            \n",
      "                                                                                                  \n",
      " dot_4 (Dot)                    (None, 1)            0           ['p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " bu__user_bias_reshaped (Reshap  (None, 1)           0           ['bu__user_bias[0][0]']          \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bi__movie_bias_reshaped (Resha  (None, 1)           0           ['bi__movie_bias[0][0]']         \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1)            0           ['dot_4[0][0]',                  \n",
      "                                                                  'bu__user_bias_reshaped[0][0]', \n",
      "                                                                  'bi__movie_bias_reshaped[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "mf_bias_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 12.6478 - mse: 12.6478\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 10.0553 - mse: 10.0553\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 4.5344 - mse: 4.5344\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.1147 - mse: 2.1147\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.4584 - mse: 1.4584\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1544 - mse: 1.1544\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9801 - mse: 0.9801\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8680 - mse: 0.8680\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7902 - mse: 0.7901\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7332 - mse: 0.7332\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6896 - mse: 0.6895\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6556 - mse: 0.6556\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6281 - mse: 0.6281\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6053 - mse: 0.6053\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5860 - mse: 0.5860\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5691 - mse: 0.5691\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5539 - mse: 0.5539\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5399 - mse: 0.5399\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5263 - mse: 0.5263\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5135 - mse: 0.5135\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "    history = mf_bias_reg_model.fit(X_train, y_train, epochs=20, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 35/316 [==>...........................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:16:55.958568: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 4ms/step\n",
      " Test RMSE : 1.0409464905069137 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_reg_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a slightly better RMSE, but sometimes regularization is very important for achieving good test performances, in depends on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of setting manually the maximum number of epochs, we prefer to use *early stopping*. When training with early stopping, keras keeps a given validation set though the parameter `validation_split`, on which it is going to monitor a performance measure you give it (here the `mse`) at every epoch, and continue optimization while the mse on the validation set keeps going down, and stops it when it goes back up. This mechanism is an easy way to avoid over-fitting, you can read more about it there : https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "In general when using early stopping we setup a high number of maximum epochs, that is never reach because the optimization is stopped by early stopping first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list.append(early_stopping)\n",
    "cb_list.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  1/160 [..............................] - ETA: 1:56 - loss: 13.3462 - mse: 13.3462"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:16:58.203713: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - ETA: 0s - loss: 12.7118 - mse: 12.7118"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:17:00.295723: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 3s 14ms/step - loss: 12.7118 - mse: 12.7118 - val_loss: 12.0607 - val_mse: 12.0607\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 10.6461 - mse: 10.6461 - val_loss: 8.4819 - val_mse: 8.4819\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 5.6608 - mse: 5.6608 - val_loss: 3.6071 - val_mse: 3.6071\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 2.5279 - mse: 2.5278 - val_loss: 2.2111 - val_mse: 2.2111\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 1.6400 - mse: 1.6399 - val_loss: 1.7358 - val_mse: 1.7358\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 1.2577 - mse: 1.2577 - val_loss: 1.4998 - val_mse: 1.4998\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 1.0451 - mse: 1.0451 - val_loss: 1.3662 - val_mse: 1.3662\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.9129 - mse: 0.9129 - val_loss: 1.2829 - val_mse: 1.2829\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.8245 - mse: 0.8245 - val_loss: 1.2279 - val_mse: 1.2279\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.7610 - mse: 0.7610 - val_loss: 1.1918 - val_mse: 1.1918\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.7144 - mse: 0.7144 - val_loss: 1.1671 - val_mse: 1.1670\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.6785 - mse: 0.6785 - val_loss: 1.1497 - val_mse: 1.1497\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.6497 - mse: 0.6497 - val_loss: 1.1376 - val_mse: 1.1376\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.6263 - mse: 0.6263 - val_loss: 1.1290 - val_mse: 1.1290\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.6063 - mse: 0.6063 - val_loss: 1.1225 - val_mse: 1.1225\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5887 - mse: 0.5887 - val_loss: 1.1200 - val_mse: 1.1200\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5729 - mse: 0.5729 - val_loss: 1.1168 - val_mse: 1.1168\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5582 - mse: 0.5582 - val_loss: 1.1141 - val_mse: 1.1141\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5441 - mse: 0.5441 - val_loss: 1.1138 - val_mse: 1.1138\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5310 - mse: 0.5309 - val_loss: 1.1147 - val_mse: 1.1147\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.5182 - mse: 0.5181 - val_loss: 1.1145 - val_mse: 1.1145\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5054 - mse: 0.5053 - val_loss: 1.1150 - val_mse: 1.1150\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4929 - mse: 0.4929 - val_loss: 1.1144 - val_mse: 1.1143\n",
      "Epoch 24/500\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.4796 - mse: 0.4796Restoring model weights from the end of the best epoch: 19.\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4804 - mse: 0.4803 - val_loss: 1.1138 - val_mse: 1.1138\n",
      "Epoch 24: early stopping\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    history = mf_bias_reg_model.fit(\n",
    "        \n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=500, \n",
    "        batch_size=512, \n",
    "        validation_split=0.1, \n",
    "        callbacks=cb_list\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-990f0e4264f56ef7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-990f0e4264f56ef7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stops before 500 epochs, when the validation MSE stops decreasing during 5 consecutive epochs (the patience value = 5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search embedding size and regularization factor with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the moment we didn't grid search our model hyper-parameters, such as `k` and `lambda_`. There exists some scikit-learn wrappers for keras models in order to use scikit grid search functions, unfortunately they only work with single input keras models, which is not our case as we have two inputs: the user and the movie indexes.\n",
    "\n",
    "So let's implement your own grid search function for the two parameters `k` and `lambda_`. With big enough datasets, it is not necessary to do a cross-validation for each hyper-parameter combination, and we can simply split the training set into a sub-training set and a validation set to test our hyper-parameters. It does work because the validation set is big enough to see enough data variations, and with very big datasets, it is anyway not possible anymore to do a full cross-validation as it takes too much time to train. \n",
    "\n",
    "Fill in the `grid_search` function below and use early stopping with a validation split (just like above), and retrieve the validation RMSE (you can get the MSE from the `history` variable that is returned by the `fit` method (and then take the `sqrt` of that)) for all the hyper-parameter combinations from the `param_grid` dictionary of hyper-parameter values. Call the `get_model_function` parameter (yes, you can pass functions as parameters!) to generate each model, and return the hyper-parameters that give the lowest RMSE on the 10% validation set, the RMSE value, and the best corresponding trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "mse\n",
      "val_loss\n",
      "val_mse\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(history.history):\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "for i, value in enumerate(history.history[\"mse\"]):\n",
    "    epoch_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1138215065002441"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mse = history.history['mse']\n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "min(val_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4qUlEQVR4nO3deXxb9YHv/e+RZMu7HduJl8ROHAiExCmEBCiUraWkDcuwdIGBoenM8/QFT0MLze20pExL6LTJ0GfKpS0FBu5cCkNpmXsHKHOBoWmBAE0pWUgJgSZAnNiJ4zibLa+yJZ37x5Fk2ZYsOZGOZOnzfr3OS0fn/OTzA6P6299qmKZpCgAAwCaOdFcAAADkFsIHAACwFeEDAADYivABAABsRfgAAAC2InwAAABbET4AAICtCB8AAMBWrnRXYKxAIKD29naVlpbKMIx0VwcAACTANE319PSovr5eDsfEbRsZFz7a29vV0NCQ7moAAIDj0NbWplmzZk1YJuPCR2lpqSSr8mVlZWmuDQAASITH41FDQ0P47/hEMi58hLpaysrKCB8AAEwxiQyZYMApAACwFeEDAADYivABAABsRfgAAAC2InwAAABbET4AAICtCB8AAMBWhA8AAGArwgcAALAV4QMAANiK8AEAAGxF+AAAALbKnfAxcEza+DPpN7emuyYAAOS03AkfviFp/fekt/9NOrYn3bUBACBn5U74KK2R5lxgnb/7dHrrAgBADsud8CFJzZ+zXt/9j/TWAwCAHJZb4eO0KyVHnnTwXanzL+muDQAAOSm3wkdRpXTyJdY5rR8AAKRFboUPSWr+vPX67n9IppneugAAkINyL3yculxyFUpHP5IObEt3bQAAyDm5Fz7cJdKpn7XO6XoBAMB2uRc+pIhZL89IgUB66wIAQI7JzfBx8qWSu0zy7JPa/pTu2gAAkFNyM3zkFUjzr7DO6XoBAMBWuRk+pJGul/eelfy+tFYFAIBckrvhY+5FUlGV1HdI2vNaumsDAEDOyN3w4cyTFlxlnW+n6wUAALvkbviQRhYce/8/JZ83vXUBACBH5Hb4aDxXKq2XvN3Sh79Ld20AAMgJkw4fr732mq688krV19fLMAw9++yz4XvDw8P69re/rUWLFqm4uFj19fX60pe+pPb29mTWOXkcDqn5WuucWS8AANhi0uGjr69Pp59+uu6///5x9/r7+7V161Z997vf1datW/X0009r165d+qu/+qukVPZEHO0b0uqnt+umf/2TzMg9XULhY+eL0lBfeioHAEAOcU32A8uXL9fy5cuj3isvL9f69etHXfvZz36ms88+W62trWpsbDy+WiZBUb5T/765Tf6AqYMer2rLC6wb9WdK05qkYy1WAFn0+bTVEQCAXJDyMR/d3d0yDEMVFRVR73u9Xnk8nlFHKhTkOTVvRokkafv+7pEbhhGx3PrTKXk2AAAYkdLwMTg4qDvuuEM33HCDysrKopZZt26dysvLw0dDQ0PK6tM8s1zSmPAhjYSPD9dLA10pez4AAEhh+BgeHtb111+vQCCgBx54IGa51atXq7u7O3y0tbWlqkpaFAof+7pG36hZIM1YIPmHpL/8n5Q9HwAApCh8DA8P64tf/KJaWlq0fv36mK0ekuR2u1VWVjbqSJWRlg/P6EGn0sjA0+3/O2XPBwAAKQgfoeDxwQcf6He/+52qqqqS/YjjtqCuTA5DOtzr1UHPmEXFQl0vLRuk3kP2Vw4AgBwx6fDR29urbdu2adu2bZKklpYWbdu2Ta2trfL5fPr85z+vzZs365e//KX8fr86OjrU0dGhoaGhZNd90grznZo3o1RSlHEflXOtmS9mwNpsDgAApMSkw8fmzZu1ePFiLV68WJK0atUqLV68WN/73ve0b98+Pffcc9q3b5/OOOMM1dXVhY+NGzcmvfLHI+agU2lkmi0LjgEAkDKTXufj4osvHj9eIsJE9zLBopll+o+t0rvRwsfCa6SX7pRa/yh175PKZ9lfQQAAslzO7e2yaNYELR9l9dLsT1jnrPkBAEBK5Fz4WFBXLochHerx6qBncHwB9noBACClci58FOY7dXJopdN9UVo/FlwlGU7pwDbpyEf2Vg4AgByQc+FDijPotLhaOumT1jmtHwAAJF1Oho/QSqdRB51KI2t+bP/fUoYPoAUAYKrJ6fARteVDkuZfLjnd0uGd0sEdNtYMAIDsl5PhY0G9tdJpZ49XndEGnRaUS/Mutc7fZbl1AACSKSfDR1G+SydNDw46jdX6EbngGF0vAAAkTU6GDymBrpd5n5HyS6SuVmnfZhtrBgBAdsvZ8NEcb9BpfpF06mXWObNeAABImpwNHx+baKXTkNCslx3PSAG/DbUCACD75Wz4CA06PejxqrMnyqBTSTrpU1JBhdTbIe39g631AwAgW+Vs+IgcdBqz68WVLy34K+ucrhcAAJIiZ8OHFDHodJ8ndqFQ18t7v5F8QzbUCgCA7JbT4WPCZdZD5lwgFc+QBo5Ju1+1p2IAAGSxnA4fi8KDTrtiF3I4pYXXWOcsOAYAwAnL6fCxoK5MRrxBp9LIgmN/eV4aHrCncgAAZKmcDh/F7gQGnUrSrLOk8kZpqFfa9ZJNtQMAIDvldPiQEhx0ahhS87XWObNeAAA4ITkfPhIadCqNzHr54LfS4ARBBQAATCjnw8eieMush9QukqpPkXyD0s4XbKgZAADZKefDx8J6a9Bph2dQh3q8sQsaxkjrB10vAAAct5wPH8Vul+ZWF0tKoPUjFD4+elnqP5rimgEAkJ1yPnxIEYNO44WP6nlS7cekgM9a8RQAAEwa4UOTGHQq0fUCAMAJInxoEoNOpZEpt3vekDwHUlgrAACyE+FD0sKZ5TIM6UD3oA73TjDoVJIqGqWGcySZ0o5nbKkfAADZhPAhqcTtUlNw0GliXS/B5dbpegEAYNIIH0Hhrpd9CYSPhVdLhkPav1k6tiel9QIAINsQPoISnvEiSSUzpKYLrXNaPwAAmBTCR1DzZAadStL8K6zXvRtTVCMAALIT4SNoYX2ZJKm9e1BH4g06laTp863Xo7tTWCsAALIP4SOotCBPc6dPYtBp1UnW67G9kn84hTUDACC7ED4iTGq9j5JayVUomX6pqzXFNQMAIHsQPiJMatCpwyFVzrXO6XoBACBhhI8I4WXWE5luK0lVwfBx5KMU1QgAgOxD+Igw6UGntHwAADBphI8IpQV5mjuZlU4rg4NOj9LyAQBAoggfY0xqvY/QjBe6XQAASBjhY4xJDToNdbt0tTLdFgCABBE+xhhp+fDEL1xax3RbAAAmifAxxsKZ1qDT/V0DOto3NHFhwxhp/aDrBQCAhBA+xigryFPTZAadVjHjBQCAySB8RDGpQafMeAEAYFImHT5ee+01XXnllaqvr5dhGHr22WdH3TdNU2vWrFF9fb0KCwt18cUXa8eOHcmqry0WBbteElpsjG4XAAAmZdLho6+vT6effrruv//+qPd/9KMf6d5779X999+vTZs2qba2Vpdeeql6enpOuLJ2aZ7MjJfQdFu6XQAASIhrsh9Yvny5li9fHvWeaZq67777dOedd+raa6+VJD322GOqqanRk08+qZtvvvnEamuTUPjY3zWgY31DmlacH7twqNslNN3WmWdDDQEAmLqSOuajpaVFHR0dWrZsWfia2+3WRRddpI0bN0b9jNfrlcfjGXWkW1lBnuZUFUlKoPWjtFbKK7Km2x7ba0PtAACY2pIaPjo6OiRJNTU1o67X1NSE7421bt06lZeXh4+GhoZkVum4Jdz1Ejndlq4XAADiSslsF8MwRr03TXPctZDVq1eru7s7fLS1taWiSpO2aFIzXkLhg0GnAADEM+kxHxOpra2VZLWA1NXVha93dnaOaw0JcbvdcrvdyaxGUhzXMuu0fAAAEFdSWz6amppUW1ur9evXh68NDQ1pw4YNOu+885L5qJRbGAwf+45Zg04nxAZzAAAkbNItH729vfrwww/D71taWrRt2zZVVlaqsbFRt99+u9auXat58+Zp3rx5Wrt2rYqKinTDDTckteKpVl6Yp9lVRdp7pF/vtnfrgnnTYxdmoTEAABI26fCxefNmffKTnwy/X7VqlSRpxYoV+sUvfqFvfetbGhgY0Fe/+lUdO3ZM55xzjn7729+qtLQ0ebW2SfPMcu090q/t++OFj4jdbX1DkmuCqbkAAOS4SYePiy++WKZpxrxvGIbWrFmjNWvWnEi9MsLHZpbr+XcOxB90Wlor5RVLw31WAKk+2Z4KAgAwBbG3ywRCg07fibfM+qjptnS9AAAwEcLHBCY16LSyyXplxgsAABMifEwgNOhUkt5tj9P6wYwXAAASQviII+GVTpnxAgBAQggfcSS80ikLjQEAkBDCRxwJr3RaFbG7rS/O+BAAAHIY4SOO5norfLQdHVBX/wShoqTGmm5rBqQudrcFACAWwkcc5UV5aqwMDjrd74ldkN1tAQBICOEjAYl3vQTDBzNeAACIifCRgOZJDzolfAAAEAvhIwEJt3yEp9vS7QIAQCyEjwQ0zyyTJLUe7Vd3/3Dsgiw0BgBAXISPBFQU5auhslBSnJVOQ90u3W1MtwUAIAbCR4IS6nopqZHyS5huCwDABAgfCUpomXXDGNlgjq4XAACiInwkaPLLrBM+AACIhvCRoNBKp3uP9Kt7YIJBp8x4AQBgQoSPBE0rztesadag0x0TtX4w4wUAgAkRPiYhoUGndLsAADAhwsckJDToNNTt0r1P8nltqBUAAFML4WMSEhp0WjJjZLrtMabbAgAwFuFjEkLhY89Eg04jp9sy6BQAgHEIH5OQ8KDT8IwXxn0AADAW4WOSEhp0yowXAABiInxMUmKDTkMzXuh2AQBgLMLHJCU06JRuFwAAYiJ8TFLkoFPPYIxBp+HdbZluCwDAWISPSZpWnK+ZFdag05itH0y3BQAgJsLHcYjb9WIYrHQKAEAMhI/jsGhWaNCpJ3ahUPhgxgsAAKMQPo5DcyKDTqvY3RYAgGgIH8ch1O3ScrhvgkGnzHgBACAawsdxqIwYdLojVtdLuNuFlg8AACIRPo5T88wySRN0vYS6XbrbmG4LAEAEwsdxCs94aY8RPoqnS/mlkkzp2B7b6gUAQKYjfByneTWlkqxxH1FF7m7LjBcAAMIIH8epqbpYktRyqE+maUYvxIwXAADGIXwcp8bKIhmG1OP16UjfUPRCzHgBAGAcwsdxKshzqr7cmvESs+uFhcYAABiH8HECwl0vscJHuNulxaYaAQCQ+QgfJyBu+KiMmG47PGhTrQAAyGyEjxMwJxg+9sQKH8XVTLcFAGAMwscJmBuv5cMwpKrQ7rbMeAEAQCJ8nJBwy8eRPgUCMabbhgadMuMFAABJKQgfPp9P//AP/6CmpiYVFhZq7ty5+v73v69AIJDsR6XdrGmFcjkMDQ4H1OGJMaajkrU+AACI5Er2D7znnnv00EMP6bHHHtPChQu1efNm/e3f/q3Ky8t12223JftxaZXndKihskgth/u053Cf6oObzY0SmvHCdFsAACSlIHz88Y9/1FVXXaXLL79ckjRnzhz96le/0ubNm5P9qIzQVF2slsN92n24T+edXD2+QCVjPgAAiJT0bpfzzz9fv//977Vr1y5J0p///Ge98cYbuuyyy6KW93q98ng8o46pZE5VnBkv4em2+5huCwCAUtDy8e1vf1vd3d2aP3++nE6n/H6/fvjDH+qv//qvo5Zft26d7r777mRXwzZN0+PMeCmultxlktdjTbedMd++ygEAkIGS3vLx1FNP6YknntCTTz6prVu36rHHHtM///M/67HHHotafvXq1eru7g4fbW1tya5SSjUFWz5ajiSwuy0zXgAASH7Lx9///d/rjjvu0PXXXy9JWrRokfbu3at169ZpxYoV48q73W653e5kV8M2oZaP1iP98vkDcjmj5LnKk6QDf2bcBwAASkHLR39/vxyO0T/W6XRm5VRbSaorK5Db5ZAvYGp/10D0Qsx4AQAgLOktH1deeaV++MMfqrGxUQsXLtTbb7+te++9V3/3d3+X7EdlBIfD0JyqYu082KPdh/s0O9gNMwoLjQEAEJb08PGzn/1M3/3ud/XVr35VnZ2dqq+v180336zvfe97yX5UxphTXaSdB3usGS+nRilQye62AACEJD18lJaW6r777tN9992X7B+dsZqqSyQdjD3jpWrMdNu8AtvqBgBApmFvlyRoqi6SNMF026Iqa7qtTOkYrR8AgNxG+EgCq+Ujzu62rHQKAIAkwkdSzAm2fLR3Dcjr80cvxIwXAAAkET6SYnqJWyVulwKm1Ha0P3ohZrwAACCJ8JEUhmGEWz92H4qzxwvdLgCAHEf4SJLQuI89sZZZD3e7ED4AALmN8JEkTVVxZryEul08+6ThGCuhAgCQAwgfSTKnOs7utkVVkrvcOj+2x55KAQCQgQgfSdIUL3xE7m7LjBcAQA4jfCRJKHwc9HjV5/VFLxQa98GMFwBADiN8JElFUb6mFeVJmmDQKTNeAAAgfCRTaNzHnsNx1vqg2wUAkMMIH0k0Mu6jN3qBKna3BQCA8JFETVWh8BGr5SMYPphuCwDIYYSPJGqaHqflo6hyZLotrR8AgBxF+EiiOcGWjz1HYrR8GIZUxe62AIDcRvhIotCYj6N9Q+ruH45eqJLptgCA3Eb4SKJit0szSt2SpJaY022Z8QIAyG2EjyRLfMYL3S4AgNxE+EiykfARZ8YL4QMAkKMIH0kWd4+X8O62+6WhGAEFAIAsRvhIspFVTmPtblspFbC7LQAgdxE+kmxuRMuHaZrjCxgGM14AADmN8JFkDZVFMgyp1+vT4d6h6IWY8QIAyGGEjyQryHNqZkWhpAnGfTDjBQCQwwgfKdAUb9wHM14AADmM8JECofCxO96MF7pdAAA5iPCRAuE9XuJ1u/S0M90WAJBzCB8pMLK7bYzwUTgtYrotu9sCAHIL4SMFmsK72/YpEIgz3ZauFwBAjiF8pMCsaYVyOQx5fQEd8AxGL8SMFwBAjiJ8pIDL6VBjZZGkiWa8BAedstAYACDHED5SZE7cPV5C3S60fAAAcgvhI0XibjBHtwsAIEcRPlIk7gZzoW4XptsCAHIM4SNF5sZr+SiqlAoqrHNaPwAAOYTwkSKhlo/Wo/3y+QPRC9H1AgDIQYSPFKkrK5Db5ZAvYGrfsYHohZjxAgDIQYSPFHE4jPAy6y1H2GAOAIAQwkcKhWe8HIoz44XptgCAHEL4SKHwjJeYLR90uwAAcg/hI4XizngJT7c9IA3FKAMAQJYhfKRQ3FVOiyqtHW4l6Si72wIAcgPhI4VCYz72dw1ocNgfvRBdLwCAHJOS8LF//379zd/8jaqqqlRUVKQzzjhDW7ZsScWjMlp1Sb5K3C6ZptR2NMYqpsx4AQDkmKSHj2PHjukTn/iE8vLy9OKLL+q9997Tj3/8Y1VUVCT7URnPMIxw68fueHu8HKHlAwCQG1zJ/oH33HOPGhoa9Oijj4avzZkzJ9mPmTLmVBdr+/7u+Hu80PIBAMgRSW/5eO6557R06VJ94Qtf0IwZM7R48WI98sgjMct7vV55PJ5RRzaJu7st3S4AgByT9PCxe/duPfjgg5o3b55eeukl3XLLLfr617+uxx9/PGr5devWqby8PHw0NDQku0pp1VRdJGmi8NFkvTLdFgCQI5IePgKBgM4880ytXbtWixcv1s0336yvfOUrevDBB6OWX716tbq7u8NHW1tbsquUVk3VJZISnW5L6wcAIPslPXzU1dVpwYIFo66ddtppam1tjVre7XarrKxs1JFNmoL7u3T2eNXn9UUvRNcLACCHJD18fOITn9DOnTtHXdu1a5dmz56d7EdNCeVFeaoszpeUwEqnzHgBAOSApIePb3zjG3rzzTe1du1affjhh3ryySf18MMPa+XKlcl+1JQxp8oa9xFzj5fQdFsWGgMA5ICkh4+zzjpLzzzzjH71q1+publZ//iP/6j77rtPN954Y7IfNWWEx33E2t023O3CEusAgOyX9HU+JOmKK67QFVdckYofPSWFZ7zE292WbhcAQA5gbxcbxJ3xUhUMH70dkrfXploBAJAehA8bzAm2fMRc5bRwmlRYaZ0z4wUAkOUIHzaYE5xue6x/WF39Q9ELTT/Vej34rk21AgAgPQgfNih2u1RT5pY0QdfLrLOs17Y/2VQrAADSg/Bhk1DrR8zptg3nWK+thA8AQHYjfNhk7vTgBnOxpts2nG29HnpfGuiyp1IAAKQB4cMmoZaPliP90QuUzBiZcrtvs021AgDAfoQPmzRVB8PH4Qmm0oa6Xhj3AQDIYoQPm4TCx57D/TJNM3qhUNdL25s21QoAAPsRPmzSWFUkw5B6vT4d6vVGL9Twcet13xbJH2MHXAAApjjCh03cLqdmVhRKslo/opo+X3KXScN9UucOG2sHAIB9CB82ijvuw+EYWe+DKbcAgCxF+LDRSPiI0fIhSY3BrhcGnQIAshThw0aJzXgJDTp9y4YaAQBgP8KHjeZEzHiJaeYSyXBI3a2Sp92mmgEAYB/Ch43mVo8ssR4IxJhu6y6Vapqtc7peAABZiPBho5kVhXI5DHl9AR3wDMYuGF5sjK4XAED2IXzYyOV0qLGqSNIEe7xIEZvMsdgYACD7ED5s1hTe42WC8NEYDB8d70hDE4wPAQBgCiJ82Cw842Wilo/yBqm0Tgr4pPa3baoZAAD2IHzYbE7EoNOYDCNiyi2DTgEA2YXwYbO54bU+JggfEjvcAgCyFuHDZqGWj7aj/Rr2B2IXbIhY6TTWLrgAAExBhA+b1ZYVqCDPIV/A1L5jAxMUXCS5CqSBY9KRD+2rIAAAKUb4sJnDYWhOVWil0wm6Xlz5Uv2Z1jlTbgEAWYTwkQah8LE73riPRsZ9AACyD+EjDZqmJ9DyIbHSKQAgKxE+0iC80Fi88DErON328E6p/2iKawUAgD0IH2kQavmIGz6Kq6Sqedb5vk0prhUAAPYgfKRBaMxHe/eABof9ExdmvQ8AQJYhfKRBdUm+St0umabUejTO3i2hlU5bCR8AgOxA+EgDwzDCi43tnmiPF0lqDC42tn+L5B9Occ0AAEg9wkeaNCWyx4tkjfkoqJB8A1LH9tRXDACAFCN8pEl4g7l4g04dDjaZAwBkFcJHmoQ2mIu70JjEoFMAQFYhfKRJwi0fEouNAQCyCuEjTUILjXX2eNXr9U1ceOaZkuGUPPul7n021A4AgNQhfKRJeVGeKovzJSXQ+pFfbO1yK7HJHABgyiN8pFFoxkvclU6lkSm3dL0AAKY4wkcahVY6TWzcBzNeAADZgfCRRnMT3eNFGhl02rFd8vamsFYAAKQW4SONQi0fLfEWGpOk8llS2SzJ9EvtW1NcMwAAUofwkUaTGvMh0fUCAMgKhI80mlNdJEnq6h/Wsb6h+B8Idb2wyRwAYApLefhYt26dDMPQ7bffnupHTTlF+S7VlhVISrDrpTEYPva9JQUCKawZAACpk9LwsWnTJj388MP62Mc+lsrHTGmh1o+EZrzUNEt5RdJgt3R4V4prBgBAaqQsfPT29urGG2/UI488omnTpqXqMVNeU3WJpATHfTjzpJlLrPM2FhsDAExNKQsfK1eu1OWXX65Pf/rTE5bzer3yeDyjjlzSFGz5SHzQKfu8AACmNlcqfuivf/1rbd26VZs2bYpbdt26dbr77rtTUY0pYVItHxI73AIAprykt3y0tbXptttu0xNPPKGCgoK45VevXq3u7u7w0dbWluwqZbSmiDEfpmnG/8CspdbrkQ+lviMprBkAAKmR9PCxZcsWdXZ2asmSJXK5XHK5XNqwYYN++tOfyuVyye/3jyrvdrtVVlY26sglDZVFchhS35Bfh3q88T9QVClNn2+d0/oBAJiCkh4+LrnkEm3fvl3btm0LH0uXLtWNN96obdu2yel0JvuRU5rb5dTMaYWSWGwMAJAbkj7mo7S0VM3NzaOuFRcXq6qqatx1WOZUFavt6IBaDvfpnLlV8T/QcI609XEGnQIApiRWOM0Ac6snsceLNDLotH2r5EtgZVQAADJISma7jPXqq6/a8Zgpa04ofBxKMHxUnSwVVkoDR6WOd0YGoQIAMAXQ8pEBQhvM7Um05cMwmHILAJiyCB8ZYCR89CsQSGC6rTQy6LSVlU4BAFML4SMDzKwoVJ7T0JAvoPbugcQ+1Phx67XtT1Ii64MAAJAhCB8ZwOV0aHaV1frxp91HE/tQ/WLJ4ZJ6D0pdrSmsHQAAyUX4yBDXLJ4pSbr/lQ/l8wfifyCvUKo73Tpn3AcAYAohfGSIFefN0bSiPLUc7tMzb+9P7EMNEV0vAABMEYSPDFHidumWi06SJP305Q80nEjrByudAgCmIMJHBvnSuXNUXeJW29EB/a/N++J/IDTd9uAOyduT2soBAJAkhI8MUpjv1Fcvtlo/7n/5A3l9/ok/UFYnVTRKZkDat9mGGgIAcOIIHxnmhnMaVVtWoPbuQf36rbb4HwgvNsY+LwCAqYHwkWEK8pxa+amTJUk/f+VDDQ7Haf1gpVMAwBRD+MhA1y1t0MyKQnX2ePXEm3snLhwKH/s2SYE4QQUAgAxA+MhA+S6HvhZs/Xjw1Y/U5/XFLjxjgZRfInk90qG/2FRDAACOH+EjQ31uySw1VhbpSN+QHv/jBK0fTpc0c4l1TtcLAGAKIHxkqDynQ7ddMk+S9C+vfaSeweHYhUNdL62EDwBA5iN8ZLCrF8/U3OnF6uof1qN/2BO7YCODTgEAUwfhI4M5HYZu//QpkqRHXt+t7v4YrR8zl0oypGMtUm+nfRUEAOA4ED4y3BWL6nRqTal6Bn36H2/sjl6osEKacZp1TusHACDDET4ynMNh6BuXWmM//ucbLTraNxS9IOt9AACmCMLHFPCZhbVaWF+mviG//uW1j6IXYqVTAMAUQfiYAgzD0KpLrbEfj2/cq0M93vGFQjvctr8t+aLcBwAgQxA+pohPzZ+h0xsqNDDs14OvRmn9qJwrFU+X/ENS+zbb6wcAQKIIH1NEZOvHE3/aq4OewbEFGPcBAJgSCB9TyIXzqrV09jQN+QL6+Ssfji8Q6nohfAAAMhjhYwoxDEOrllmtH79+q037uwZGF2j4uPXa9ifJNG2uHQAAiSF8TDHnnVStc+dWacgf0P0vfzD6Zt3pkjNf6jtkLTgGAEAGInxMQf8t2PrxvzbvU+uR/pEbeQVS3RnWOVNuAQAZivAxBS2dU6kLT5kuX8DUT34/pvUjtM9L65v2VwwAgAQQPqao0MyXZ97ep48O9Y7cYLExAECGI3xMUWc0VOjTp81QwJR+8ruI1o9ZwRkvne9Jg93pqRwAABMgfExhoR1v//Oddu3s6LEultZI0+ZIMqV9m9JWNwAAYiF8TGHNM8v12YW1Mk3pJ7/fNXIjPOWWrhcAQOYhfExx37j0FBmG9ML2Du1oD3azhBYb2/OH9FUMAIAYCB9T3Km1pbriY/WSpP++Pjj2o+ki63XvG9LWx9NUMwAAoiN8ZIHbPz1PDkP63fsH9ee2Lqn6ZOmTd1o3n/9vUhtjPwAAmYPwkQVOml6iqxfPlCTduz449uOCb0rzr7B2uf33m6SejjTWEACAEYSPLHHbJfPkdBjasOuQtuw9Kjkc0jUPSdPnSz0HpKduknzedFcTAADCR7aYXVWsLyyZJUn68W+DrR/uUun6JyV3ubTvLenFb6WxhgAAWAgfWeTWT52sPKehjR8d0R8/OmJdrDpJ+vy/SjKkLb+QNv/PdFYRAADCRzaZNa1I15/VKEn67+t3yTRN68a8S6VLvmudv/At9n0BAKQV4SPLrPzkycp3OfTWnqN648PDIzfOXyUtuEoKDFvjPzzt6askACCnET6yTG15gW48x2r9+M4z27Vl7zHrhmFIVz0gzVgo9XVKT/2NNDyYxpoCAHIV4SMLrfzkyaorL1Db0QF94aGNWvvC+xoc9kvuEun6X0oFFdL+LdLzq6RQ1wwAADYhfGSh6hK3XrztAl27eKYCpvTwa7t12U9ft1pBKpukLzwqGQ5p2y+ltx5Jd3UBADkm6eFj3bp1Ouuss1RaWqoZM2bo6quv1s6dO5P9GMRRUZSve687Q//jS0s1o9St3Yf69PmHNuqHz7+nwcaLpE/fbRV8abW05430VhYAkFOSHj42bNiglStX6s0339T69evl8/m0bNky9fX1JftRSMCnF9Ro/Tcu0rVnzpRpSo+83qLLfvK6tsy8UWr+vBTwSf++QupqS3dVAQA5wjDN1Hb6Hzp0SDNmzNCGDRt04YUXxi3v8XhUXl6u7u5ulZWVpbJqOed37x3Ud57Zrs4erwxDuuXcOv39/q/LcXC7VHe69HcvSXmF6a4mAGAKmszf75SP+ejutrZ5r6ysjHrf6/XK4/GMOpAaY1tBHtx4QDf2fF3D7krpwJ+l/7yNAagAgJRLafgwTVOrVq3S+eefr+bm5qhl1q1bp/Ly8vDR0NCQyirlvPKiPN37xTP0ryussSB/PFqsL/X8fwrIKb3zlPTmA+muIgAgy6W022XlypV6/vnn9cYbb2jWrFlRy3i9Xnm9IxueeTweNTQ00O1ig+7+Yd39f3bo6a379bfOF3VX3r/JNJwybnpamntxuqsHAJhCMqLb5Wtf+5qee+45vfLKKzGDhyS53W6VlZWNOmCPyFaQF4qu0n/4L5Bh+tX/5Jc02Lk73dUDAGSppIcP0zR166236umnn9bLL7+spqamZD8CSXbJaTX67Tcu1lsLv6c/B+aqyNetfQ9dq60f7Et31QAAWSjp4WPlypV64okn9OSTT6q0tFQdHR3q6OjQwMBAsh+FJCovytM915+t3qse1VGV6+RAi/Y//v/oB/+5QwND/nRXDwCQRZI+5sMwjKjXH330UX35y1+O+3mm2qZf787XVPirq+WUX+uG/1q/nXa9/v/Pf0xL50SfsQQAwGT+fruS/fAULxsCG5SceqF02T3SC9/Ut/Ke0vtHG/WFf+nTWbMrddmiWi1fVKeasoJ0VxMAMEWlfJGxyaLlI0OYpvTc16S3/039jhItH/i+9pq1kqwNcpc0TtNli+q0fFGt6spZmAwAct1k/n4TPhCbzyv94nJp3yYNV52qXy/8Fz27c8DaoC7CmY0VumxRnS5bVKf6CoIIAOQiwgeSx3NAevgiqfegVFAhXbBKB+bfpBff79aL7x7Q5r3HRi2KekZDhS4PtojMmlaUtmoDAOxF+EBytW+TnrlZOvQX633ZTOniO6TTb9DBPp9e3H5AL7zboU17jo4KIqfPKg+3iDRUEkQAIJsRPpB8Ab/0519Lr6yVPMH1P6pPlS75rjT/Cskw1OkZ1H/t6NAL2w/orZajCkT8l7VophVELl9Up8YqgggAZBvCB1JneFDa9Ij0+o+lgeDYj1lnSZ++W5rziXCxQz1e/deODr24/YDe3H1kVBBZWF+mS+bP0MKZ5VpQV6ZZ0wpjTtEGAEwNhA+k3mC39IefSH98QPIFF5Cbt0y65C6pdvQmgod7vXppR4de3N6hP+4+In9g9H9ypQUuLagr04L6Mp1WV6YFdWWaV1Mit8tp1z8NAOAEET5gn54OacM90pbHJNMvyZA+9kXpk3dK02aPK36k16v17x3U5r3H9F67Rx909mjYP/4/QZfD0MkzSrSgviwcTBbUlamiKN+GfygAwGQRPmC/Ix9JL/+jtOMZ670jTzrr/5Uu/KZUXB3zY0O+gD7s7NX7Bzx674BH77Vbr90Dw1HLz6woDLaOlAYDSbkaKum2AYB0I3wgffZvlX5/t7T7Vet9fql03tekc1dK7pKEfoRpmmrvHrSCSLtH7x3o1nsHPGo7Gn1/oFK3Sw2VRaqvKNTMigLVVxSGj5kVhZpe6pbTQTgBgFQifCD9PnpZ+t0a6cCfrffF06ULvyUt+bLkOr6uk+6BYf3lgGekleSAR7s6ejXkD0z4OZfDUE1ZgWZWFKo+IpzMDL7WVRSorCDvuOoEALAQPpAZAgHpvWekl38gHd1tXZs2R/rkP0jNn5McJ76p8rA/oJbDfdp3rF/7uwZ1oGtA7V0Dau8a1P6uAXV4BscNcI2m1O0KhpIC1ZYXqLI4X9OKrKOyOF/TivNVWZSviuI8lbpddPMAwBiED2QW/7C09THp1Xukvk7rWtXJ1vogp3xWajhbcqRmZos/YKqzZ1DtXQOjwsn+Lutae/eAuvqjjy+JxeUwRsJIUV44nEwryhsVVqYVWWXKCl0qyncp33XiYQsAMhXhA5lpqE968wHpDz+VvJ6R64XTrGm6p3xGOukSqbDC1mr1D/nUHgojwdaSrv5hHe0b0rH+4NFnvR8Y9h/3c/KdDhW7nSp2u1Tidqk4eJS4nSrOd426XhIsF+1agcupgjyn3C6HHIxlAZAhCB/IbIPd0gfrpV3/Zb0Odo3cc7ikxnOtFpFTPitVn5y2akYzOOzXsf4hK5j0DYfDydG+oVGBJfL9iQSWePJdDrldDhXkOVWQ5wgHk4I8RzCgOOUOXx9fzp3nUJ7TOvJdDuU7jfD7kWsO5bms6/nBa9Z9wzp3EIIAED4wlfh90r63pJ0vSrtekg7vHH2/8iTp1OVWq0jjuZJz6g0MHfYH1O/1q3fIpz6vT71e69U698e81jcUed0fPvclMIbFbi6HFU5cTkMuhyGX0yGXw5AzeN3pCF035HRY9yLf5wXLWp8f+azTYcjhMOQ0gueGIadDUa6NnLvCn9GozzuC9x3B60bo3LDOrc/LKhN5HiwX+XmHYcgwJEPWa0jktbH3jeB9he5LMgwjfH1UWUPh+hkR5RV87zCifNaRwM8c8+zIejGOCSeK8IGp6+huaddvpV0vSnv+IAUixmO4y6WTL7FaROZdKhVVpq+eaeTzB+T1BTQ47Ndg6HXYr8HhgLzDfg36/PIOBzTos66F7g1G3PMG73l9fg35TA37A+FjyBfQkD/imi+goeD14eD1TAxASI5YwSixD0/yYWbk6cgb04xaJHjP/v/2JnriZKsTLeNF+9cWLQxGLxft58V/iNvl0PY1n4lax+NF+EB2GPRIu1+xWkR2vST1Hx65ZzikWWdbLSKnfFaacVr0byFSwh+IDCxmOLT4AqZ8wXASKuMPmMHrpnyBQPC6GbwekC98Pvr9cCCgQMCUPyD5TdM6D71GnptWmZHzMfcDpgKmqYCp8Lk59twcKRMIRDsPvg+dB6w/gqasPz6mzPAfodA1Ba9Z7yPKBs9ljvxRC9XDjPiMxrwPlQGSId/l0K4fLE/qzyR8IPsE/NYCZrv+ywoiB7ePvu8ut/aUqWmWahdZ59NPk/IK0lNfIIVMM3qwCQT/53xskIksEwo1I+XGBymZGv0+oTolUEbj/9/7qG6riLujr4/9UEJVGv3gBItF/dHG2LdRWiUSqFO0f0dR//0mdinhnxfrd1NfURj9xnEifCD7dbVJHwRbRHZvkPze8WUMp1R9ihVEaheNBJOSGfbXFwCyHOEDucU3JB3eJXVslw6+K3W8I3W8Kw0cjV6+pCaihSR4VJ2csrVGACAXTObvt8umOgGp48oPtm40j1wzTanngBVIQsfBd60N8HoPWsdHv4/4GQXSjAXWz5ix0NqRt6JRKm+QCgjBAJBMhA9kJ8OQyuqt45SIEd1DfdLB96wxIx3brRaSgzuk4T6pfat1jFVQYQWRsUd5g/Vq86JoADDVET6QW/KLpYazrCMkEJCOtYy0kBzeaY0p6Wq1um4Gu6SOLqs7Jxp3uVTRED2YVDRaK7gyEwcAwhjzAUzE22MFke5gGOnaG3wNvo+c/huLq8Da1bd4ujXYdex5+NoMK6gkYcM9ALAbYz6AZHGXSjULrCOaob6IcBIKJhHhpK9T8g1a97vb4j/P4ZKKqqWS6SOBpCT4WjzdOi+YZnX1FFRY41Gm4KqvAHIb4QM4EfnF0oz51hHN8EBwgOshK4j0HRo57+2U+g6PnA92SQGf1NthHQnXoSQYRMojQknEeWHwfbRzVwFdQgBsR/gAUimvUJo2xzri8Q1Z4SR09HYGA8vh0ecDXdbmfEM91ueGeq3Ds2/y9XPkSe4SKb/UClLuEivMhK6Nej/m3F06/p4rf/J1AJBzCB9ApnDlS+UzrSMRfp8VQga7rGMg+DrYHf08FFpC182AtXfOwDHrSAaHS8ortkJXfpGUV2Sd50Wcj7oeLJtXaIWfUdeKJJfbOne5rVaa0OHkf7qAqYxvMDBVOV1ScZV1TFYgYLWWeD2SN9hy4u0JvvaOtKZ4I197rDEu0cqHVpgN+CRvt3WkkuGMCCUR4SQvFFAirofKOd3B627JmR9xLT8YaCa6lj/yjMhrdFkBx4XwAeQih8MarJqsBdT8w1YYGR4IHv0jx1D/+GvDA8HroXt9I58dijj3DY4c/qGR55n+kYCUTo68YBjJt15HHXkjIcWZZ4UaZ97I/XGfyRt5Df1cp2vkviPiPN51R17w5+WNnLOCLzII4QPAiXPmSUWVqX1GIBARRrySb8B6HQ6+RgaVmNeHrFYan9cKM6OuhV6jXQuWDQyPqdOwdQz3pfafPRkMR0QQcUUEHddI8AmdRw0vkfdcMa5P9Jkx7x3O8eEosty457kIUlmE8AFganA4rPEi+UXpq0MgYIUWv9dq7QmFGP9w8NpQxPXh4Psxhy/GtcBw8DPBzwUizo/n+tjdTc1AsI5RNmGccowYQSYytLjGh5xR5caEGodrTMiJdi8yQEUJS6Oe7Zr43oTvsz9cET4AIFEOh+QIji3JZKYpBfwjgSTgGwklkSFnMucBX8R7X8T1sWV9MT4T+d43+l7AN/K5secBX7R/wJFWp6xkjA8j0QLShO/jhBxnvvSZH6btn5DwAQDZxjCC4z9c1oDbqcw0x4SVyAAzbIWscSEmMgD5I+6NDT6+6O+jno99ln/0vcjnjq3H2LKR701/tH/o1Icrp5vwAQBAVIYxMpZkqgepaAIBK4BMFIZS8V7pnalF+AAAIF0cDkmOnNsmgR2sAACArQgfAADAVoQPAABgK8IHAACwFeEDAADYivABAABslbLw8cADD6ipqUkFBQVasmSJXn/99VQ9CgAATCEpCR9PPfWUbr/9dt155516++23dcEFF2j58uVqbW1NxeMAAMAUYpimacYvNjnnnHOOzjzzTD344IPha6eddpquvvpqrVu3bsLPejwelZeXq7u7W2VlSdruGwAApNRk/n4nveVjaGhIW7Zs0bJly0ZdX7ZsmTZu3JjsxwEAgCkm6curHz58WH6/XzU1NaOu19TUqKOjY1x5r9crr3dki2ePx5PsKgEAgAySsgGnhjF60xrTNMddk6R169apvLw8fDQ0NKSqSgAAIAMkPXxUV1fL6XSOa+Xo7Owc1xoiSatXr1Z3d3f4aGtrS3aVAABABkl6t0t+fr6WLFmi9evX65prrglfX79+va666qpx5d1ut9xud/h9aPwr3S8AAEwdob/bicxjSXr4kKRVq1bppptu0tKlS3Xuuefq4YcfVmtrq2655Za4n+3p6ZEkul8AAJiCenp6VF5ePmGZlISP6667TkeOHNH3v/99HThwQM3NzXrhhRc0e/bsuJ+tr69XW1ubSktLo44ROREej0cNDQ1qa2tjGm8a8XvIDPweMgO/h8zA7+HEmaapnp4e1dfXxy2bknU+MhVriGQGfg+Zgd9DZuD3kBn4PdiLvV0AAICtCB8AAMBWORU+3G637rrrrlGza2A/fg+Zgd9DZuD3kBn4Pdgrp8Z8AACA9Muplg8AAJB+hA8AAGArwgcAALAV4QMAANgqZ8LHAw88oKamJhUUFGjJkiV6/fXX012lnLNmzRoZhjHqqK2tTXe1st5rr72mK6+8UvX19TIMQ88+++yo+6Zpas2aNaqvr1dhYaEuvvhi7dixIz2VzWLxfg9f/vKXx30/Pv7xj6ensllq3bp1Ouuss1RaWqoZM2bo6quv1s6dO0eV4ftgj5wIH0899ZRuv/123XnnnXr77bd1wQUXaPny5WptbU131XLOwoULdeDAgfCxffv2dFcp6/X19en000/X/fffH/X+j370I9177726//77tWnTJtXW1urSSy8N77OE5Ij3e5Ckz372s6O+Hy+88IKNNcx+GzZs0MqVK/Xmm29q/fr18vl8WrZsmfr6+sJl+D7YxMwBZ599tnnLLbeMujZ//nzzjjvuSFONctNdd91lnn766emuRk6TZD7zzDPh94FAwKytrTX/6Z/+KXxtcHDQLC8vNx966KE01DA3jP09mKZprlixwrzqqqvSUp9c1dnZaUoyN2zYYJom3wc7ZX3Lx9DQkLZs2aJly5aNur5s2TJt3LgxTbXKXR988IHq6+vV1NSk66+/Xrt37053lXJaS0uLOjo6Rn0/3G63LrroIr4fafDqq69qxowZOuWUU/SVr3xFnZ2d6a5SVuvu7pYkVVZWSuL7YKesDx+HDx+W3+9XTU3NqOs1NTXq6OhIU61y0znnnKPHH39cL730kh555BF1dHTovPPO05EjR9JdtZwV+g7w/Ui/5cuX65e//KVefvll/fjHP9amTZv0qU99Sl6vN91Vy0qmaWrVqlU6//zz1dzcLInvg51c6a6AXQzDGPXeNM1x15Bay5cvD58vWrRI5557rk466SQ99thjWrVqVRprBr4f6XfdddeFz5ubm7V06VLNnj1bzz//vK699to01iw73XrrrXrnnXf0xhtvjLvH9yH1sr7lo7q6Wk6nc1xq7ezsHJduYa/i4mItWrRIH3zwQbqrkrNCs434fmSeuro6zZ49m+9HCnzta1/Tc889p1deeUWzZs0KX+f7YJ+sDx/5+flasmSJ1q9fP+r6+vXrdd5556WpVpAkr9er999/X3V1demuSs5qampSbW3tqO/H0NCQNmzYwPcjzY4cOaK2tja+H0lkmqZuvfVWPf3003r55ZfV1NQ06j7fB/vkRLfLqlWrdNNNN2np0qU699xz9fDDD6u1tVW33HJLuquWU775zW/qyiuvVGNjozo7O/WDH/xAHo9HK1asSHfVslpvb68+/PDD8PuWlhZt27ZNlZWVamxs1O233661a9dq3rx5mjdvntauXauioiLdcMMNaax19pno91BZWak1a9boc5/7nOrq6rRnzx595zvfUXV1ta655po01jq7rFy5Uk8++aR+85vfqLS0NNzCUV5ersLCQhmGwffBLmmda2Ojn//85+bs2bPN/Px888wzzwxPrYJ9rrvuOrOurs7My8sz6+vrzWuvvdbcsWNHuquV9V555RVT0rhjxYoVpmla0wvvuusus7a21nS73eaFF15obt++Pb2VzkIT/R76+/vNZcuWmdOnTzfz8vLMxsZGc8WKFWZra2u6q51Vov37l2Q++uij4TJ8H+xhmKZp2h95AABArsr6MR8AACCzED4AAICtCB8AAMBWhA8AAGArwgcAALAV4QMAANiK8AEAAGxF+AAAALYifAAAAFsRPgAAgK0IHwAAwFaEDwAAYKv/C9D4OZHvAIPDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=epoch_list, y=val_mse)\n",
    "sns.lineplot(x=epoch_list, y=train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data, param_grid, get_model_function, nb_users, nb_movies, validation_size = 0.1):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    score_list = []\n",
    "    param_list = []\n",
    "    model_list = []\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    # Here we will make a loop for each parameters tested\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        for k, lambda_ in product(ks, lambdas_):\n",
    "            print(\"Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\")\n",
    "            print(\"Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\")\n",
    "            print(f\"TESTING... {k} | {lambda_}\")\n",
    "            print(\"\")\n",
    "            model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, \n",
    "                y_train, \n",
    "                epochs=500, \n",
    "                batch_size=512, \n",
    "                validation_split = 0.1,\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "            \n",
    "            \n",
    "            val_mse = history.history['val_mse']\n",
    "            min_mse = min(val_mse)\n",
    "            \n",
    "            score_list.append(min(val_mse))\n",
    "            param_list.append((k, lambda_))\n",
    "            model_list.append(model)\n",
    "            \n",
    "            print(f\"k={k}, lambda_={lambda_}, val_mse={val_mse}\")\n",
    "        \n",
    "    min_index = score_list.index(min(score_list))\n",
    "\n",
    "    best_score = score_list[min_index]\n",
    "    best_params = {'k': param_list[min_index][0], 'lambda':param_list[min_index][1]}\n",
    "    best_model = model_list[min_index]\n",
    "    \n",
    "    \n",
    "\n",
    "    return best_params, best_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambdas_ = [0.0002, 0.00005, 0.00002]\n",
    "ks = [15,30]\n",
    "\n",
    "param_grid = dict(k=ks, lambda_=lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 15 | 0.0002\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 12.7319 - mse: 12.7227 - val_loss: 12.0811 - val_mse: 12.0698\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.8042 - mse: 10.7461 - val_loss: 9.0716 - val_mse: 8.9275\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.0840 - mse: 6.7975 - val_loss: 5.4609 - val_mse: 5.0188\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.3885 - mse: 3.8027 - val_loss: 3.9020 - val_mse: 3.1865\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.3896 - mse: 2.5797 - val_loss: 3.3951 - val_mse: 2.5026\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.0069 - mse: 2.0543 - val_loss: 3.1645 - val_mse: 2.1570\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8098 - mse: 1.7601 - val_loss: 3.0338 - val_mse: 1.9455\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6915 - mse: 1.5730 - val_loss: 2.9502 - val_mse: 1.8040\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6134 - mse: 1.4450 - val_loss: 2.8935 - val_mse: 1.7051\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5573 - mse: 1.3535 - val_loss: 2.8477 - val_mse: 1.6295\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5140 - mse: 1.2841 - val_loss: 2.8124 - val_mse: 1.5739\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4787 - mse: 1.2328 - val_loss: 2.7794 - val_mse: 1.5270\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4484 - mse: 1.1915 - val_loss: 2.7503 - val_mse: 1.4907\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4214 - mse: 1.1596 - val_loss: 2.7223 - val_mse: 1.4585\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3964 - mse: 1.1331 - val_loss: 2.6953 - val_mse: 1.4319\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3727 - mse: 1.1108 - val_loss: 2.6692 - val_mse: 1.4092\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3505 - mse: 1.0926 - val_loss: 2.6415 - val_mse: 1.3864\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3286 - mse: 1.0776 - val_loss: 2.6161 - val_mse: 1.3685\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3074 - mse: 1.0639 - val_loss: 2.5899 - val_mse: 1.3512\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2866 - mse: 1.0521 - val_loss: 2.5632 - val_mse: 1.3343\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2660 - mse: 1.0427 - val_loss: 2.5387 - val_mse: 1.3197\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2457 - mse: 1.0328 - val_loss: 2.5124 - val_mse: 1.3051\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2257 - mse: 1.0249 - val_loss: 2.4862 - val_mse: 1.2908\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2059 - mse: 1.0162 - val_loss: 2.4610 - val_mse: 1.2781\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1863 - mse: 1.0097 - val_loss: 2.4353 - val_mse: 1.2644\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1672 - mse: 1.0027 - val_loss: 2.4102 - val_mse: 1.2519\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1482 - mse: 0.9965 - val_loss: 2.3861 - val_mse: 1.2398\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1295 - mse: 0.9897 - val_loss: 2.3619 - val_mse: 1.2289\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1113 - mse: 0.9845 - val_loss: 2.3393 - val_mse: 1.2179\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0932 - mse: 0.9779 - val_loss: 2.3157 - val_mse: 1.2075\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0759 - mse: 0.9734 - val_loss: 2.2932 - val_mse: 1.1965\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0586 - mse: 0.9677 - val_loss: 2.2710 - val_mse: 1.1861\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0419 - mse: 0.9627 - val_loss: 2.2504 - val_mse: 1.1763\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0255 - mse: 0.9571 - val_loss: 2.2289 - val_mse: 1.1659\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0095 - mse: 0.9515 - val_loss: 2.2091 - val_mse: 1.1569\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9942 - mse: 0.9462 - val_loss: 2.1914 - val_mse: 1.1490\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9793 - mse: 0.9416 - val_loss: 2.1719 - val_mse: 1.1388\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9646 - mse: 0.9362 - val_loss: 2.1545 - val_mse: 1.1309\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9506 - mse: 0.9311 - val_loss: 2.1379 - val_mse: 1.1228\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9370 - mse: 0.9260 - val_loss: 2.1221 - val_mse: 1.1154\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9239 - mse: 0.9206 - val_loss: 2.1054 - val_mse: 1.1064\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9111 - mse: 0.9153 - val_loss: 2.0906 - val_mse: 1.0989\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8989 - mse: 0.9111 - val_loss: 2.0768 - val_mse: 1.0913\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8870 - mse: 0.9046 - val_loss: 2.0638 - val_mse: 1.0849\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8756 - mse: 0.9001 - val_loss: 2.0506 - val_mse: 1.0783\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8647 - mse: 0.8949 - val_loss: 2.0389 - val_mse: 1.0716\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8543 - mse: 0.8898 - val_loss: 2.0264 - val_mse: 1.0640\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8441 - mse: 0.8841 - val_loss: 2.0155 - val_mse: 1.0580\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8344 - mse: 0.8791 - val_loss: 2.0047 - val_mse: 1.0518\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8252 - mse: 0.8743 - val_loss: 1.9955 - val_mse: 1.0460\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8165 - mse: 0.8686 - val_loss: 1.9865 - val_mse: 1.0409\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8081 - mse: 0.8636 - val_loss: 1.9777 - val_mse: 1.0354\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8003 - mse: 0.8597 - val_loss: 1.9690 - val_mse: 1.0298\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7927 - mse: 0.8548 - val_loss: 1.9619 - val_mse: 1.0254\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7857 - mse: 0.8507 - val_loss: 1.9548 - val_mse: 1.0205\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7789 - mse: 0.8456 - val_loss: 1.9480 - val_mse: 1.0166\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7725 - mse: 0.8417 - val_loss: 1.9405 - val_mse: 1.0114\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7666 - mse: 0.8380 - val_loss: 1.9351 - val_mse: 1.0078\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7609 - mse: 0.8342 - val_loss: 1.9293 - val_mse: 1.0037\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7555 - mse: 0.8306 - val_loss: 1.9237 - val_mse: 1.0003\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7504 - mse: 0.8272 - val_loss: 1.9180 - val_mse: 0.9955\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7456 - mse: 0.8238 - val_loss: 1.9135 - val_mse: 0.9925\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7411 - mse: 0.8203 - val_loss: 1.9088 - val_mse: 0.9886\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7368 - mse: 0.8174 - val_loss: 1.9048 - val_mse: 0.9860\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7327 - mse: 0.8145 - val_loss: 1.9008 - val_mse: 0.9831\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7287 - mse: 0.8109 - val_loss: 1.8969 - val_mse: 0.9801\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7251 - mse: 0.8086 - val_loss: 1.8935 - val_mse: 0.9773\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7216 - mse: 0.8050 - val_loss: 1.8905 - val_mse: 0.9751\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7183 - mse: 0.8033 - val_loss: 1.8870 - val_mse: 0.9720\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7151 - mse: 0.8000 - val_loss: 1.8840 - val_mse: 0.9697\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7120 - mse: 0.7979 - val_loss: 1.8815 - val_mse: 0.9674\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7092 - mse: 0.7949 - val_loss: 1.8787 - val_mse: 0.9650\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7064 - mse: 0.7927 - val_loss: 1.8764 - val_mse: 0.9628\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7040 - mse: 0.7898 - val_loss: 1.8742 - val_mse: 0.9610\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7014 - mse: 0.7879 - val_loss: 1.8721 - val_mse: 0.9590\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6990 - mse: 0.7857 - val_loss: 1.8698 - val_mse: 0.9567\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6968 - mse: 0.7834 - val_loss: 1.8682 - val_mse: 0.9549\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6947 - mse: 0.7806 - val_loss: 1.8663 - val_mse: 0.9525\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6926 - mse: 0.7788 - val_loss: 1.8650 - val_mse: 0.9513\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6906 - mse: 0.7765 - val_loss: 1.8639 - val_mse: 0.9496\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6887 - mse: 0.7741 - val_loss: 1.8623 - val_mse: 0.9481\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6869 - mse: 0.7725 - val_loss: 1.8611 - val_mse: 0.9463\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6852 - mse: 0.7701 - val_loss: 1.8603 - val_mse: 0.9451\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6836 - mse: 0.7679 - val_loss: 1.8594 - val_mse: 0.9437\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6821 - mse: 0.7660 - val_loss: 1.8583 - val_mse: 0.9424\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6806 - mse: 0.7642 - val_loss: 1.8576 - val_mse: 0.9411\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6792 - mse: 0.7622 - val_loss: 1.8573 - val_mse: 0.9402\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6778 - mse: 0.7603 - val_loss: 1.8560 - val_mse: 0.9384\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6765 - mse: 0.7587 - val_loss: 1.8554 - val_mse: 0.9373\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6753 - mse: 0.7564 - val_loss: 1.8549 - val_mse: 0.9362\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6740 - mse: 0.7547 - val_loss: 1.8550 - val_mse: 0.9355\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6729 - mse: 0.7529 - val_loss: 1.8543 - val_mse: 0.9345\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6718 - mse: 0.7514 - val_loss: 1.8536 - val_mse: 0.9331\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6707 - mse: 0.7496 - val_loss: 1.8532 - val_mse: 0.9319\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6698 - mse: 0.7479 - val_loss: 1.8532 - val_mse: 0.9311\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6689 - mse: 0.7461 - val_loss: 1.8531 - val_mse: 0.9306\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6679 - mse: 0.7449 - val_loss: 1.8528 - val_mse: 0.9296\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6671 - mse: 0.7434 - val_loss: 1.8527 - val_mse: 0.9286\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6663 - mse: 0.7418 - val_loss: 1.8522 - val_mse: 0.9274\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6653 - mse: 0.7399 - val_loss: 1.8522 - val_mse: 0.9267\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6647 - mse: 0.7384 - val_loss: 1.8522 - val_mse: 0.9260\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6640 - mse: 0.7370 - val_loss: 1.8521 - val_mse: 0.9251\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6632 - mse: 0.7363 - val_loss: 1.8527 - val_mse: 0.9248\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6626 - mse: 0.7345 - val_loss: 1.8523 - val_mse: 0.9237\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6619 - mse: 0.7332 - val_loss: 1.8526 - val_mse: 0.9235\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6613 - mse: 0.7318 - val_loss: 1.8528 - val_mse: 0.9230\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6607 - mse: 0.7305 - val_loss: 1.8527 - val_mse: 0.9221\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6602 - mse: 0.7291 - val_loss: 1.8526 - val_mse: 0.9215\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6596 - mse: 0.7278 - val_loss: 1.8531 - val_mse: 0.9213\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6591 - mse: 0.7270 - val_loss: 1.8535 - val_mse: 0.9208\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6586 - mse: 0.7254 - val_loss: 1.8532 - val_mse: 0.9197\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6581 - mse: 0.7238 - val_loss: 1.8535 - val_mse: 0.9193\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6577 - mse: 0.7234 - val_loss: 1.8535 - val_mse: 0.9186\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6572 - mse: 0.7218 - val_loss: 1.8538 - val_mse: 0.9182\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6569 - mse: 0.7208 - val_loss: 1.8537 - val_mse: 0.9177\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6565 - mse: 0.7199 - val_loss: 1.8544 - val_mse: 0.9176\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6561 - mse: 0.7188 - val_loss: 1.8546 - val_mse: 0.9171\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6557 - mse: 0.7173 - val_loss: 1.8550 - val_mse: 0.9172\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6554 - mse: 0.7167 - val_loss: 1.8549 - val_mse: 0.9165\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6549 - mse: 0.7159 - val_loss: 1.8552 - val_mse: 0.9160\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6547 - mse: 0.7149 - val_loss: 1.8556 - val_mse: 0.9158\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6544 - mse: 0.7141 - val_loss: 1.8560 - val_mse: 0.9154\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6542 - mse: 0.7133 - val_loss: 1.8555 - val_mse: 0.9148\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6538 - mse: 0.7124 - val_loss: 1.8561 - val_mse: 0.9148\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6536 - mse: 0.7116 - val_loss: 1.8561 - val_mse: 0.9141\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6533 - mse: 0.7110 - val_loss: 1.8566 - val_mse: 0.9140\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6530 - mse: 0.7095 - val_loss: 1.8570 - val_mse: 0.9139\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6529 - mse: 0.7092 - val_loss: 1.8576 - val_mse: 0.9139\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6526 - mse: 0.7084 - val_loss: 1.8576 - val_mse: 0.9134\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6523 - mse: 0.7076 - val_loss: 1.8578 - val_mse: 0.9129\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6522 - mse: 0.7066 - val_loss: 1.8581 - val_mse: 0.9127\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6520 - mse: 0.7062 - val_loss: 1.8582 - val_mse: 0.9125\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6518 - mse: 0.7052 - val_loss: 1.8582 - val_mse: 0.9119\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6515 - mse: 0.7050 - val_loss: 1.8585 - val_mse: 0.9118\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6514 - mse: 0.7040 - val_loss: 1.8589 - val_mse: 0.9119\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6513 - mse: 0.7037 - val_loss: 1.8593 - val_mse: 0.9115\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6511 - mse: 0.7030 - val_loss: 1.8595 - val_mse: 0.9113\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6509 - mse: 0.7023 - val_loss: 1.8599 - val_mse: 0.9113\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6508 - mse: 0.7017 - val_loss: 1.8599 - val_mse: 0.9111\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6506 - mse: 0.7010 - val_loss: 1.8601 - val_mse: 0.9110\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6505 - mse: 0.7007 - val_loss: 1.8603 - val_mse: 0.9105\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6504 - mse: 0.7002 - val_loss: 1.8606 - val_mse: 0.9103\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6503 - mse: 0.6999 - val_loss: 1.8607 - val_mse: 0.9101\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6502 - mse: 0.6992 - val_loss: 1.8608 - val_mse: 0.9100\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6500 - mse: 0.6985 - val_loss: 1.8613 - val_mse: 0.9099\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6499 - mse: 0.6980 - val_loss: 1.8617 - val_mse: 0.9101\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6498 - mse: 0.6977 - val_loss: 1.8617 - val_mse: 0.9097\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6497 - mse: 0.6974 - val_loss: 1.8617 - val_mse: 0.9093\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6496 - mse: 0.6971 - val_loss: 1.8623 - val_mse: 0.9097\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6495 - mse: 0.6967 - val_loss: 1.8620 - val_mse: 0.9091\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6494 - mse: 0.6957 - val_loss: 1.8625 - val_mse: 0.9091\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6493 - mse: 0.6954 - val_loss: 1.8626 - val_mse: 0.9090\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6492 - mse: 0.6950 - val_loss: 1.8627 - val_mse: 0.9088\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6491 - mse: 0.6951 - val_loss: 1.8630 - val_mse: 0.9088\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6490 - mse: 0.6942 - val_loss: 1.8635 - val_mse: 0.9088\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6489 - mse: 0.6939 - val_loss: 1.8638 - val_mse: 0.9088\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6488 - mse: 0.6935 - val_loss: 1.8637 - val_mse: 0.9081\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6488 - mse: 0.6929 - val_loss: 1.8639 - val_mse: 0.9082\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6487 - mse: 0.6926 - val_loss: 1.8642 - val_mse: 0.9083\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6486 - mse: 0.6924 - val_loss: 1.8646 - val_mse: 0.9085\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6486 - mse: 0.6920 - val_loss: 1.8644 - val_mse: 0.9080\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6484 - mse: 0.6913 - val_loss: 1.8645 - val_mse: 0.9078\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6485 - mse: 0.6921 - val_loss: 1.8648 - val_mse: 0.9080\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6484 - mse: 0.6910 - val_loss: 1.8649 - val_mse: 0.9078\n",
      "Epoch 165/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6483 - mse: 0.6907 - val_loss: 1.8653 - val_mse: 0.9076\n",
      "Epoch 166/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6483 - mse: 0.6904 - val_loss: 1.8654 - val_mse: 0.9076\n",
      "Epoch 167/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6481 - mse: 0.6901 - val_loss: 1.8656 - val_mse: 0.9076\n",
      "Epoch 168/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6481 - mse: 0.6899 - val_loss: 1.8658 - val_mse: 0.9074\n",
      "Epoch 169/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6481 - mse: 0.6896 - val_loss: 1.8659 - val_mse: 0.9074\n",
      "Epoch 170/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6480 - mse: 0.6888 - val_loss: 1.8660 - val_mse: 0.9074\n",
      "Epoch 171/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6480 - mse: 0.6890 - val_loss: 1.8659 - val_mse: 0.9072\n",
      "Epoch 172/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6479 - mse: 0.6886 - val_loss: 1.8664 - val_mse: 0.9073\n",
      "Epoch 173/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6479 - mse: 0.6884 - val_loss: 1.8665 - val_mse: 0.9071\n",
      "Epoch 174/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6478 - mse: 0.6883 - val_loss: 1.8666 - val_mse: 0.9070\n",
      "Epoch 175/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6478 - mse: 0.6875 - val_loss: 1.8667 - val_mse: 0.9068\n",
      "Epoch 176/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6477 - mse: 0.6878 - val_loss: 1.8667 - val_mse: 0.9068\n",
      "Epoch 177/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6477 - mse: 0.6877 - val_loss: 1.8667 - val_mse: 0.9066\n",
      "Epoch 178/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6477 - mse: 0.6873 - val_loss: 1.8675 - val_mse: 0.9070\n",
      "Epoch 179/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6476 - mse: 0.6874 - val_loss: 1.8671 - val_mse: 0.9067\n",
      "Epoch 180/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6476 - mse: 0.6868 - val_loss: 1.8674 - val_mse: 0.9066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6476 - mse: 0.6865 - val_loss: 1.8678 - val_mse: 0.9069\n",
      "Epoch 182/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6475 - mse: 0.6864 - val_loss: 1.8677 - val_mse: 0.9066\n",
      "Epoch 183/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6474 - mse: 0.6859 - val_loss: 1.8677 - val_mse: 0.9064\n",
      "Epoch 184/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6474 - mse: 0.6854 - val_loss: 1.8682 - val_mse: 0.9070\n",
      "Epoch 185/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6473 - mse: 0.6858 - val_loss: 1.8682 - val_mse: 0.9067\n",
      "Epoch 186/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6474 - mse: 0.6854 - val_loss: 1.8682 - val_mse: 0.9063\n",
      "Epoch 187/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6474 - mse: 0.6855 - val_loss: 1.8681 - val_mse: 0.9064\n",
      "Epoch 188/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6473 - mse: 0.6852 - val_loss: 1.8683 - val_mse: 0.9065\n",
      "Epoch 189/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6472 - mse: 0.6848 - val_loss: 1.8685 - val_mse: 0.9062\n",
      "Epoch 190/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6472 - mse: 0.6846 - val_loss: 1.8685 - val_mse: 0.9062\n",
      "Epoch 191/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6472 - mse: 0.6848 - val_loss: 1.8687 - val_mse: 0.9060\n",
      "Epoch 192/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6472 - mse: 0.6845 - val_loss: 1.8687 - val_mse: 0.9056\n",
      "Epoch 193/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6471 - mse: 0.6842 - val_loss: 1.8690 - val_mse: 0.9061\n",
      "Epoch 194/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6471 - mse: 0.6837 - val_loss: 1.8691 - val_mse: 0.9061\n",
      "Epoch 195/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6471 - mse: 0.6837 - val_loss: 1.8690 - val_mse: 0.9057\n",
      "Epoch 196/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6470 - mse: 0.6833 - val_loss: 1.8692 - val_mse: 0.9058\n",
      "Epoch 197/500\n",
      "141/160 [=========================>....] - ETA: 0s - loss: 1.6440 - mse: 0.6801Restoring model weights from the end of the best epoch: 192.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6471 - mse: 0.6833 - val_loss: 1.8691 - val_mse: 0.9059\n",
      "Epoch 197: early stopping\n",
      "k=15, lambda_=0.0002, val_mse=[12.06982421875, 8.927496910095215, 5.018836975097656, 3.1865251064300537, 2.502556324005127, 2.1569972038269043, 1.94548761844635, 1.8040469884872437, 1.7051278352737427, 1.6294658184051514, 1.5738630294799805, 1.5270427465438843, 1.4907230138778687, 1.4585261344909668, 1.431902527809143, 1.4091697931289673, 1.3864185810089111, 1.3685489892959595, 1.3512097597122192, 1.334291696548462, 1.3196758031845093, 1.3051444292068481, 1.2907698154449463, 1.2780835628509521, 1.2644091844558716, 1.2518603801727295, 1.2397774457931519, 1.2288784980773926, 1.2178875207901, 1.2075423002243042, 1.196520447731018, 1.1861088275909424, 1.1762689352035522, 1.165850281715393, 1.156869888305664, 1.1490031480789185, 1.138767957687378, 1.1309049129486084, 1.1227972507476807, 1.1154009103775024, 1.1064257621765137, 1.098885178565979, 1.0913053750991821, 1.0849437713623047, 1.0783110857009888, 1.0716078281402588, 1.0639528036117554, 1.0579913854599, 1.0517807006835938, 1.0460025072097778, 1.0409090518951416, 1.0353742837905884, 1.0298188924789429, 1.0254433155059814, 1.020538568496704, 1.016640067100525, 1.0113744735717773, 1.0077857971191406, 1.0037106275558472, 1.0003303289413452, 0.9955322742462158, 0.9925079345703125, 0.9885687232017517, 0.9859911203384399, 0.9831307530403137, 0.9801218509674072, 0.9772700667381287, 0.9750660061836243, 0.9719762206077576, 0.9697323441505432, 0.9673908352851868, 0.9649516940116882, 0.9628411531448364, 0.9609968066215515, 0.9590040445327759, 0.9566746950149536, 0.9548612236976624, 0.9524508714675903, 0.9512689113616943, 0.9495949149131775, 0.9481469392776489, 0.9463343620300293, 0.9450552463531494, 0.9436674118041992, 0.9423693418502808, 0.9411041140556335, 0.9401819109916687, 0.9384335279464722, 0.9373167753219604, 0.9361749291419983, 0.9355303049087524, 0.9345149993896484, 0.9330811500549316, 0.9319387674331665, 0.9311298727989197, 0.9305523633956909, 0.9295944571495056, 0.928596556186676, 0.9274051189422607, 0.9266529083251953, 0.926010012626648, 0.9251363277435303, 0.9248486757278442, 0.923676609992981, 0.9235174655914307, 0.9229705929756165, 0.9221245050430298, 0.9215219616889954, 0.9213463664054871, 0.9207998514175415, 0.9196619987487793, 0.9192856550216675, 0.9186482429504395, 0.9181531667709351, 0.9177414178848267, 0.9175796508789062, 0.9170952439308167, 0.91717928647995, 0.9165202379226685, 0.915974497795105, 0.9157649278640747, 0.9153524041175842, 0.914754331111908, 0.9147911667823792, 0.9141096472740173, 0.9140450954437256, 0.9138982892036438, 0.9139071106910706, 0.9133999943733215, 0.9128977656364441, 0.91274493932724, 0.9124554395675659, 0.9118996858596802, 0.9118266105651855, 0.9118828773498535, 0.9115238189697266, 0.9112577438354492, 0.9112787246704102, 0.9111052751541138, 0.910960853099823, 0.9105228185653687, 0.910327136516571, 0.9101057648658752, 0.9099625945091248, 0.9098601341247559, 0.9101038575172424, 0.9096606373786926, 0.9093438982963562, 0.9097025990486145, 0.9091249108314514, 0.9091399908065796, 0.908989667892456, 0.908778965473175, 0.9087970852851868, 0.9087738394737244, 0.9087873697280884, 0.9081236124038696, 0.9081786870956421, 0.9083003997802734, 0.9084718227386475, 0.9080248475074768, 0.9078311324119568, 0.9079668521881104, 0.9077976942062378, 0.9075775742530823, 0.9075670838356018, 0.9075788259506226, 0.9074108004570007, 0.9073914885520935, 0.9073823690414429, 0.9071722030639648, 0.9072785377502441, 0.9070643186569214, 0.9070175886154175, 0.9068020582199097, 0.9068350195884705, 0.9066171050071716, 0.9070390462875366, 0.9066682457923889, 0.906591534614563, 0.9068793654441833, 0.9065534472465515, 0.9063994288444519, 0.9070073962211609, 0.906737208366394, 0.9062903523445129, 0.9064333438873291, 0.9064560532569885, 0.9061802625656128, 0.9062474966049194, 0.9060109257698059, 0.9056389331817627, 0.9061213135719299, 0.9061217308044434, 0.9057225584983826, 0.9058353304862976, 0.9058505892753601]\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 15 | 5e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 12.7300 - mse: 12.7270 - val_loss: 12.0963 - val_mse: 12.0928\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.9096 - mse: 10.8955 - val_loss: 9.1816 - val_mse: 9.1466\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 6.9729 - mse: 6.8992 - val_loss: 5.0939 - val_mse: 4.9763\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.8014 - mse: 3.6411 - val_loss: 3.1630 - val_mse: 2.9627\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5470 - mse: 2.3158 - val_loss: 2.5055 - val_mse: 2.2467\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0471 - mse: 1.7665 - val_loss: 2.1940 - val_mse: 1.8930\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7788 - mse: 1.4607 - val_loss: 2.0105 - val_mse: 1.6761\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6128 - mse: 1.2643 - val_loss: 1.8964 - val_mse: 1.5346\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5021 - mse: 1.1288 - val_loss: 1.8182 - val_mse: 1.4337\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4242 - mse: 1.0299 - val_loss: 1.7639 - val_mse: 1.3602\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3675 - mse: 0.9557 - val_loss: 1.7246 - val_mse: 1.3047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3244 - mse: 0.8975 - val_loss: 1.6960 - val_mse: 1.2626\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2916 - mse: 0.8525 - val_loss: 1.6733 - val_mse: 1.2287\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2655 - mse: 0.8160 - val_loss: 1.6557 - val_mse: 1.2017\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2446 - mse: 0.7867 - val_loss: 1.6412 - val_mse: 1.1794\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2271 - mse: 0.7621 - val_loss: 1.6304 - val_mse: 1.1625\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2127 - mse: 0.7423 - val_loss: 1.6203 - val_mse: 1.1474\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2004 - mse: 0.7255 - val_loss: 1.6114 - val_mse: 1.1344\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1897 - mse: 0.7113 - val_loss: 1.6039 - val_mse: 1.1240\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1802 - mse: 0.6992 - val_loss: 1.5968 - val_mse: 1.1146\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1715 - mse: 0.6884 - val_loss: 1.5903 - val_mse: 1.1065\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1640 - mse: 0.6794 - val_loss: 1.5830 - val_mse: 1.0982\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1568 - mse: 0.6719 - val_loss: 1.5767 - val_mse: 1.0914\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1500 - mse: 0.6646 - val_loss: 1.5712 - val_mse: 1.0857\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1439 - mse: 0.6586 - val_loss: 1.5648 - val_mse: 1.0796\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1376 - mse: 0.6527 - val_loss: 1.5578 - val_mse: 1.0732\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1319 - mse: 0.6479 - val_loss: 1.5523 - val_mse: 1.0687\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1262 - mse: 0.6430 - val_loss: 1.5467 - val_mse: 1.0639\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1209 - mse: 0.6389 - val_loss: 1.5401 - val_mse: 1.0585\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1153 - mse: 0.6345 - val_loss: 1.5345 - val_mse: 1.0546\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1100 - mse: 0.6308 - val_loss: 1.5283 - val_mse: 1.0498\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1048 - mse: 0.6274 - val_loss: 1.5227 - val_mse: 1.0458\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0994 - mse: 0.6235 - val_loss: 1.5145 - val_mse: 1.0393\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0944 - mse: 0.6201 - val_loss: 1.5103 - val_mse: 1.0367\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0891 - mse: 0.6165 - val_loss: 1.5034 - val_mse: 1.0315\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0836 - mse: 0.6128 - val_loss: 1.4978 - val_mse: 1.0275\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0783 - mse: 0.6090 - val_loss: 1.4923 - val_mse: 1.0240\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0727 - mse: 0.6054 - val_loss: 1.4860 - val_mse: 1.0192\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0670 - mse: 0.6011 - val_loss: 1.4804 - val_mse: 1.0154\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0615 - mse: 0.5976 - val_loss: 1.4759 - val_mse: 1.0126\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0556 - mse: 0.5933 - val_loss: 1.4695 - val_mse: 1.0076\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0497 - mse: 0.5886 - val_loss: 1.4637 - val_mse: 1.0035\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0435 - mse: 0.5843 - val_loss: 1.4596 - val_mse: 1.0007\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0371 - mse: 0.5790 - val_loss: 1.4535 - val_mse: 0.9961\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0308 - mse: 0.5742 - val_loss: 1.4489 - val_mse: 0.9931\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0243 - mse: 0.5691 - val_loss: 1.4434 - val_mse: 0.9885\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0175 - mse: 0.5632 - val_loss: 1.4395 - val_mse: 0.9858\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0109 - mse: 0.5578 - val_loss: 1.4342 - val_mse: 0.9818\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0036 - mse: 0.5516 - val_loss: 1.4296 - val_mse: 0.9778\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9968 - mse: 0.5457 - val_loss: 1.4260 - val_mse: 0.9752\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9897 - mse: 0.5393 - val_loss: 1.4217 - val_mse: 0.9719\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9824 - mse: 0.5330 - val_loss: 1.4172 - val_mse: 0.9679\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9755 - mse: 0.5266 - val_loss: 1.4142 - val_mse: 0.9657\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9681 - mse: 0.5199 - val_loss: 1.4109 - val_mse: 0.9630\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9613 - mse: 0.5137 - val_loss: 1.4078 - val_mse: 0.9605\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9541 - mse: 0.5070 - val_loss: 1.4052 - val_mse: 0.9580\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9472 - mse: 0.5002 - val_loss: 1.4029 - val_mse: 0.9561\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9401 - mse: 0.4936 - val_loss: 1.3999 - val_mse: 0.9533\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9334 - mse: 0.4871 - val_loss: 1.3974 - val_mse: 0.9511\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9267 - mse: 0.4807 - val_loss: 1.3953 - val_mse: 0.9490\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9202 - mse: 0.4743 - val_loss: 1.3928 - val_mse: 0.9469\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9137 - mse: 0.4681 - val_loss: 1.3909 - val_mse: 0.9451\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9075 - mse: 0.4620 - val_loss: 1.3899 - val_mse: 0.9444\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9016 - mse: 0.4563 - val_loss: 1.3880 - val_mse: 0.9423\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8957 - mse: 0.4502 - val_loss: 1.3867 - val_mse: 0.9411\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8899 - mse: 0.4445 - val_loss: 1.3851 - val_mse: 0.9394\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8845 - mse: 0.4391 - val_loss: 1.3835 - val_mse: 0.9381\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8790 - mse: 0.4336 - val_loss: 1.3823 - val_mse: 0.9366\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8738 - mse: 0.4284 - val_loss: 1.3818 - val_mse: 0.9361\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8688 - mse: 0.4233 - val_loss: 1.3818 - val_mse: 0.9361\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8638 - mse: 0.4183 - val_loss: 1.3803 - val_mse: 0.9345\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8592 - mse: 0.4136 - val_loss: 1.3789 - val_mse: 0.9332\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8545 - mse: 0.4089 - val_loss: 1.3780 - val_mse: 0.9323\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8501 - mse: 0.4045 - val_loss: 1.3776 - val_mse: 0.9317\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8458 - mse: 0.4000 - val_loss: 1.3768 - val_mse: 0.9307\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8417 - mse: 0.3958 - val_loss: 1.3769 - val_mse: 0.9307\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8376 - mse: 0.3916 - val_loss: 1.3766 - val_mse: 0.9303\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8337 - mse: 0.3875 - val_loss: 1.3757 - val_mse: 0.9294\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.3838 - val_loss: 1.3753 - val_mse: 0.9290\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8265 - mse: 0.3802 - val_loss: 1.3754 - val_mse: 0.9289\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8229 - mse: 0.3765 - val_loss: 1.3751 - val_mse: 0.9285\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8195 - mse: 0.3731 - val_loss: 1.3754 - val_mse: 0.9289\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8163 - mse: 0.3698 - val_loss: 1.3741 - val_mse: 0.9274\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8131 - mse: 0.3666 - val_loss: 1.3742 - val_mse: 0.9274\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8100 - mse: 0.3633 - val_loss: 1.3744 - val_mse: 0.9276\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8071 - mse: 0.3604 - val_loss: 1.3744 - val_mse: 0.9273\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8042 - mse: 0.3575 - val_loss: 1.3741 - val_mse: 0.9270\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8013 - mse: 0.3543 - val_loss: 1.3742 - val_mse: 0.9269\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7986 - mse: 0.3516 - val_loss: 1.3743 - val_mse: 0.9271\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7961 - mse: 0.3490 - val_loss: 1.3747 - val_mse: 0.9275\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7935 - mse: 0.3464 - val_loss: 1.3744 - val_mse: 0.9272\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7911 - mse: 0.3442 - val_loss: 1.3740 - val_mse: 0.9267\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7887 - mse: 0.3416 - val_loss: 1.3730 - val_mse: 0.9257\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7865 - mse: 0.3392 - val_loss: 1.3739 - val_mse: 0.9265\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7842 - mse: 0.3369 - val_loss: 1.3734 - val_mse: 0.9261\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7821 - mse: 0.3349 - val_loss: 1.3734 - val_mse: 0.9260\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7801 - mse: 0.3328 - val_loss: 1.3733 - val_mse: 0.9257\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7779 - mse: 0.3307 - val_loss: 1.3743 - val_mse: 0.9267\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7760 - mse: 0.3286 - val_loss: 1.3733 - val_mse: 0.9258\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7740 - mse: 0.3266 - val_loss: 1.3733 - val_mse: 0.9259\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7722 - mse: 0.3249 - val_loss: 1.3731 - val_mse: 0.9255\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7705 - mse: 0.3231 - val_loss: 1.3729 - val_mse: 0.9254\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7687 - mse: 0.3214 - val_loss: 1.3723 - val_mse: 0.9249\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7670 - mse: 0.3198 - val_loss: 1.3726 - val_mse: 0.9250\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7654 - mse: 0.3179 - val_loss: 1.3725 - val_mse: 0.9250\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7637 - mse: 0.3165 - val_loss: 1.3727 - val_mse: 0.9251\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7621 - mse: 0.3148 - val_loss: 1.3726 - val_mse: 0.9251\n",
      "Epoch 108/500\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.7604 - mse: 0.3132Restoring model weights from the end of the best epoch: 103.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7606 - mse: 0.3134 - val_loss: 1.3723 - val_mse: 0.9250\n",
      "Epoch 108: early stopping\n",
      "k=15, lambda_=5e-05, val_mse=[12.09277057647705, 9.146592140197754, 4.97631311416626, 2.962739944458008, 2.2466797828674316, 1.8930009603500366, 1.67612886428833, 1.5345609188079834, 1.433661699295044, 1.3602198362350464, 1.304744005203247, 1.262583613395691, 1.2287262678146362, 1.2017111778259277, 1.1794204711914062, 1.1625198125839233, 1.147359848022461, 1.1343967914581299, 1.1239815950393677, 1.1145527362823486, 1.1064845323562622, 1.0982019901275635, 1.0914108753204346, 1.085724949836731, 1.0796407461166382, 1.0732048749923706, 1.0687460899353027, 1.0639317035675049, 1.0585472583770752, 1.0545951128005981, 1.0498459339141846, 1.0457950830459595, 1.0392694473266602, 1.0367330312728882, 1.031487226486206, 1.0274732112884521, 1.0239828824996948, 1.0192381143569946, 1.0153958797454834, 1.012614130973816, 1.0076318979263306, 1.0034914016723633, 1.0006870031356812, 0.9961144328117371, 0.9930793642997742, 0.9884835481643677, 0.9858454465866089, 0.9817631840705872, 0.9778061509132385, 0.9752366542816162, 0.9719443917274475, 0.967871904373169, 0.9657250642776489, 0.9629766941070557, 0.960465669631958, 0.957994282245636, 0.9560726284980774, 0.9533401131629944, 0.9511445164680481, 0.9490248560905457, 0.9468653798103333, 0.9451291561126709, 0.9444151520729065, 0.9422900676727295, 0.9410730004310608, 0.9393511414527893, 0.9380955696105957, 0.9366194009780884, 0.9360759258270264, 0.9361394047737122, 0.9345453381538391, 0.9331839084625244, 0.9322808384895325, 0.9317306876182556, 0.9306607246398926, 0.9307443499565125, 0.9303274750709534, 0.9293817281723022, 0.9289811253547668, 0.9288945198059082, 0.9285035729408264, 0.9289465546607971, 0.9273913502693176, 0.927436113357544, 0.9276394844055176, 0.9273396134376526, 0.9269742965698242, 0.9269426465034485, 0.9271236658096313, 0.9275261759757996, 0.9271922707557678, 0.9267123937606812, 0.9256644248962402, 0.926480770111084, 0.926123857498169, 0.9259594082832336, 0.9256517291069031, 0.9267415404319763, 0.9257529377937317, 0.9259246587753296, 0.9255427122116089, 0.9254045486450195, 0.9249479174613953, 0.9250127673149109, 0.9250408411026001, 0.9251208305358887, 0.9250732660293579, 0.9249690771102905]\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 15 | 2e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7402 - mse: 12.7387 - val_loss: 12.1107 - val_mse: 12.1089\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 11.0178 - mse: 11.0124 - val_loss: 9.4980 - val_mse: 9.4855\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.2867 - mse: 7.2594 - val_loss: 5.2767 - val_mse: 5.2316\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.8104 - mse: 3.7470 - val_loss: 3.0440 - val_mse: 2.9630\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3779 - mse: 2.2832 - val_loss: 2.3013 - val_mse: 2.1943\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8270 - mse: 1.7103 - val_loss: 1.9573 - val_mse: 1.8315\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5354 - mse: 1.4019 - val_loss: 1.7588 - val_mse: 1.6180\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3556 - mse: 1.2084 - val_loss: 1.6322 - val_mse: 1.4786\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2363 - mse: 1.0773 - val_loss: 1.5473 - val_mse: 1.3830\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1527 - mse: 0.9838 - val_loss: 1.4877 - val_mse: 1.3141\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0919 - mse: 0.9143 - val_loss: 1.4444 - val_mse: 1.2628\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0466 - mse: 0.8615 - val_loss: 1.4128 - val_mse: 1.2242\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0119 - mse: 0.8202 - val_loss: 1.3874 - val_mse: 1.1927\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9844 - mse: 0.7870 - val_loss: 1.3680 - val_mse: 1.1680\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9625 - mse: 0.7600 - val_loss: 1.3531 - val_mse: 1.1486\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9446 - mse: 0.7380 - val_loss: 1.3410 - val_mse: 1.1323\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9301 - mse: 0.7196 - val_loss: 1.3311 - val_mse: 1.1187\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9178 - mse: 0.7040 - val_loss: 1.3232 - val_mse: 1.1078\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9068 - mse: 0.6901 - val_loss: 1.3167 - val_mse: 1.0987\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8974 - mse: 0.6782 - val_loss: 1.3111 - val_mse: 1.0908\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8891 - mse: 0.6678 - val_loss: 1.3060 - val_mse: 1.0836\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8817 - mse: 0.6586 - val_loss: 1.3015 - val_mse: 1.0775\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8747 - mse: 0.6500 - val_loss: 1.2980 - val_mse: 1.0727\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8681 - mse: 0.6422 - val_loss: 1.2942 - val_mse: 1.0676\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8615 - mse: 0.6344 - val_loss: 1.2910 - val_mse: 1.0635\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8556 - mse: 0.6276 - val_loss: 1.2879 - val_mse: 1.0596\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8493 - mse: 0.6207 - val_loss: 1.2846 - val_mse: 1.0556\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8432 - mse: 0.6140 - val_loss: 1.2817 - val_mse: 1.0522\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8371 - mse: 0.6074 - val_loss: 1.2802 - val_mse: 1.0503\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8310 - mse: 0.6010 - val_loss: 1.2777 - val_mse: 1.0475\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8246 - mse: 0.5943 - val_loss: 1.2749 - val_mse: 1.0446\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8181 - mse: 0.5877 - val_loss: 1.2733 - val_mse: 1.0429\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8116 - mse: 0.5812 - val_loss: 1.2710 - val_mse: 1.0405\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8048 - mse: 0.5743 - val_loss: 1.2690 - val_mse: 1.0384\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7978 - mse: 0.5673 - val_loss: 1.2683 - val_mse: 1.0378\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7909 - mse: 0.5605 - val_loss: 1.2668 - val_mse: 1.0363\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7837 - mse: 0.5533 - val_loss: 1.2646 - val_mse: 1.0342\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7762 - mse: 0.5460 - val_loss: 1.2637 - val_mse: 1.0334\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7688 - mse: 0.5386 - val_loss: 1.2624 - val_mse: 1.0323\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7613 - mse: 0.5313 - val_loss: 1.2618 - val_mse: 1.0319\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7537 - mse: 0.5237 - val_loss: 1.2601 - val_mse: 1.0302\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7458 - mse: 0.5161 - val_loss: 1.2583 - val_mse: 1.0284\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7383 - mse: 0.5085 - val_loss: 1.2591 - val_mse: 1.0294\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7303 - mse: 0.5008 - val_loss: 1.2586 - val_mse: 1.0290\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7227 - mse: 0.4932 - val_loss: 1.2579 - val_mse: 1.0283\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7150 - mse: 0.4856 - val_loss: 1.2569 - val_mse: 1.0273\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7074 - mse: 0.4779 - val_loss: 1.2582 - val_mse: 1.0287\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6999 - mse: 0.4705 - val_loss: 1.2572 - val_mse: 1.0275\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6923 - mse: 0.4628 - val_loss: 1.2579 - val_mse: 1.0283\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6852 - mse: 0.4558 - val_loss: 1.2582 - val_mse: 1.0287\n",
      "Epoch 51/500\n",
      "146/160 [==========================>...] - ETA: 0s - loss: 0.6771 - mse: 0.4476Restoring model weights from the end of the best epoch: 46.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6779 - mse: 0.4484 - val_loss: 1.2576 - val_mse: 1.0279\n",
      "Epoch 51: early stopping\n",
      "k=15, lambda_=2e-05, val_mse=[12.108927726745605, 9.485468864440918, 5.231626510620117, 2.962977409362793, 2.194307327270508, 1.8314728736877441, 1.6179547309875488, 1.4786280393600464, 1.3829909563064575, 1.3141430616378784, 1.262832522392273, 1.224191427230835, 1.1927409172058105, 1.1679586172103882, 1.1485562324523926, 1.1323232650756836, 1.1187448501586914, 1.1078088283538818, 1.0986592769622803, 1.090821385383606, 1.0836377143859863, 1.0774801969528198, 1.0726523399353027, 1.0676429271697998, 1.0634527206420898, 1.0595685243606567, 1.0555936098098755, 1.0521880388259888, 1.050335168838501, 1.0474852323532104, 1.0446405410766602, 1.0428860187530518, 1.0404772758483887, 1.038382649421692, 1.0377715826034546, 1.0362900495529175, 1.034214735031128, 1.0334478616714478, 1.0323153734207153, 1.0319229364395142, 1.0301920175552368, 1.0284128189086914, 1.0294321775436401, 1.0290427207946777, 1.0283217430114746, 1.0272712707519531, 1.0286509990692139, 1.027482032775879, 1.028306484222412, 1.0286544561386108, 1.0278809070587158]\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 30 | 0.0002\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7199 - mse: 12.7034 - val_loss: 12.0370 - val_mse: 12.0194\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.3380 - mse: 10.2443 - val_loss: 7.9358 - val_mse: 7.6991\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.6623 - mse: 5.2023 - val_loss: 4.2733 - val_mse: 3.5945\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.5740 - mse: 2.7438 - val_loss: 3.4653 - val_mse: 2.5137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.0484 - mse: 2.0140 - val_loss: 3.2000 - val_mse: 2.0964\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8350 - mse: 1.6794 - val_loss: 3.0768 - val_mse: 1.8772\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.7226 - mse: 1.4898 - val_loss: 3.0063 - val_mse: 1.7436\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6528 - mse: 1.3689 - val_loss: 2.9580 - val_mse: 1.6554\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6034 - mse: 1.2875 - val_loss: 2.9195 - val_mse: 1.5928\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5645 - mse: 1.2304 - val_loss: 2.8868 - val_mse: 1.5459\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5315 - mse: 1.1869 - val_loss: 2.8544 - val_mse: 1.5069\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5017 - mse: 1.1534 - val_loss: 2.8245 - val_mse: 1.4766\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4739 - mse: 1.1272 - val_loss: 2.7926 - val_mse: 1.4484\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4467 - mse: 1.1057 - val_loss: 2.7630 - val_mse: 1.4249\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4205 - mse: 1.0865 - val_loss: 2.7341 - val_mse: 1.4036\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3942 - mse: 1.0694 - val_loss: 2.7043 - val_mse: 1.3841\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3683 - mse: 1.0543 - val_loss: 2.6729 - val_mse: 1.3648\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3423 - mse: 1.0407 - val_loss: 2.6446 - val_mse: 1.3497\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3167 - mse: 1.0276 - val_loss: 2.6141 - val_mse: 1.3332\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2916 - mse: 1.0170 - val_loss: 2.5841 - val_mse: 1.3158\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2663 - mse: 1.0050 - val_loss: 2.5563 - val_mse: 1.3016\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2418 - mse: 0.9934 - val_loss: 2.5285 - val_mse: 1.2892\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2177 - mse: 0.9848 - val_loss: 2.5005 - val_mse: 1.2750\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1940 - mse: 0.9748 - val_loss: 2.4727 - val_mse: 1.2612\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1710 - mse: 0.9650 - val_loss: 2.4458 - val_mse: 1.2477\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1487 - mse: 0.9568 - val_loss: 2.4200 - val_mse: 1.2350\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1271 - mse: 0.9490 - val_loss: 2.3947 - val_mse: 1.2225\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1058 - mse: 0.9398 - val_loss: 2.3723 - val_mse: 1.2130\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0855 - mse: 0.9324 - val_loss: 2.3479 - val_mse: 1.2008\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0659 - mse: 0.9250 - val_loss: 2.3237 - val_mse: 1.1885\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0471 - mse: 0.9172 - val_loss: 2.3013 - val_mse: 1.1773\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0287 - mse: 0.9107 - val_loss: 2.2809 - val_mse: 1.1680\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0112 - mse: 0.9036 - val_loss: 2.2611 - val_mse: 1.1586\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9943 - mse: 0.8963 - val_loss: 2.2406 - val_mse: 1.1485\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9781 - mse: 0.8904 - val_loss: 2.2220 - val_mse: 1.1393\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9626 - mse: 0.8848 - val_loss: 2.2031 - val_mse: 1.1292\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9477 - mse: 0.8780 - val_loss: 2.1853 - val_mse: 1.1208\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9331 - mse: 0.8718 - val_loss: 2.1686 - val_mse: 1.1133\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9195 - mse: 0.8674 - val_loss: 2.1511 - val_mse: 1.1045\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9064 - mse: 0.8625 - val_loss: 2.1354 - val_mse: 1.0960\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8936 - mse: 0.8568 - val_loss: 2.1211 - val_mse: 1.0893\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8816 - mse: 0.8522 - val_loss: 2.1073 - val_mse: 1.0814\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8701 - mse: 0.8474 - val_loss: 2.0933 - val_mse: 1.0739\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8591 - mse: 0.8422 - val_loss: 2.0801 - val_mse: 1.0667\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8483 - mse: 0.8380 - val_loss: 2.0672 - val_mse: 1.0603\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8382 - mse: 0.8334 - val_loss: 2.0548 - val_mse: 1.0534\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8285 - mse: 0.8286 - val_loss: 2.0439 - val_mse: 1.0477\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8195 - mse: 0.8253 - val_loss: 2.0330 - val_mse: 1.0420\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8106 - mse: 0.8212 - val_loss: 2.0233 - val_mse: 1.0352\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8024 - mse: 0.8166 - val_loss: 2.0137 - val_mse: 1.0299\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7943 - mse: 0.8128 - val_loss: 2.0050 - val_mse: 1.0250\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7865 - mse: 0.8077 - val_loss: 1.9963 - val_mse: 1.0205\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7791 - mse: 0.8044 - val_loss: 1.9879 - val_mse: 1.0153\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7725 - mse: 0.8009 - val_loss: 1.9799 - val_mse: 1.0103\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7659 - mse: 0.7975 - val_loss: 1.9742 - val_mse: 1.0074\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7598 - mse: 0.7933 - val_loss: 1.9671 - val_mse: 1.0029\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7539 - mse: 0.7897 - val_loss: 1.9606 - val_mse: 0.9987\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7480 - mse: 0.7860 - val_loss: 1.9550 - val_mse: 0.9951\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7429 - mse: 0.7839 - val_loss: 1.9493 - val_mse: 0.9907\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7378 - mse: 0.7788 - val_loss: 1.9435 - val_mse: 0.9867\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7330 - mse: 0.7771 - val_loss: 1.9391 - val_mse: 0.9836\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7284 - mse: 0.7735 - val_loss: 1.9346 - val_mse: 0.9800\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7242 - mse: 0.7703 - val_loss: 1.9311 - val_mse: 0.9778\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7200 - mse: 0.7665 - val_loss: 1.9264 - val_mse: 0.9738\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7163 - mse: 0.7640 - val_loss: 1.9225 - val_mse: 0.9711\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7127 - mse: 0.7617 - val_loss: 1.9191 - val_mse: 0.9682\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7091 - mse: 0.7580 - val_loss: 1.9161 - val_mse: 0.9660\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7061 - mse: 0.7563 - val_loss: 1.9125 - val_mse: 0.9630\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7030 - mse: 0.7528 - val_loss: 1.9102 - val_mse: 0.9609\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7000 - mse: 0.7506 - val_loss: 1.9069 - val_mse: 0.9578\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6974 - mse: 0.7475 - val_loss: 1.9049 - val_mse: 0.9565\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6947 - mse: 0.7458 - val_loss: 1.9031 - val_mse: 0.9548\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6923 - mse: 0.7435 - val_loss: 1.9009 - val_mse: 0.9524\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6901 - mse: 0.7413 - val_loss: 1.8990 - val_mse: 0.9504\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6879 - mse: 0.7391 - val_loss: 1.8968 - val_mse: 0.9482\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6858 - mse: 0.7363 - val_loss: 1.8951 - val_mse: 0.9465\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6838 - mse: 0.7348 - val_loss: 1.8943 - val_mse: 0.9457\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6820 - mse: 0.7325 - val_loss: 1.8926 - val_mse: 0.9432\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6802 - mse: 0.7301 - val_loss: 1.8908 - val_mse: 0.9413\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6786 - mse: 0.7283 - val_loss: 1.8898 - val_mse: 0.9405\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6771 - mse: 0.7272 - val_loss: 1.8886 - val_mse: 0.9391\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6756 - mse: 0.7254 - val_loss: 1.8878 - val_mse: 0.9373\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6742 - mse: 0.7234 - val_loss: 1.8863 - val_mse: 0.9356\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6728 - mse: 0.7211 - val_loss: 1.8858 - val_mse: 0.9349\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6717 - mse: 0.7197 - val_loss: 1.8851 - val_mse: 0.9334\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6705 - mse: 0.7184 - val_loss: 1.8844 - val_mse: 0.9326\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6693 - mse: 0.7165 - val_loss: 1.8838 - val_mse: 0.9314\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6683 - mse: 0.7157 - val_loss: 1.8826 - val_mse: 0.9299\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6672 - mse: 0.7134 - val_loss: 1.8827 - val_mse: 0.9292\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6664 - mse: 0.7123 - val_loss: 1.8821 - val_mse: 0.9283\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6655 - mse: 0.7116 - val_loss: 1.8815 - val_mse: 0.9274\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6646 - mse: 0.7090 - val_loss: 1.8809 - val_mse: 0.9262\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6639 - mse: 0.7087 - val_loss: 1.8806 - val_mse: 0.9252\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6633 - mse: 0.7067 - val_loss: 1.8806 - val_mse: 0.9245\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6625 - mse: 0.7061 - val_loss: 1.8800 - val_mse: 0.9236\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6618 - mse: 0.7045 - val_loss: 1.8800 - val_mse: 0.9234\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6610 - mse: 0.7031 - val_loss: 1.8801 - val_mse: 0.9229\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6606 - mse: 0.7026 - val_loss: 1.8802 - val_mse: 0.9223\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6599 - mse: 0.7011 - val_loss: 1.8791 - val_mse: 0.9205\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6594 - mse: 0.7004 - val_loss: 1.8791 - val_mse: 0.9197\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6590 - mse: 0.6993 - val_loss: 1.8795 - val_mse: 0.9199\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6585 - mse: 0.6987 - val_loss: 1.8799 - val_mse: 0.9199\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6581 - mse: 0.6972 - val_loss: 1.8792 - val_mse: 0.9185\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6576 - mse: 0.6963 - val_loss: 1.8792 - val_mse: 0.9183\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6572 - mse: 0.6955 - val_loss: 1.8787 - val_mse: 0.9167\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6567 - mse: 0.6939 - val_loss: 1.8786 - val_mse: 0.9169\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6564 - mse: 0.6938 - val_loss: 1.8788 - val_mse: 0.9162\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6560 - mse: 0.6931 - val_loss: 1.8793 - val_mse: 0.9165\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6557 - mse: 0.6922 - val_loss: 1.8788 - val_mse: 0.9151\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6553 - mse: 0.6913 - val_loss: 1.8792 - val_mse: 0.9152\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6551 - mse: 0.6907 - val_loss: 1.8791 - val_mse: 0.9148\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6548 - mse: 0.6902 - val_loss: 1.8787 - val_mse: 0.9141\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6545 - mse: 0.6891 - val_loss: 1.8793 - val_mse: 0.9138\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6541 - mse: 0.6882 - val_loss: 1.8795 - val_mse: 0.9141\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6540 - mse: 0.6876 - val_loss: 1.8794 - val_mse: 0.9131\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6537 - mse: 0.6880 - val_loss: 1.8796 - val_mse: 0.9129\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6535 - mse: 0.6865 - val_loss: 1.8799 - val_mse: 0.9125\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6534 - mse: 0.6854 - val_loss: 1.8794 - val_mse: 0.9122\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6532 - mse: 0.6855 - val_loss: 1.8796 - val_mse: 0.9116\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6531 - mse: 0.6843 - val_loss: 1.8800 - val_mse: 0.9117\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6528 - mse: 0.6843 - val_loss: 1.8797 - val_mse: 0.9114\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6527 - mse: 0.6840 - val_loss: 1.8799 - val_mse: 0.9107\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6524 - mse: 0.6823 - val_loss: 1.8802 - val_mse: 0.9110\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6523 - mse: 0.6827 - val_loss: 1.8801 - val_mse: 0.9107\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6521 - mse: 0.6818 - val_loss: 1.8803 - val_mse: 0.9100\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6520 - mse: 0.6818 - val_loss: 1.8805 - val_mse: 0.9102\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6520 - mse: 0.6808 - val_loss: 1.8805 - val_mse: 0.9097\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6517 - mse: 0.6804 - val_loss: 1.8808 - val_mse: 0.9095\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6516 - mse: 0.6799 - val_loss: 1.8812 - val_mse: 0.9100\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6516 - mse: 0.6794 - val_loss: 1.8808 - val_mse: 0.9091\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6513 - mse: 0.6792 - val_loss: 1.8813 - val_mse: 0.9091\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6512 - mse: 0.6784 - val_loss: 1.8814 - val_mse: 0.9090\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6512 - mse: 0.6786 - val_loss: 1.8811 - val_mse: 0.9086\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6510 - mse: 0.6779 - val_loss: 1.8817 - val_mse: 0.9087\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6510 - mse: 0.6776 - val_loss: 1.8818 - val_mse: 0.9082\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6509 - mse: 0.6773 - val_loss: 1.8820 - val_mse: 0.9088\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6508 - mse: 0.6767 - val_loss: 1.8819 - val_mse: 0.9084\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6506 - mse: 0.6762 - val_loss: 1.8822 - val_mse: 0.9080\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6507 - mse: 0.6764 - val_loss: 1.8822 - val_mse: 0.9079\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6506 - mse: 0.6756 - val_loss: 1.8822 - val_mse: 0.9074\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6505 - mse: 0.6756 - val_loss: 1.8822 - val_mse: 0.9070\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6504 - mse: 0.6748 - val_loss: 1.8827 - val_mse: 0.9074\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6504 - mse: 0.6746 - val_loss: 1.8828 - val_mse: 0.9076\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6502 - mse: 0.6744 - val_loss: 1.8827 - val_mse: 0.9072\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6503 - mse: 0.6743 - val_loss: 1.8830 - val_mse: 0.9071\n",
      "Epoch 146/500\n",
      "143/160 [=========================>....] - ETA: 0s - loss: 1.6490 - mse: 0.6723Restoring model weights from the end of the best epoch: 141.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6502 - mse: 0.6735 - val_loss: 1.8832 - val_mse: 0.9071\n",
      "Epoch 146: early stopping\n",
      "k=30, lambda_=0.0002, val_mse=[12.019424438476562, 7.699077606201172, 3.5945329666137695, 2.5136525630950928, 2.0964221954345703, 1.8771840333938599, 1.7435567378997803, 1.6554126739501953, 1.592797040939331, 1.5459319353103638, 1.5068937540054321, 1.4766398668289185, 1.4484003782272339, 1.4248870611190796, 1.4036128520965576, 1.3841251134872437, 1.3647710084915161, 1.3497364521026611, 1.3332101106643677, 1.3157703876495361, 1.3015702962875366, 1.2892098426818848, 1.275005578994751, 1.261174201965332, 1.247675895690918, 1.2350431680679321, 1.2225335836410522, 1.2129886150360107, 1.2008180618286133, 1.1885470151901245, 1.1773089170455933, 1.1680352687835693, 1.1585955619812012, 1.1484861373901367, 1.1392933130264282, 1.129189133644104, 1.1208257675170898, 1.1132584810256958, 1.1044797897338867, 1.0960086584091187, 1.0893394947052002, 1.0813965797424316, 1.0739445686340332, 1.0667164325714111, 1.0603474378585815, 1.0534064769744873, 1.0477081537246704, 1.0419986248016357, 1.0351799726486206, 1.029883861541748, 1.0250052213668823, 1.0205272436141968, 1.0152833461761475, 1.0103423595428467, 1.0073944330215454, 1.0029492378234863, 0.998725175857544, 0.9951083660125732, 0.9907227754592896, 0.986685037612915, 0.983555018901825, 0.9799627065658569, 0.9777595400810242, 0.9738170504570007, 0.9711037278175354, 0.9682415723800659, 0.9660358428955078, 0.962998628616333, 0.9608865976333618, 0.9578037261962891, 0.9565204977989197, 0.9547755718231201, 0.9524290561676025, 0.9504436254501343, 0.9482105374336243, 0.9465360045433044, 0.9457401037216187, 0.9432452917098999, 0.9413468837738037, 0.9405385851860046, 0.9390713572502136, 0.9373056888580322, 0.9356103539466858, 0.9348549246788025, 0.9333780407905579, 0.932579755783081, 0.9313603043556213, 0.9299412965774536, 0.9291600584983826, 0.928263783454895, 0.9274007081985474, 0.9261558055877686, 0.9251545071601868, 0.924518883228302, 0.9236434698104858, 0.9233776330947876, 0.9228807091712952, 0.9222854971885681, 0.920527458190918, 0.9197049736976624, 0.9199273586273193, 0.9198852777481079, 0.9184868335723877, 0.9183005690574646, 0.9167258739471436, 0.9168742895126343, 0.9161546230316162, 0.9164955019950867, 0.9151362180709839, 0.9151597619056702, 0.9148252606391907, 0.9141483902931213, 0.9137743711471558, 0.914064347743988, 0.9131463170051575, 0.9129256010055542, 0.9124791622161865, 0.9121997356414795, 0.9115537405014038, 0.9117015600204468, 0.911428689956665, 0.9107202887535095, 0.9110074639320374, 0.9107192158699036, 0.909964919090271, 0.9102057218551636, 0.9096686244010925, 0.9094951748847961, 0.9099856019020081, 0.9091370701789856, 0.9090574383735657, 0.9090275168418884, 0.908617377281189, 0.9087299108505249, 0.9082059264183044, 0.9087546467781067, 0.9083817601203918, 0.9080193638801575, 0.9078748226165771, 0.907416045665741, 0.9069991111755371, 0.9073975086212158, 0.9075567722320557, 0.9071776866912842, 0.9071327447891235, 0.9071295261383057]\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 30 | 5e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7145 - mse: 12.7088 - val_loss: 12.0616 - val_mse: 12.0554\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.6055 - mse: 10.5840 - val_loss: 8.4309 - val_mse: 8.3786\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.7585 - mse: 5.6478 - val_loss: 3.8674 - val_mse: 3.6936\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8634 - mse: 2.6382 - val_loss: 2.5967 - val_mse: 2.3280\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0580 - mse: 1.7573 - val_loss: 2.1789 - val_mse: 1.8496\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7236 - mse: 1.3715 - val_loss: 1.9790 - val_mse: 1.6053\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5444 - mse: 1.1533 - val_loss: 1.8702 - val_mse: 1.4625\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4358 - mse: 1.0147 - val_loss: 1.8047 - val_mse: 1.3706\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3647 - mse: 0.9200 - val_loss: 1.7632 - val_mse: 1.3083\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3149 - mse: 0.8519 - val_loss: 1.7365 - val_mse: 1.2653\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2779 - mse: 0.8004 - val_loss: 1.7165 - val_mse: 1.2327\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2491 - mse: 0.7603 - val_loss: 1.7047 - val_mse: 1.2107\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2259 - mse: 0.7284 - val_loss: 1.6933 - val_mse: 1.1919\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2062 - mse: 0.7021 - val_loss: 1.6847 - val_mse: 1.1773\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1891 - mse: 0.6795 - val_loss: 1.6771 - val_mse: 1.1654\n",
      "Epoch 16/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1737 - mse: 0.6604 - val_loss: 1.6743 - val_mse: 1.1594\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1602 - mse: 0.6440 - val_loss: 1.6699 - val_mse: 1.1525\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1471 - mse: 0.6291 - val_loss: 1.6663 - val_mse: 1.1470\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1347 - mse: 0.6151 - val_loss: 1.6604 - val_mse: 1.1402\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1231 - mse: 0.6029 - val_loss: 1.6579 - val_mse: 1.1373\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1120 - mse: 0.5915 - val_loss: 1.6544 - val_mse: 1.1335\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1008 - mse: 0.5798 - val_loss: 1.6518 - val_mse: 1.1305\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0903 - mse: 0.5696 - val_loss: 1.6478 - val_mse: 1.1272\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0802 - mse: 0.5602 - val_loss: 1.6441 - val_mse: 1.1241\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0702 - mse: 0.5503 - val_loss: 1.6410 - val_mse: 1.1210\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0602 - mse: 0.5413 - val_loss: 1.6364 - val_mse: 1.1174\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0506 - mse: 0.5322 - val_loss: 1.6338 - val_mse: 1.1155\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0411 - mse: 0.5236 - val_loss: 1.6293 - val_mse: 1.1120\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0316 - mse: 0.5148 - val_loss: 1.6267 - val_mse: 1.1103\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0225 - mse: 0.5065 - val_loss: 1.6225 - val_mse: 1.1069\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0131 - mse: 0.4983 - val_loss: 1.6177 - val_mse: 1.1025\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0042 - mse: 0.4899 - val_loss: 1.6140 - val_mse: 1.0998\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9952 - mse: 0.4818 - val_loss: 1.6109 - val_mse: 1.0974\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9864 - mse: 0.4738 - val_loss: 1.6072 - val_mse: 1.0944\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9777 - mse: 0.4658 - val_loss: 1.6039 - val_mse: 1.0918\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9690 - mse: 0.4576 - val_loss: 1.6001 - val_mse: 1.0888\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9605 - mse: 0.4499 - val_loss: 1.5952 - val_mse: 1.0846\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9519 - mse: 0.4419 - val_loss: 1.5920 - val_mse: 1.0818\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9438 - mse: 0.4344 - val_loss: 1.5892 - val_mse: 1.0796\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9355 - mse: 0.4265 - val_loss: 1.5833 - val_mse: 1.0741\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9276 - mse: 0.4190 - val_loss: 1.5815 - val_mse: 1.0728\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9198 - mse: 0.4117 - val_loss: 1.5770 - val_mse: 1.0686\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9121 - mse: 0.4043 - val_loss: 1.5741 - val_mse: 1.0662\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9044 - mse: 0.3970 - val_loss: 1.5698 - val_mse: 1.0618\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8970 - mse: 0.3900 - val_loss: 1.5659 - val_mse: 1.0584\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8896 - mse: 0.3827 - val_loss: 1.5621 - val_mse: 1.0550\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8826 - mse: 0.3761 - val_loss: 1.5595 - val_mse: 1.0526\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8755 - mse: 0.3692 - val_loss: 1.5558 - val_mse: 1.0494\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8688 - mse: 0.3625 - val_loss: 1.5528 - val_mse: 1.0462\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8621 - mse: 0.3560 - val_loss: 1.5505 - val_mse: 1.0443\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8557 - mse: 0.3498 - val_loss: 1.5476 - val_mse: 1.0416\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8492 - mse: 0.3437 - val_loss: 1.5440 - val_mse: 1.0380\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8429 - mse: 0.3371 - val_loss: 1.5414 - val_mse: 1.0356\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8369 - mse: 0.3316 - val_loss: 1.5382 - val_mse: 1.0326\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8309 - mse: 0.3256 - val_loss: 1.5356 - val_mse: 1.0299\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8252 - mse: 0.3201 - val_loss: 1.5322 - val_mse: 1.0263\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8194 - mse: 0.3139 - val_loss: 1.5287 - val_mse: 1.0230\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8141 - mse: 0.3088 - val_loss: 1.5267 - val_mse: 1.0211\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8088 - mse: 0.3039 - val_loss: 1.5249 - val_mse: 1.0193\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8036 - mse: 0.2985 - val_loss: 1.5219 - val_mse: 1.0163\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7985 - mse: 0.2934 - val_loss: 1.5205 - val_mse: 1.0149\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7937 - mse: 0.2887 - val_loss: 1.5183 - val_mse: 1.0129\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7889 - mse: 0.2840 - val_loss: 1.5158 - val_mse: 1.0103\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7842 - mse: 0.2792 - val_loss: 1.5136 - val_mse: 1.0085\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7798 - mse: 0.2750 - val_loss: 1.5121 - val_mse: 1.0069\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7755 - mse: 0.2708 - val_loss: 1.5097 - val_mse: 1.0045\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7713 - mse: 0.2664 - val_loss: 1.5074 - val_mse: 1.0023\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7672 - mse: 0.2626 - val_loss: 1.5049 - val_mse: 0.9998\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7632 - mse: 0.2584 - val_loss: 1.5042 - val_mse: 0.9991\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7594 - mse: 0.2548 - val_loss: 1.5016 - val_mse: 0.9966\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7557 - mse: 0.2513 - val_loss: 1.5003 - val_mse: 0.9954\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7520 - mse: 0.2476 - val_loss: 1.4982 - val_mse: 0.9933\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7484 - mse: 0.2442 - val_loss: 1.4958 - val_mse: 0.9909\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7451 - mse: 0.2408 - val_loss: 1.4945 - val_mse: 0.9898\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7418 - mse: 0.2377 - val_loss: 1.4934 - val_mse: 0.9890\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7386 - mse: 0.2348 - val_loss: 1.4918 - val_mse: 0.9874\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7356 - mse: 0.2317 - val_loss: 1.4902 - val_mse: 0.9863\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7325 - mse: 0.2291 - val_loss: 1.4890 - val_mse: 0.9850\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7295 - mse: 0.2258 - val_loss: 1.4886 - val_mse: 0.9849\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7268 - mse: 0.2236 - val_loss: 1.4854 - val_mse: 0.9819\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7241 - mse: 0.2212 - val_loss: 1.4844 - val_mse: 0.9811\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7216 - mse: 0.2188 - val_loss: 1.4834 - val_mse: 0.9803\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7188 - mse: 0.2164 - val_loss: 1.4809 - val_mse: 0.9779\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7165 - mse: 0.2139 - val_loss: 1.4804 - val_mse: 0.9779\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7140 - mse: 0.2119 - val_loss: 1.4780 - val_mse: 0.9754\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7116 - mse: 0.2097 - val_loss: 1.4782 - val_mse: 0.9759\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7097 - mse: 0.2080 - val_loss: 1.4772 - val_mse: 0.9752\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7073 - mse: 0.2060 - val_loss: 1.4754 - val_mse: 0.9736\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7052 - mse: 0.2041 - val_loss: 1.4741 - val_mse: 0.9728\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7032 - mse: 0.2023 - val_loss: 1.4734 - val_mse: 0.9721\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7011 - mse: 0.2004 - val_loss: 1.4712 - val_mse: 0.9702\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6992 - mse: 0.1990 - val_loss: 1.4706 - val_mse: 0.9701\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6973 - mse: 0.1972 - val_loss: 1.4695 - val_mse: 0.9691\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6956 - mse: 0.1961 - val_loss: 1.4686 - val_mse: 0.9686\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6939 - mse: 0.1946 - val_loss: 1.4676 - val_mse: 0.9677\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6920 - mse: 0.1927 - val_loss: 1.4662 - val_mse: 0.9665\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6904 - mse: 0.1916 - val_loss: 1.4653 - val_mse: 0.9659\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6888 - mse: 0.1902 - val_loss: 1.4640 - val_mse: 0.9649\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6873 - mse: 0.1891 - val_loss: 1.4637 - val_mse: 0.9652\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6857 - mse: 0.1877 - val_loss: 1.4622 - val_mse: 0.9638\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6843 - mse: 0.1867 - val_loss: 1.4613 - val_mse: 0.9632\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6829 - mse: 0.1853 - val_loss: 1.4603 - val_mse: 0.9624\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6815 - mse: 0.1844 - val_loss: 1.4600 - val_mse: 0.9625\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6801 - mse: 0.1834 - val_loss: 1.4583 - val_mse: 0.9613\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6788 - mse: 0.1823 - val_loss: 1.4579 - val_mse: 0.9612\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6775 - mse: 0.1813 - val_loss: 1.4570 - val_mse: 0.9606\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6762 - mse: 0.1805 - val_loss: 1.4558 - val_mse: 0.9596\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6751 - mse: 0.1795 - val_loss: 1.4555 - val_mse: 0.9596\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6739 - mse: 0.1786 - val_loss: 1.4543 - val_mse: 0.9585\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6727 - mse: 0.1777 - val_loss: 1.4528 - val_mse: 0.9574\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6717 - mse: 0.1768 - val_loss: 1.4529 - val_mse: 0.9580\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6706 - mse: 0.1761 - val_loss: 1.4518 - val_mse: 0.9571\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6696 - mse: 0.1755 - val_loss: 1.4509 - val_mse: 0.9564\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6685 - mse: 0.1746 - val_loss: 1.4498 - val_mse: 0.9556\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6675 - mse: 0.1741 - val_loss: 1.4488 - val_mse: 0.9548\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6666 - mse: 0.1731 - val_loss: 1.4488 - val_mse: 0.9551\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6657 - mse: 0.1727 - val_loss: 1.4477 - val_mse: 0.9543\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6647 - mse: 0.1718 - val_loss: 1.4467 - val_mse: 0.9536\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6638 - mse: 0.1711 - val_loss: 1.4461 - val_mse: 0.9535\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6630 - mse: 0.1709 - val_loss: 1.4456 - val_mse: 0.9528\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6621 - mse: 0.1701 - val_loss: 1.4453 - val_mse: 0.9531\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6613 - mse: 0.1696 - val_loss: 1.4443 - val_mse: 0.9523\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6606 - mse: 0.1691 - val_loss: 1.4435 - val_mse: 0.9517\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6598 - mse: 0.1685 - val_loss: 1.4428 - val_mse: 0.9512\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6590 - mse: 0.1679 - val_loss: 1.4422 - val_mse: 0.9508\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6582 - mse: 0.1675 - val_loss: 1.4416 - val_mse: 0.9505\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6576 - mse: 0.1668 - val_loss: 1.4423 - val_mse: 0.9514\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6568 - mse: 0.1665 - val_loss: 1.4406 - val_mse: 0.9499\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6561 - mse: 0.1661 - val_loss: 1.4397 - val_mse: 0.9492\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6555 - mse: 0.1655 - val_loss: 1.4385 - val_mse: 0.9482\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6549 - mse: 0.1651 - val_loss: 1.4392 - val_mse: 0.9491\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6542 - mse: 0.1648 - val_loss: 1.4386 - val_mse: 0.9488\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6536 - mse: 0.1644 - val_loss: 1.4379 - val_mse: 0.9481\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6530 - mse: 0.1637 - val_loss: 1.4373 - val_mse: 0.9477\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6525 - mse: 0.1636 - val_loss: 1.4370 - val_mse: 0.9477\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6518 - mse: 0.1630 - val_loss: 1.4363 - val_mse: 0.9472\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6513 - mse: 0.1627 - val_loss: 1.4359 - val_mse: 0.9469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6507 - mse: 0.1624 - val_loss: 1.4350 - val_mse: 0.9462\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6502 - mse: 0.1620 - val_loss: 1.4342 - val_mse: 0.9456\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6497 - mse: 0.1616 - val_loss: 1.4346 - val_mse: 0.9462\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6492 - mse: 0.1613 - val_loss: 1.4331 - val_mse: 0.9449\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6487 - mse: 0.1610 - val_loss: 1.4332 - val_mse: 0.9452\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6483 - mse: 0.1608 - val_loss: 1.4323 - val_mse: 0.9444\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6478 - mse: 0.1604 - val_loss: 1.4320 - val_mse: 0.9443\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6473 - mse: 0.1601 - val_loss: 1.4323 - val_mse: 0.9447\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6468 - mse: 0.1597 - val_loss: 1.4317 - val_mse: 0.9442\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6464 - mse: 0.1594 - val_loss: 1.4319 - val_mse: 0.9447\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6459 - mse: 0.1594 - val_loss: 1.4317 - val_mse: 0.9445\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6456 - mse: 0.1589 - val_loss: 1.4308 - val_mse: 0.9439\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6452 - mse: 0.1588 - val_loss: 1.4304 - val_mse: 0.9435\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6448 - mse: 0.1583 - val_loss: 1.4300 - val_mse: 0.9432\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6445 - mse: 0.1582 - val_loss: 1.4294 - val_mse: 0.9427\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6441 - mse: 0.1579 - val_loss: 1.4292 - val_mse: 0.9427\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6437 - mse: 0.1575 - val_loss: 1.4287 - val_mse: 0.9423\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6434 - mse: 0.1575 - val_loss: 1.4281 - val_mse: 0.9417\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6430 - mse: 0.1573 - val_loss: 1.4285 - val_mse: 0.9424\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6427 - mse: 0.1568 - val_loss: 1.4280 - val_mse: 0.9419\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6423 - mse: 0.1568 - val_loss: 1.4277 - val_mse: 0.9418\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6419 - mse: 0.1564 - val_loss: 1.4271 - val_mse: 0.9413\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6417 - mse: 0.1565 - val_loss: 1.4266 - val_mse: 0.9408\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6414 - mse: 0.1560 - val_loss: 1.4266 - val_mse: 0.9410\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6411 - mse: 0.1560 - val_loss: 1.4268 - val_mse: 0.9411\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6409 - mse: 0.1557 - val_loss: 1.4267 - val_mse: 0.9411\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6405 - mse: 0.1555 - val_loss: 1.4262 - val_mse: 0.9408\n",
      "Epoch 165/500\n",
      "139/160 [=========================>....] - ETA: 0s - loss: 0.6379 - mse: 0.1529Restoring model weights from the end of the best epoch: 160.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6403 - mse: 0.1553 - val_loss: 1.4261 - val_mse: 0.9409\n",
      "Epoch 165: early stopping\n",
      "k=30, lambda_=5e-05, val_mse=[12.055388450622559, 8.378586769104004, 3.693565607070923, 2.3279972076416016, 1.8495858907699585, 1.6052656173706055, 1.4624971151351929, 1.3705942630767822, 1.3083494901657104, 1.2652689218521118, 1.2327486276626587, 1.2107447385787964, 1.1919161081314087, 1.1772572994232178, 1.1653581857681274, 1.1593971252441406, 1.1524919271469116, 1.1470394134521484, 1.140236735343933, 1.1372692584991455, 1.1334785223007202, 1.1304696798324585, 1.1272389888763428, 1.1241072416305542, 1.1210286617279053, 1.1173956394195557, 1.115496039390564, 1.112014889717102, 1.1103239059448242, 1.106909990310669, 1.1025153398513794, 1.0997587442398071, 1.0973886251449585, 1.0943851470947266, 1.0918289422988892, 1.08876633644104, 1.0845792293548584, 1.081811785697937, 1.079568862915039, 1.0740846395492554, 1.0727698802947998, 1.0686357021331787, 1.0662015676498413, 1.0618128776550293, 1.0583926439285278, 1.0550127029418945, 1.0525531768798828, 1.0493813753128052, 1.0462275743484497, 1.0442519187927246, 1.0416134595870972, 1.0379750728607178, 1.035567283630371, 1.032593846321106, 1.0298748016357422, 1.0262633562088013, 1.0229843854904175, 1.021109938621521, 1.0192739963531494, 1.0163134336471558, 1.014888048171997, 1.0128912925720215, 1.0103318691253662, 1.0085322856903076, 1.0068633556365967, 1.0045220851898193, 1.0022779703140259, 0.9998350739479065, 0.9991363883018494, 0.9965936541557312, 0.9954034090042114, 0.9933158755302429, 0.9908525347709656, 0.9898498058319092, 0.9890161752700806, 0.987417459487915, 0.9862686395645142, 0.9850117564201355, 0.9849480390548706, 0.9818637371063232, 0.9810738563537598, 0.9802736639976501, 0.9778507947921753, 0.9778550863265991, 0.9753625392913818, 0.9758816361427307, 0.9752026796340942, 0.973634660243988, 0.9728434681892395, 0.9720666408538818, 0.9702125787734985, 0.9701071381568909, 0.9691086411476135, 0.9685810804367065, 0.9676758646965027, 0.9664502143859863, 0.9658859372138977, 0.9649426341056824, 0.9652256369590759, 0.9637632369995117, 0.9631896615028381, 0.9624393582344055, 0.9624723792076111, 0.9613052606582642, 0.9611777663230896, 0.9605859518051147, 0.9596408605575562, 0.9595629572868347, 0.9585015773773193, 0.957371711730957, 0.9579823017120361, 0.9571019411087036, 0.956396222114563, 0.9555736184120178, 0.9547833204269409, 0.9551436305046082, 0.9542627334594727, 0.9536080360412598, 0.9535294771194458, 0.9528235197067261, 0.9530754089355469, 0.9522712230682373, 0.9517189860343933, 0.9512436389923096, 0.950799286365509, 0.9504699110984802, 0.9513529539108276, 0.9499160051345825, 0.9492015242576599, 0.9482018351554871, 0.9490944743156433, 0.9487975835800171, 0.9480977654457092, 0.9476972222328186, 0.9476901888847351, 0.9472048282623291, 0.9468592405319214, 0.9462341666221619, 0.9455928802490234, 0.9462419152259827, 0.9449173808097839, 0.9452400803565979, 0.9444424510002136, 0.9443193674087524, 0.9447386860847473, 0.944222629070282, 0.9447436332702637, 0.9445236921310425, 0.9439110159873962, 0.9435378909111023, 0.9432255029678345, 0.9427053928375244, 0.942660927772522, 0.9423238635063171, 0.9417484402656555, 0.9423635601997375, 0.9418660998344421, 0.9418226480484009, 0.9413170218467712, 0.9407564401626587, 0.9410399794578552, 0.9411187767982483, 0.9411264657974243, 0.9408407211303711, 0.9408636093139648]\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°\n",
      "TESTING... 30 | 2e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7036 - mse: 12.7008 - val_loss: 12.0041 - val_mse: 12.0008\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.2169 - mse: 10.2050 - val_loss: 7.6451 - val_mse: 7.6175\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.0410 - mse: 4.9878 - val_loss: 3.3857 - val_mse: 3.3062\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4986 - mse: 2.3991 - val_loss: 2.3052 - val_mse: 2.1884\n",
      "Epoch 5/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7729 - mse: 1.6430 - val_loss: 1.9080 - val_mse: 1.7662\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4458 - mse: 1.2940 - val_loss: 1.7078 - val_mse: 1.5468\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2617 - mse: 1.0925 - val_loss: 1.5933 - val_mse: 1.4164\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1465 - mse: 0.9630 - val_loss: 1.5211 - val_mse: 1.3313\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0680 - mse: 0.8727 - val_loss: 1.4746 - val_mse: 1.2742\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0118 - mse: 0.8067 - val_loss: 1.4433 - val_mse: 1.2337\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9696 - mse: 0.7562 - val_loss: 1.4217 - val_mse: 1.2046\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9371 - mse: 0.7166 - val_loss: 1.4056 - val_mse: 1.1819\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9109 - mse: 0.6845 - val_loss: 1.3963 - val_mse: 1.1672\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8896 - mse: 0.6583 - val_loss: 1.3877 - val_mse: 1.1541\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8717 - mse: 0.6363 - val_loss: 1.3814 - val_mse: 1.1441\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8559 - mse: 0.6170 - val_loss: 1.3791 - val_mse: 1.1388\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8423 - mse: 0.6007 - val_loss: 1.3744 - val_mse: 1.1315\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8300 - mse: 0.5860 - val_loss: 1.3729 - val_mse: 1.1280\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8184 - mse: 0.5726 - val_loss: 1.3698 - val_mse: 1.1231\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8082 - mse: 0.5607 - val_loss: 1.3687 - val_mse: 1.1205\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7978 - mse: 0.5492 - val_loss: 1.3673 - val_mse: 1.1182\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7886 - mse: 0.5391 - val_loss: 1.3667 - val_mse: 1.1167\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7792 - mse: 0.5290 - val_loss: 1.3658 - val_mse: 1.1151\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7702 - mse: 0.5194 - val_loss: 1.3671 - val_mse: 1.1162\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7611 - mse: 0.5101 - val_loss: 1.3652 - val_mse: 1.1138\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7524 - mse: 0.5010 - val_loss: 1.3638 - val_mse: 1.1123\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7432 - mse: 0.4917 - val_loss: 1.3640 - val_mse: 1.1125\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7341 - mse: 0.4825 - val_loss: 1.3641 - val_mse: 1.1125\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7249 - mse: 0.4734 - val_loss: 1.3634 - val_mse: 1.1116\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7156 - mse: 0.4639 - val_loss: 1.3623 - val_mse: 1.1105\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7064 - mse: 0.4547 - val_loss: 1.3633 - val_mse: 1.1113\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6966 - mse: 0.4447 - val_loss: 1.3618 - val_mse: 1.1098\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6869 - mse: 0.4350 - val_loss: 1.3639 - val_mse: 1.1118\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6769 - mse: 0.4249 - val_loss: 1.3620 - val_mse: 1.1099\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6671 - mse: 0.4150 - val_loss: 1.3602 - val_mse: 1.1078\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6570 - mse: 0.4047 - val_loss: 1.3602 - val_mse: 1.1075\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6469 - mse: 0.3944 - val_loss: 1.3617 - val_mse: 1.1088\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6366 - mse: 0.3838 - val_loss: 1.3618 - val_mse: 1.1085\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6267 - mse: 0.3735 - val_loss: 1.3619 - val_mse: 1.1082\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6168 - mse: 0.3632 - val_loss: 1.3631 - val_mse: 1.1090\n",
      "Epoch 41/500\n",
      "146/160 [==========================>...] - ETA: 0s - loss: 0.6050 - mse: 0.3510Restoring model weights from the end of the best epoch: 36.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6068 - mse: 0.3528 - val_loss: 1.3651 - val_mse: 1.1107\n",
      "Epoch 41: early stopping\n",
      "k=30, lambda_=2e-05, val_mse=[12.000773429870605, 7.617453575134277, 3.3061599731445312, 2.1883511543273926, 1.7661643028259277, 1.546751618385315, 1.4164142608642578, 1.331311821937561, 1.2741512060165405, 1.23371422290802, 1.204589605331421, 1.1819007396697998, 1.1671792268753052, 1.1541228294372559, 1.1440860033035278, 1.138815999031067, 1.1314584016799927, 1.1279724836349487, 1.1230593919754028, 1.1205447912216187, 1.1181535720825195, 1.1167384386062622, 1.1151399612426758, 1.1161565780639648, 1.1138265132904053, 1.1122589111328125, 1.1124539375305176, 1.1125404834747314, 1.111649513244629, 1.1105116605758667, 1.111341953277588, 1.1098216772079468, 1.1118420362472534, 1.1099408864974976, 1.1078091859817505, 1.1075164079666138, 1.1088247299194336, 1.1085463762283325, 1.1081870794296265, 1.1090320348739624, 1.1107007265090942]\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, best_model = grid_search(\n",
    "    train, \n",
    "    param_grid, \n",
    "    get_mf_bias_l2_reg_model,\n",
    "    nb_users, \n",
    "    nb_movies, \n",
    "    validation_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters : {'k': 15, 'lambda': 0.0002}\n",
      "Best validation RMSE : 0.9056389331817627\n"
     ]
    }
   ],
   "source": [
    "print('Best hyper-parameters : ' + str(best_params))\n",
    "print('Best validation RMSE : ' + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([447, 607, 447, ..., 431, 112, 473]),\n",
       " array([ 361, 1806, 2940, ..., 1896, 4674, 2771])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 466us/step\n",
      "Best model test RMSE : 0.9517949564279149 \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on all the dataset with the best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually other hyper-parameters such as the ones of SGD should also be grid-searched, like the number of epochs or the batch size. But that would be a bit long for this course. \n",
    "\n",
    "Now we want to do the best prediction possible, so retrain below your model on the whole dataset, including the test set, with the best values obtained from your grid search to make new predictions with our optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  6/178 [>.............................] - ETA: 1s - loss: 13.3633 - mse: 13.3386  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:21:35.944540: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 10ms/step - loss: 12.6753 - mse: 12.6662 - val_loss: 11.9444 - val_mse: 11.9312\n",
      "Epoch 2/500\n",
      "  1/178 [..............................] - ETA: 1s - loss: 12.0313 - mse: 12.0181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-11 21:21:37.713143: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 9ms/step - loss: 10.3518 - mse: 10.2732 - val_loss: 8.3101 - val_mse: 8.1157\n",
      "Epoch 3/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 6.1953 - mse: 5.8304 - val_loss: 4.8183 - val_mse: 4.2781\n",
      "Epoch 4/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 3.8969 - mse: 3.2160 - val_loss: 3.6623 - val_mse: 2.8599\n",
      "Epoch 5/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 3.1859 - mse: 2.2994 - val_loss: 3.2743 - val_mse: 2.3158\n",
      "Epoch 6/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.8959 - mse: 1.8840 - val_loss: 3.0817 - val_mse: 2.0202\n",
      "Epoch 7/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.7403 - mse: 1.6418 - val_loss: 2.9707 - val_mse: 1.8391\n",
      "Epoch 8/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.6447 - mse: 1.4870 - val_loss: 2.8970 - val_mse: 1.7157\n",
      "Epoch 9/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5796 - mse: 1.3796 - val_loss: 2.8430 - val_mse: 1.6277\n",
      "Epoch 10/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5311 - mse: 1.3038 - val_loss: 2.7977 - val_mse: 1.5604\n",
      "Epoch 11/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4921 - mse: 1.2477 - val_loss: 2.7593 - val_mse: 1.5082\n",
      "Epoch 12/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4593 - mse: 1.2040 - val_loss: 2.7247 - val_mse: 1.4666\n",
      "Epoch 13/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4297 - mse: 1.1705 - val_loss: 2.6922 - val_mse: 1.4326\n",
      "Epoch 14/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4023 - mse: 1.1434 - val_loss: 2.6618 - val_mse: 1.4038\n",
      "Epoch 15/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.3769 - mse: 1.1220 - val_loss: 2.6314 - val_mse: 1.3790\n",
      "Epoch 16/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 2.3520 - mse: 1.1035 - val_loss: 2.6001 - val_mse: 1.3552\n",
      "Epoch 17/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 2.3280 - mse: 1.0876 - val_loss: 2.5695 - val_mse: 1.3343\n",
      "Epoch 18/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.3046 - mse: 1.0753 - val_loss: 2.5394 - val_mse: 1.3151\n",
      "Epoch 19/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2813 - mse: 1.0636 - val_loss: 2.5098 - val_mse: 1.2979\n",
      "Epoch 20/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2584 - mse: 1.0531 - val_loss: 2.4802 - val_mse: 1.2816\n",
      "Epoch 21/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2359 - mse: 1.0441 - val_loss: 2.4509 - val_mse: 1.2658\n",
      "Epoch 22/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2139 - mse: 1.0360 - val_loss: 2.4211 - val_mse: 1.2502\n",
      "Epoch 23/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1919 - mse: 1.0286 - val_loss: 2.3933 - val_mse: 1.2361\n",
      "Epoch 24/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1704 - mse: 1.0203 - val_loss: 2.3651 - val_mse: 1.2232\n",
      "Epoch 25/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1494 - mse: 1.0143 - val_loss: 2.3374 - val_mse: 1.2100\n",
      "Epoch 26/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1289 - mse: 1.0083 - val_loss: 2.3104 - val_mse: 1.1968\n",
      "Epoch 27/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1085 - mse: 1.0017 - val_loss: 2.2846 - val_mse: 1.1856\n",
      "Epoch 28/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0887 - mse: 0.9964 - val_loss: 2.2590 - val_mse: 1.1729\n",
      "Epoch 29/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0696 - mse: 0.9898 - val_loss: 2.2354 - val_mse: 1.1623\n",
      "Epoch 30/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0509 - mse: 0.9849 - val_loss: 2.2107 - val_mse: 1.1499\n",
      "Epoch 31/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0326 - mse: 0.9786 - val_loss: 2.1879 - val_mse: 1.1400\n",
      "Epoch 32/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0151 - mse: 0.9727 - val_loss: 2.1672 - val_mse: 1.1314\n",
      "Epoch 33/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9981 - mse: 0.9671 - val_loss: 2.1464 - val_mse: 1.1216\n",
      "Epoch 34/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9818 - mse: 0.9626 - val_loss: 2.1257 - val_mse: 1.1113\n",
      "Epoch 35/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9658 - mse: 0.9568 - val_loss: 2.1069 - val_mse: 1.1032\n",
      "Epoch 36/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9506 - mse: 0.9510 - val_loss: 2.0887 - val_mse: 1.0945\n",
      "Epoch 37/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9360 - mse: 0.9464 - val_loss: 2.0711 - val_mse: 1.0858\n",
      "Epoch 38/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9218 - mse: 0.9407 - val_loss: 2.0537 - val_mse: 1.0765\n",
      "Epoch 39/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9082 - mse: 0.9343 - val_loss: 2.0385 - val_mse: 1.0693\n",
      "Epoch 40/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8952 - mse: 0.9292 - val_loss: 2.0241 - val_mse: 1.0618\n",
      "Epoch 41/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8828 - mse: 0.9236 - val_loss: 2.0105 - val_mse: 1.0550\n",
      "Epoch 42/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8709 - mse: 0.9184 - val_loss: 1.9977 - val_mse: 1.0478\n",
      "Epoch 43/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8594 - mse: 0.9125 - val_loss: 1.9850 - val_mse: 1.0412\n",
      "Epoch 44/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8486 - mse: 0.9069 - val_loss: 1.9737 - val_mse: 1.0348\n",
      "Epoch 45/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8383 - mse: 0.9009 - val_loss: 1.9634 - val_mse: 1.0291\n",
      "Epoch 46/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8286 - mse: 0.8960 - val_loss: 1.9531 - val_mse: 1.0229\n",
      "Epoch 47/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8194 - mse: 0.8909 - val_loss: 1.9432 - val_mse: 1.0172\n",
      "Epoch 48/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8108 - mse: 0.8865 - val_loss: 1.9347 - val_mse: 1.0119\n",
      "Epoch 49/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8026 - mse: 0.8813 - val_loss: 1.9259 - val_mse: 1.0065\n",
      "Epoch 50/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7949 - mse: 0.8767 - val_loss: 1.9186 - val_mse: 1.0017\n",
      "Epoch 51/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7877 - mse: 0.8723 - val_loss: 1.9114 - val_mse: 0.9972\n",
      "Epoch 52/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7809 - mse: 0.8676 - val_loss: 1.9047 - val_mse: 0.9926\n",
      "Epoch 53/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7745 - mse: 0.8632 - val_loss: 1.8987 - val_mse: 0.9893\n",
      "Epoch 54/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7682 - mse: 0.8595 - val_loss: 1.8931 - val_mse: 0.9855\n",
      "Epoch 55/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7625 - mse: 0.8551 - val_loss: 1.8878 - val_mse: 0.9818\n",
      "Epoch 56/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7570 - mse: 0.8520 - val_loss: 1.8823 - val_mse: 0.9776\n",
      "Epoch 57/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7519 - mse: 0.8476 - val_loss: 1.8778 - val_mse: 0.9739\n",
      "Epoch 58/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7472 - mse: 0.8441 - val_loss: 1.8737 - val_mse: 0.9712\n",
      "Epoch 59/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7426 - mse: 0.8406 - val_loss: 1.8699 - val_mse: 0.9686\n",
      "Epoch 60/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7382 - mse: 0.8370 - val_loss: 1.8665 - val_mse: 0.9655\n",
      "Epoch 61/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7341 - mse: 0.8336 - val_loss: 1.8629 - val_mse: 0.9628\n",
      "Epoch 62/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7303 - mse: 0.8307 - val_loss: 1.8595 - val_mse: 0.9593\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7265 - mse: 0.8263 - val_loss: 1.8568 - val_mse: 0.9574\n",
      "Epoch 64/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7231 - mse: 0.8233 - val_loss: 1.8539 - val_mse: 0.9546\n",
      "Epoch 65/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7197 - mse: 0.8205 - val_loss: 1.8515 - val_mse: 0.9523\n",
      "Epoch 66/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7166 - mse: 0.8167 - val_loss: 1.8491 - val_mse: 0.9500\n",
      "Epoch 67/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7136 - mse: 0.8145 - val_loss: 1.8475 - val_mse: 0.9483\n",
      "Epoch 68/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7108 - mse: 0.8108 - val_loss: 1.8455 - val_mse: 0.9457\n",
      "Epoch 69/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7080 - mse: 0.8081 - val_loss: 1.8439 - val_mse: 0.9444\n",
      "Epoch 70/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7056 - mse: 0.8053 - val_loss: 1.8425 - val_mse: 0.9425\n",
      "Epoch 71/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7032 - mse: 0.8026 - val_loss: 1.8417 - val_mse: 0.9414\n",
      "Epoch 72/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7008 - mse: 0.7996 - val_loss: 1.8404 - val_mse: 0.9387\n",
      "Epoch 73/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6987 - mse: 0.7965 - val_loss: 1.8394 - val_mse: 0.9374\n",
      "Epoch 74/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6966 - mse: 0.7941 - val_loss: 1.8387 - val_mse: 0.9365\n",
      "Epoch 75/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6946 - mse: 0.7918 - val_loss: 1.8380 - val_mse: 0.9351\n",
      "Epoch 76/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6928 - mse: 0.7890 - val_loss: 1.8372 - val_mse: 0.9333\n",
      "Epoch 77/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6910 - mse: 0.7863 - val_loss: 1.8366 - val_mse: 0.9321\n",
      "Epoch 78/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6894 - mse: 0.7841 - val_loss: 1.8360 - val_mse: 0.9310\n",
      "Epoch 79/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6879 - mse: 0.7818 - val_loss: 1.8358 - val_mse: 0.9294\n",
      "Epoch 80/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6863 - mse: 0.7792 - val_loss: 1.8354 - val_mse: 0.9281\n",
      "Epoch 81/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6850 - mse: 0.7773 - val_loss: 1.8351 - val_mse: 0.9269\n",
      "Epoch 82/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6837 - mse: 0.7747 - val_loss: 1.8347 - val_mse: 0.9257\n",
      "Epoch 83/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6824 - mse: 0.7733 - val_loss: 1.8341 - val_mse: 0.9245\n",
      "Epoch 84/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6812 - mse: 0.7710 - val_loss: 1.8342 - val_mse: 0.9236\n",
      "Epoch 85/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6801 - mse: 0.7687 - val_loss: 1.8343 - val_mse: 0.9227\n",
      "Epoch 86/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6790 - mse: 0.7667 - val_loss: 1.8343 - val_mse: 0.9220\n",
      "Epoch 87/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6780 - mse: 0.7648 - val_loss: 1.8341 - val_mse: 0.9209\n",
      "Epoch 88/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6771 - mse: 0.7634 - val_loss: 1.8344 - val_mse: 0.9203\n",
      "Epoch 89/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6762 - mse: 0.7612 - val_loss: 1.8345 - val_mse: 0.9197\n",
      "Epoch 90/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6754 - mse: 0.7598 - val_loss: 1.8342 - val_mse: 0.9186\n",
      "Epoch 91/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6745 - mse: 0.7579 - val_loss: 1.8344 - val_mse: 0.9182\n",
      "Epoch 92/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6737 - mse: 0.7565 - val_loss: 1.8347 - val_mse: 0.9174\n",
      "Epoch 93/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6731 - mse: 0.7554 - val_loss: 1.8349 - val_mse: 0.9169\n",
      "Epoch 94/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6724 - mse: 0.7534 - val_loss: 1.8350 - val_mse: 0.9162\n",
      "Epoch 95/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6718 - mse: 0.7523 - val_loss: 1.8353 - val_mse: 0.9156\n",
      "Epoch 96/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6712 - mse: 0.7508 - val_loss: 1.8355 - val_mse: 0.9152\n",
      "Epoch 97/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6706 - mse: 0.7501 - val_loss: 1.8356 - val_mse: 0.9145\n",
      "Epoch 98/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6699 - mse: 0.7484 - val_loss: 1.8358 - val_mse: 0.9137\n",
      "Epoch 99/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6695 - mse: 0.7468 - val_loss: 1.8358 - val_mse: 0.9132\n",
      "Epoch 100/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6690 - mse: 0.7457 - val_loss: 1.8357 - val_mse: 0.9128\n",
      "Epoch 101/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6685 - mse: 0.7447 - val_loss: 1.8361 - val_mse: 0.9125\n",
      "Epoch 102/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6680 - mse: 0.7438 - val_loss: 1.8362 - val_mse: 0.9115\n",
      "Epoch 103/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6676 - mse: 0.7430 - val_loss: 1.8365 - val_mse: 0.9112\n",
      "Epoch 104/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6672 - mse: 0.7412 - val_loss: 1.8363 - val_mse: 0.9105\n",
      "Epoch 105/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6667 - mse: 0.7406 - val_loss: 1.8369 - val_mse: 0.9105\n",
      "Epoch 106/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6664 - mse: 0.7395 - val_loss: 1.8371 - val_mse: 0.9096\n",
      "Epoch 107/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6661 - mse: 0.7384 - val_loss: 1.8371 - val_mse: 0.9090\n",
      "Epoch 108/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6657 - mse: 0.7372 - val_loss: 1.8376 - val_mse: 0.9092\n",
      "Epoch 109/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6654 - mse: 0.7370 - val_loss: 1.8377 - val_mse: 0.9087\n",
      "Epoch 110/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6650 - mse: 0.7356 - val_loss: 1.8377 - val_mse: 0.9082\n",
      "Epoch 111/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6647 - mse: 0.7351 - val_loss: 1.8380 - val_mse: 0.9080\n",
      "Epoch 112/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6645 - mse: 0.7336 - val_loss: 1.8385 - val_mse: 0.9078\n",
      "Epoch 113/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6643 - mse: 0.7329 - val_loss: 1.8384 - val_mse: 0.9070\n",
      "Epoch 114/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6639 - mse: 0.7325 - val_loss: 1.8388 - val_mse: 0.9068\n",
      "Epoch 115/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6637 - mse: 0.7312 - val_loss: 1.8391 - val_mse: 0.9066\n",
      "Epoch 116/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6634 - mse: 0.7302 - val_loss: 1.8391 - val_mse: 0.9064\n",
      "Epoch 117/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6632 - mse: 0.7297 - val_loss: 1.8396 - val_mse: 0.9063\n",
      "Epoch 118/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6631 - mse: 0.7290 - val_loss: 1.8397 - val_mse: 0.9057\n",
      "Epoch 119/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6629 - mse: 0.7286 - val_loss: 1.8399 - val_mse: 0.9055\n",
      "Epoch 120/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6626 - mse: 0.7274 - val_loss: 1.8406 - val_mse: 0.9057\n",
      "Epoch 121/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6625 - mse: 0.7271 - val_loss: 1.8406 - val_mse: 0.9052\n",
      "Epoch 122/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6623 - mse: 0.7265 - val_loss: 1.8409 - val_mse: 0.9048\n",
      "Epoch 123/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6620 - mse: 0.7255 - val_loss: 1.8413 - val_mse: 0.9051\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6619 - mse: 0.7252 - val_loss: 1.8416 - val_mse: 0.9048\n",
      "Epoch 125/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6617 - mse: 0.7244 - val_loss: 1.8417 - val_mse: 0.9047\n",
      "Epoch 126/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6616 - mse: 0.7239 - val_loss: 1.8421 - val_mse: 0.9045\n",
      "Epoch 127/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6614 - mse: 0.7233 - val_loss: 1.8421 - val_mse: 0.9040\n",
      "Epoch 128/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6613 - mse: 0.7230 - val_loss: 1.8427 - val_mse: 0.9041\n",
      "Epoch 129/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6611 - mse: 0.7222 - val_loss: 1.8433 - val_mse: 0.9043\n",
      "Epoch 130/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6610 - mse: 0.7215 - val_loss: 1.8433 - val_mse: 0.9042\n",
      "Epoch 131/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6609 - mse: 0.7212 - val_loss: 1.8434 - val_mse: 0.9038\n",
      "Epoch 132/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6609 - mse: 0.7210 - val_loss: 1.8437 - val_mse: 0.9040\n",
      "Epoch 133/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6607 - mse: 0.7203 - val_loss: 1.8442 - val_mse: 0.9038\n",
      "Epoch 134/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6607 - mse: 0.7200 - val_loss: 1.8440 - val_mse: 0.9033\n",
      "Epoch 135/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6605 - mse: 0.7191 - val_loss: 1.8444 - val_mse: 0.9033\n",
      "Epoch 136/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6603 - mse: 0.7189 - val_loss: 1.8445 - val_mse: 0.9033\n",
      "Epoch 137/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6602 - mse: 0.7185 - val_loss: 1.8448 - val_mse: 0.9032\n",
      "Epoch 138/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6602 - mse: 0.7182 - val_loss: 1.8452 - val_mse: 0.9030\n",
      "Epoch 139/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6601 - mse: 0.7177 - val_loss: 1.8453 - val_mse: 0.9026\n",
      "Epoch 140/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6600 - mse: 0.7170 - val_loss: 1.8456 - val_mse: 0.9027\n",
      "Epoch 141/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6600 - mse: 0.7165 - val_loss: 1.8459 - val_mse: 0.9026\n",
      "Epoch 142/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6599 - mse: 0.7164 - val_loss: 1.8457 - val_mse: 0.9021\n",
      "Epoch 143/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6598 - mse: 0.7160 - val_loss: 1.8458 - val_mse: 0.9020\n",
      "Epoch 144/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6597 - mse: 0.7158 - val_loss: 1.8461 - val_mse: 0.9022\n",
      "Epoch 145/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6596 - mse: 0.7154 - val_loss: 1.8465 - val_mse: 0.9021\n",
      "Epoch 146/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6596 - mse: 0.7149 - val_loss: 1.8466 - val_mse: 0.9021\n",
      "Epoch 147/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6596 - mse: 0.7147 - val_loss: 1.8470 - val_mse: 0.9021\n",
      "Epoch 148/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6594 - mse: 0.7141 - val_loss: 1.8471 - val_mse: 0.9022\n",
      "Epoch 149/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6595 - mse: 0.7139 - val_loss: 1.8476 - val_mse: 0.9028\n",
      "Epoch 150/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6593 - mse: 0.7138 - val_loss: 1.8477 - val_mse: 0.9021\n",
      "Epoch 151/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6593 - mse: 0.7138 - val_loss: 1.8481 - val_mse: 0.9023\n",
      "Epoch 152/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6592 - mse: 0.7129 - val_loss: 1.8481 - val_mse: 0.9023\n",
      "Epoch 153/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6592 - mse: 0.7130 - val_loss: 1.8482 - val_mse: 0.9017\n",
      "Epoch 154/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6591 - mse: 0.7123 - val_loss: 1.8482 - val_mse: 0.9017\n",
      "Epoch 155/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6591 - mse: 0.7126 - val_loss: 1.8487 - val_mse: 0.9022\n",
      "Epoch 156/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6591 - mse: 0.7120 - val_loss: 1.8488 - val_mse: 0.9019\n",
      "Epoch 157/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6590 - mse: 0.7119 - val_loss: 1.8486 - val_mse: 0.9015\n",
      "Epoch 158/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6589 - mse: 0.7114 - val_loss: 1.8489 - val_mse: 0.9014\n",
      "Epoch 159/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6589 - mse: 0.7110 - val_loss: 1.8492 - val_mse: 0.9016\n",
      "Epoch 160/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6589 - mse: 0.7109 - val_loss: 1.8493 - val_mse: 0.9014\n",
      "Epoch 161/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6588 - mse: 0.7109 - val_loss: 1.8494 - val_mse: 0.9016\n",
      "Epoch 162/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6588 - mse: 0.7106 - val_loss: 1.8493 - val_mse: 0.9013\n",
      "Epoch 163/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6587 - mse: 0.7098 - val_loss: 1.8497 - val_mse: 0.9013\n",
      "Epoch 164/500\n",
      "178/178 [==============================] - 2s 11ms/step - loss: 1.6587 - mse: 0.7101 - val_loss: 1.8500 - val_mse: 0.9011\n",
      "Epoch 165/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6587 - mse: 0.7095 - val_loss: 1.8502 - val_mse: 0.9011\n",
      "Epoch 166/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6587 - mse: 0.7094 - val_loss: 1.8504 - val_mse: 0.9014\n",
      "Epoch 167/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6586 - mse: 0.7089 - val_loss: 1.8506 - val_mse: 0.9012\n",
      "Epoch 168/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6585 - mse: 0.7089 - val_loss: 1.8505 - val_mse: 0.9015\n",
      "Epoch 169/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6586 - mse: 0.7090 - val_loss: 1.8509 - val_mse: 0.9014\n",
      "Epoch 170/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6584 - mse: 0.7085 - val_loss: 1.8512 - val_mse: 0.9010\n",
      "Epoch 171/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6585 - mse: 0.7084 - val_loss: 1.8514 - val_mse: 0.9011\n",
      "Epoch 172/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6584 - mse: 0.7081 - val_loss: 1.8514 - val_mse: 0.9011\n",
      "Epoch 173/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6584 - mse: 0.7075 - val_loss: 1.8516 - val_mse: 0.9009\n",
      "Epoch 174/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6584 - mse: 0.7074 - val_loss: 1.8513 - val_mse: 0.9009\n",
      "Epoch 175/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6583 - mse: 0.7076 - val_loss: 1.8518 - val_mse: 0.9014\n",
      "Epoch 176/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6583 - mse: 0.7078 - val_loss: 1.8519 - val_mse: 0.9011\n",
      "Epoch 177/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6583 - mse: 0.7073 - val_loss: 1.8519 - val_mse: 0.9008\n",
      "Epoch 178/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6583 - mse: 0.7069 - val_loss: 1.8519 - val_mse: 0.9008\n",
      "Epoch 179/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6582 - mse: 0.7069 - val_loss: 1.8524 - val_mse: 0.9007\n",
      "Epoch 180/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6582 - mse: 0.7061 - val_loss: 1.8526 - val_mse: 0.9011\n",
      "Epoch 181/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6581 - mse: 0.7062 - val_loss: 1.8526 - val_mse: 0.9010\n",
      "Epoch 182/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6582 - mse: 0.7063 - val_loss: 1.8528 - val_mse: 0.9011\n",
      "Epoch 183/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6581 - mse: 0.7059 - val_loss: 1.8528 - val_mse: 0.9008\n",
      "Epoch 184/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6581 - mse: 0.7057 - val_loss: 1.8531 - val_mse: 0.9010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7056 - val_loss: 1.8533 - val_mse: 0.9011\n",
      "Epoch 186/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7057 - val_loss: 1.8532 - val_mse: 0.9009\n",
      "Epoch 187/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7055 - val_loss: 1.8532 - val_mse: 0.9006\n",
      "Epoch 188/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7049 - val_loss: 1.8534 - val_mse: 0.9007\n",
      "Epoch 189/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7053 - val_loss: 1.8536 - val_mse: 0.9008\n",
      "Epoch 190/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6580 - mse: 0.7052 - val_loss: 1.8532 - val_mse: 0.9000\n",
      "Epoch 191/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6579 - mse: 0.7046 - val_loss: 1.8539 - val_mse: 0.9006\n",
      "Epoch 192/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6579 - mse: 0.7046 - val_loss: 1.8536 - val_mse: 0.9003\n",
      "Epoch 193/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6578 - mse: 0.7040 - val_loss: 1.8537 - val_mse: 0.9003\n",
      "Epoch 194/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6579 - mse: 0.7041 - val_loss: 1.8539 - val_mse: 0.9005\n",
      "Epoch 195/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6579 - mse: 0.7041 - val_loss: 1.8543 - val_mse: 0.9007\n",
      "Epoch 196/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6578 - mse: 0.7037 - val_loss: 1.8540 - val_mse: 0.9002\n",
      "Epoch 197/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6578 - mse: 0.7037 - val_loss: 1.8543 - val_mse: 0.9004\n",
      "Epoch 198/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6578 - mse: 0.7039 - val_loss: 1.8544 - val_mse: 0.9005\n",
      "Epoch 199/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6578 - mse: 0.7037 - val_loss: 1.8545 - val_mse: 0.9002\n",
      "Epoch 200/500\n",
      "175/178 [============================>.] - ETA: 0s - loss: 1.6573 - mse: 0.7025Restoring model weights from the end of the best epoch: 190.\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6577 - mse: 0.7030 - val_loss: 1.8550 - val_mse: 0.9008\n",
      "Epoch 200: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "X = [dataset[\"userId\"].to_numpy(), dataset[\"movieId\"].to_numpy()]\n",
    "y = dataset[\"rating\"].to_numpy()\n",
    "\n",
    "best_model=get_mf_bias_l2_reg_model(nb_users, nb_movies, k = best_params['k'], lambda_ = best_params[\"lambda\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    best_model.fit(X, y, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152/3152 [==============================] - 10s 3ms/step\n",
      "Best model test RMSE : 0.8422931194848687 \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    y_pred = best_model.predict(X)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend the top-5 movies for the 10 first users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your retrained best model with optimal hyper parameters, compute the predictions for all the ratings that are not in the `dataset` for the 10 first users (indexes from 0 to 9). That means all the movies $i$ that these users $u \\in 0,\\ldots,9$ haven't rated, thus all the $u,i$ combinations that are not in the `dataset` dataframe rows.\n",
    "\n",
    "Order these predicted ratings for these users by decreasing order, and print out the 5 first ones, i.e. the ones that have the highest predicted ratings. Use the *movies.csv* file to print the real titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9742, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title = pd.read_csv(\"../data/ml-latest-small/movies.csv\")\n",
    "data = pd.read_csv(\"../data/ml-latest-small/ratings.csv\")\n",
    "movie_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 3: 1,\n",
       " 6: 2,\n",
       " 47: 3,\n",
       " 50: 4,\n",
       " 70: 5,\n",
       " 101: 6,\n",
       " 110: 7,\n",
       " 151: 8,\n",
       " 157: 9,\n",
       " 163: 10,\n",
       " 216: 11,\n",
       " 223: 12,\n",
       " 231: 13,\n",
       " 235: 14,\n",
       " 260: 15,\n",
       " 296: 16,\n",
       " 316: 17,\n",
       " 333: 18,\n",
       " 349: 19,\n",
       " 356: 20,\n",
       " 362: 21,\n",
       " 367: 22,\n",
       " 423: 23,\n",
       " 441: 24,\n",
       " 457: 25,\n",
       " 480: 26,\n",
       " 500: 27,\n",
       " 527: 28,\n",
       " 543: 29,\n",
       " 552: 30,\n",
       " 553: 31,\n",
       " 590: 32,\n",
       " 592: 33,\n",
       " 593: 34,\n",
       " 596: 35,\n",
       " 608: 36,\n",
       " 648: 37,\n",
       " 661: 38,\n",
       " 673: 39,\n",
       " 733: 40,\n",
       " 736: 41,\n",
       " 780: 42,\n",
       " 804: 43,\n",
       " 919: 44,\n",
       " 923: 45,\n",
       " 940: 46,\n",
       " 943: 47,\n",
       " 954: 48,\n",
       " 1009: 49,\n",
       " 1023: 50,\n",
       " 1024: 51,\n",
       " 1025: 52,\n",
       " 1029: 53,\n",
       " 1030: 54,\n",
       " 1031: 55,\n",
       " 1032: 56,\n",
       " 1042: 57,\n",
       " 1049: 58,\n",
       " 1060: 59,\n",
       " 1073: 60,\n",
       " 1080: 61,\n",
       " 1089: 62,\n",
       " 1090: 63,\n",
       " 1092: 64,\n",
       " 1097: 65,\n",
       " 1127: 66,\n",
       " 1136: 67,\n",
       " 1196: 68,\n",
       " 1197: 69,\n",
       " 1198: 70,\n",
       " 1206: 71,\n",
       " 1208: 72,\n",
       " 1210: 73,\n",
       " 1213: 74,\n",
       " 1214: 75,\n",
       " 1219: 76,\n",
       " 1220: 77,\n",
       " 1222: 78,\n",
       " 1224: 79,\n",
       " 1226: 80,\n",
       " 1240: 81,\n",
       " 1256: 82,\n",
       " 1258: 83,\n",
       " 1265: 84,\n",
       " 1270: 85,\n",
       " 1275: 86,\n",
       " 1278: 87,\n",
       " 1282: 88,\n",
       " 1291: 89,\n",
       " 1298: 90,\n",
       " 1348: 91,\n",
       " 1377: 92,\n",
       " 1396: 93,\n",
       " 1408: 94,\n",
       " 1445: 95,\n",
       " 1473: 96,\n",
       " 1500: 97,\n",
       " 1517: 98,\n",
       " 1552: 99,\n",
       " 1573: 100,\n",
       " 1580: 101,\n",
       " 1587: 102,\n",
       " 1617: 103,\n",
       " 1620: 104,\n",
       " 1625: 105,\n",
       " 1644: 106,\n",
       " 1676: 107,\n",
       " 1732: 108,\n",
       " 1777: 109,\n",
       " 1793: 110,\n",
       " 1804: 111,\n",
       " 1805: 112,\n",
       " 1920: 113,\n",
       " 1927: 114,\n",
       " 1954: 115,\n",
       " 1967: 116,\n",
       " 2000: 117,\n",
       " 2005: 118,\n",
       " 2012: 119,\n",
       " 2018: 120,\n",
       " 2028: 121,\n",
       " 2033: 122,\n",
       " 2046: 123,\n",
       " 2048: 124,\n",
       " 2054: 125,\n",
       " 2058: 126,\n",
       " 2078: 127,\n",
       " 2090: 128,\n",
       " 2093: 129,\n",
       " 2094: 130,\n",
       " 2096: 131,\n",
       " 2099: 132,\n",
       " 2105: 133,\n",
       " 2115: 134,\n",
       " 2116: 135,\n",
       " 2137: 136,\n",
       " 2139: 137,\n",
       " 2141: 138,\n",
       " 2143: 139,\n",
       " 2161: 140,\n",
       " 2174: 141,\n",
       " 2193: 142,\n",
       " 2253: 143,\n",
       " 2268: 144,\n",
       " 2273: 145,\n",
       " 2291: 146,\n",
       " 2329: 147,\n",
       " 2338: 148,\n",
       " 2353: 149,\n",
       " 2366: 150,\n",
       " 2387: 151,\n",
       " 2389: 152,\n",
       " 2395: 153,\n",
       " 2406: 154,\n",
       " 2414: 155,\n",
       " 2427: 156,\n",
       " 2450: 157,\n",
       " 2459: 158,\n",
       " 2470: 159,\n",
       " 2478: 160,\n",
       " 2492: 161,\n",
       " 2502: 162,\n",
       " 2528: 163,\n",
       " 2529: 164,\n",
       " 2542: 165,\n",
       " 2571: 166,\n",
       " 2580: 167,\n",
       " 2596: 168,\n",
       " 2616: 169,\n",
       " 2617: 170,\n",
       " 2628: 171,\n",
       " 2640: 172,\n",
       " 2641: 173,\n",
       " 2644: 174,\n",
       " 2648: 175,\n",
       " 2654: 176,\n",
       " 2657: 177,\n",
       " 2692: 178,\n",
       " 2700: 179,\n",
       " 2716: 180,\n",
       " 2761: 181,\n",
       " 2797: 182,\n",
       " 2826: 183,\n",
       " 2858: 184,\n",
       " 2872: 185,\n",
       " 2899: 186,\n",
       " 2916: 187,\n",
       " 2944: 188,\n",
       " 2947: 189,\n",
       " 2948: 190,\n",
       " 2949: 191,\n",
       " 2959: 192,\n",
       " 2985: 193,\n",
       " 2987: 194,\n",
       " 2991: 195,\n",
       " 2993: 196,\n",
       " 2997: 197,\n",
       " 3033: 198,\n",
       " 3034: 199,\n",
       " 3052: 200,\n",
       " 3053: 201,\n",
       " 3062: 202,\n",
       " 3147: 203,\n",
       " 3168: 204,\n",
       " 3176: 205,\n",
       " 3243: 206,\n",
       " 3247: 207,\n",
       " 3253: 208,\n",
       " 3273: 209,\n",
       " 3386: 210,\n",
       " 3439: 211,\n",
       " 3440: 212,\n",
       " 3441: 213,\n",
       " 3448: 214,\n",
       " 3450: 215,\n",
       " 3479: 216,\n",
       " 3489: 217,\n",
       " 3527: 218,\n",
       " 3578: 219,\n",
       " 3617: 220,\n",
       " 3639: 221,\n",
       " 3671: 222,\n",
       " 3702: 223,\n",
       " 3703: 224,\n",
       " 3729: 225,\n",
       " 3740: 226,\n",
       " 3744: 227,\n",
       " 3793: 228,\n",
       " 3809: 229,\n",
       " 4006: 230,\n",
       " 5060: 231,\n",
       " 318: 232,\n",
       " 1704: 233,\n",
       " 6874: 234,\n",
       " 8798: 235,\n",
       " 46970: 236,\n",
       " 48516: 237,\n",
       " 58559: 238,\n",
       " 60756: 239,\n",
       " 68157: 240,\n",
       " 71535: 241,\n",
       " 74458: 242,\n",
       " 77455: 243,\n",
       " 79132: 244,\n",
       " 80489: 245,\n",
       " 80906: 246,\n",
       " 86345: 247,\n",
       " 89774: 248,\n",
       " 91529: 249,\n",
       " 91658: 250,\n",
       " 99114: 251,\n",
       " 106782: 252,\n",
       " 109487: 253,\n",
       " 112552: 254,\n",
       " 114060: 255,\n",
       " 115713: 256,\n",
       " 122882: 257,\n",
       " 131724: 258,\n",
       " 31: 259,\n",
       " 647: 260,\n",
       " 688: 261,\n",
       " 720: 262,\n",
       " 849: 263,\n",
       " 914: 264,\n",
       " 1093: 265,\n",
       " 1124: 266,\n",
       " 1263: 267,\n",
       " 1272: 268,\n",
       " 1302: 269,\n",
       " 1371: 270,\n",
       " 2080: 271,\n",
       " 2288: 272,\n",
       " 2424: 273,\n",
       " 2851: 274,\n",
       " 3024: 275,\n",
       " 3210: 276,\n",
       " 3949: 277,\n",
       " 4518: 278,\n",
       " 5048: 279,\n",
       " 5181: 280,\n",
       " 5746: 281,\n",
       " 5764: 282,\n",
       " 5919: 283,\n",
       " 6238: 284,\n",
       " 6835: 285,\n",
       " 7899: 286,\n",
       " 7991: 287,\n",
       " 26409: 288,\n",
       " 70946: 289,\n",
       " 72378: 290,\n",
       " 21: 291,\n",
       " 32: 292,\n",
       " 45: 293,\n",
       " 52: 294,\n",
       " 58: 295,\n",
       " 106: 296,\n",
       " 125: 297,\n",
       " 126: 298,\n",
       " 162: 299,\n",
       " 171: 300,\n",
       " 176: 301,\n",
       " 190: 302,\n",
       " 215: 303,\n",
       " 222: 304,\n",
       " 232: 305,\n",
       " 247: 306,\n",
       " 265: 307,\n",
       " 319: 308,\n",
       " 342: 309,\n",
       " 345: 310,\n",
       " 348: 311,\n",
       " 351: 312,\n",
       " 357: 313,\n",
       " 368: 314,\n",
       " 417: 315,\n",
       " 450: 316,\n",
       " 475: 317,\n",
       " 492: 318,\n",
       " 509: 319,\n",
       " 538: 320,\n",
       " 539: 321,\n",
       " 588: 322,\n",
       " 595: 323,\n",
       " 599: 324,\n",
       " 708: 325,\n",
       " 759: 326,\n",
       " 800: 327,\n",
       " 892: 328,\n",
       " 898: 329,\n",
       " 899: 330,\n",
       " 902: 331,\n",
       " 904: 332,\n",
       " 908: 333,\n",
       " 910: 334,\n",
       " 912: 335,\n",
       " 920: 336,\n",
       " 930: 337,\n",
       " 937: 338,\n",
       " 1046: 339,\n",
       " 1057: 340,\n",
       " 1077: 341,\n",
       " 1079: 342,\n",
       " 1084: 343,\n",
       " 1086: 344,\n",
       " 1094: 345,\n",
       " 1103: 346,\n",
       " 1179: 347,\n",
       " 1183: 348,\n",
       " 1188: 349,\n",
       " 1199: 350,\n",
       " 1203: 351,\n",
       " 1211: 352,\n",
       " 1225: 353,\n",
       " 1250: 354,\n",
       " 1259: 355,\n",
       " 1266: 356,\n",
       " 1279: 357,\n",
       " 1283: 358,\n",
       " 1288: 359,\n",
       " 1304: 360,\n",
       " 1391: 361,\n",
       " 1449: 362,\n",
       " 1466: 363,\n",
       " 1597: 364,\n",
       " 1641: 365,\n",
       " 1719: 366,\n",
       " 1733: 367,\n",
       " 1734: 368,\n",
       " 1834: 369,\n",
       " 1860: 370,\n",
       " 1883: 371,\n",
       " 1885: 372,\n",
       " 1892: 373,\n",
       " 1895: 374,\n",
       " 1907: 375,\n",
       " 1914: 376,\n",
       " 1916: 377,\n",
       " 1923: 378,\n",
       " 1947: 379,\n",
       " 1966: 380,\n",
       " 1968: 381,\n",
       " 2019: 382,\n",
       " 2076: 383,\n",
       " 2109: 384,\n",
       " 2145: 385,\n",
       " 2150: 386,\n",
       " 2186: 387,\n",
       " 2203: 388,\n",
       " 2204: 389,\n",
       " 2282: 390,\n",
       " 2324: 391,\n",
       " 2336: 392,\n",
       " 2351: 393,\n",
       " 2359: 394,\n",
       " 2390: 395,\n",
       " 2467: 396,\n",
       " 2583: 397,\n",
       " 2599: 398,\n",
       " 2683: 399,\n",
       " 2712: 400,\n",
       " 2762: 401,\n",
       " 2763: 402,\n",
       " 2770: 403,\n",
       " 2791: 404,\n",
       " 2843: 405,\n",
       " 2874: 406,\n",
       " 2921: 407,\n",
       " 2926: 408,\n",
       " 2973: 409,\n",
       " 3044: 410,\n",
       " 3060: 411,\n",
       " 3079: 412,\n",
       " 3083: 413,\n",
       " 3160: 414,\n",
       " 3175: 415,\n",
       " 3204: 416,\n",
       " 3255: 417,\n",
       " 3317: 418,\n",
       " 3358: 419,\n",
       " 3365: 420,\n",
       " 3408: 421,\n",
       " 3481: 422,\n",
       " 3508: 423,\n",
       " 3538: 424,\n",
       " 3591: 425,\n",
       " 3788: 426,\n",
       " 3851: 427,\n",
       " 3897: 428,\n",
       " 3911: 429,\n",
       " 3967: 430,\n",
       " 3996: 431,\n",
       " 4002: 432,\n",
       " 4014: 433,\n",
       " 4020: 434,\n",
       " 4021: 435,\n",
       " 4027: 436,\n",
       " 4029: 437,\n",
       " 4033: 438,\n",
       " 4034: 439,\n",
       " 4074: 440,\n",
       " 4121: 441,\n",
       " 4144: 442,\n",
       " 4166: 443,\n",
       " 4226: 444,\n",
       " 4239: 445,\n",
       " 4246: 446,\n",
       " 4252: 447,\n",
       " 4260: 448,\n",
       " 4273: 449,\n",
       " 4308: 450,\n",
       " 4347: 451,\n",
       " 4381: 452,\n",
       " 4641: 453,\n",
       " 4741: 454,\n",
       " 4765: 455,\n",
       " 4881: 456,\n",
       " 4896: 457,\n",
       " 4902: 458,\n",
       " 4967: 459,\n",
       " 34: 460,\n",
       " 36: 461,\n",
       " 39: 462,\n",
       " 150: 463,\n",
       " 153: 464,\n",
       " 253: 465,\n",
       " 261: 466,\n",
       " 266: 467,\n",
       " 290: 468,\n",
       " 300: 469,\n",
       " 344: 470,\n",
       " 364: 471,\n",
       " 380: 472,\n",
       " 410: 473,\n",
       " 474: 474,\n",
       " 515: 475,\n",
       " 531: 476,\n",
       " 534: 477,\n",
       " 589: 478,\n",
       " 594: 479,\n",
       " 597: 480,\n",
       " 2: 481,\n",
       " 4: 482,\n",
       " 5: 483,\n",
       " 7: 484,\n",
       " 8: 485,\n",
       " 10: 486,\n",
       " 11: 487,\n",
       " 13: 488,\n",
       " 15: 489,\n",
       " 16: 490,\n",
       " 17: 491,\n",
       " 19: 492,\n",
       " 22: 493,\n",
       " 24: 494,\n",
       " 25: 495,\n",
       " 26: 496,\n",
       " 27: 497,\n",
       " 41: 498,\n",
       " 43: 499,\n",
       " 46: 500,\n",
       " 54: 501,\n",
       " 60: 502,\n",
       " 61: 503,\n",
       " 62: 504,\n",
       " 65: 505,\n",
       " 66: 506,\n",
       " 76: 507,\n",
       " 79: 508,\n",
       " 86: 509,\n",
       " 87: 510,\n",
       " 88: 511,\n",
       " 89: 512,\n",
       " 92: 513,\n",
       " 93: 514,\n",
       " 95: 515,\n",
       " 100: 516,\n",
       " 102: 517,\n",
       " 104: 518,\n",
       " 105: 519,\n",
       " 112: 520,\n",
       " 113: 521,\n",
       " 135: 522,\n",
       " 140: 523,\n",
       " 141: 524,\n",
       " 145: 525,\n",
       " 146: 526,\n",
       " 158: 527,\n",
       " 159: 528,\n",
       " 160: 529,\n",
       " 161: 530,\n",
       " 165: 531,\n",
       " 168: 532,\n",
       " 170: 533,\n",
       " 174: 534,\n",
       " 177: 535,\n",
       " 179: 536,\n",
       " 180: 537,\n",
       " 181: 538,\n",
       " 185: 539,\n",
       " 186: 540,\n",
       " 189: 541,\n",
       " 191: 542,\n",
       " 195: 543,\n",
       " 196: 544,\n",
       " 201: 545,\n",
       " 204: 546,\n",
       " 205: 547,\n",
       " 207: 548,\n",
       " 208: 549,\n",
       " 209: 550,\n",
       " 210: 551,\n",
       " 212: 552,\n",
       " 217: 553,\n",
       " 218: 554,\n",
       " 219: 555,\n",
       " 224: 556,\n",
       " 225: 557,\n",
       " 230: 558,\n",
       " 234: 559,\n",
       " 236: 560,\n",
       " 237: 561,\n",
       " 239: 562,\n",
       " 240: 563,\n",
       " 243: 564,\n",
       " 248: 565,\n",
       " 250: 566,\n",
       " 251: 567,\n",
       " 252: 568,\n",
       " 254: 569,\n",
       " 256: 570,\n",
       " 257: 571,\n",
       " 258: 572,\n",
       " 262: 573,\n",
       " 267: 574,\n",
       " 270: 575,\n",
       " 271: 576,\n",
       " 273: 577,\n",
       " 274: 578,\n",
       " 276: 579,\n",
       " 277: 580,\n",
       " 279: 581,\n",
       " 281: 582,\n",
       " 282: 583,\n",
       " 288: 584,\n",
       " 289: 585,\n",
       " 291: 586,\n",
       " 292: 587,\n",
       " 293: 588,\n",
       " 302: 589,\n",
       " 303: 590,\n",
       " 304: 591,\n",
       " 310: 592,\n",
       " 312: 593,\n",
       " 313: 594,\n",
       " 314: 595,\n",
       " 315: 596,\n",
       " 317: 597,\n",
       " 327: 598,\n",
       " 329: 599,\n",
       " 330: 600,\n",
       " 332: 601,\n",
       " 336: 602,\n",
       " 337: 603,\n",
       " 339: 604,\n",
       " 340: 605,\n",
       " 343: 606,\n",
       " 347: 607,\n",
       " 350: 608,\n",
       " 352: 609,\n",
       " 353: 610,\n",
       " 354: 611,\n",
       " 355: 612,\n",
       " 358: 613,\n",
       " 359: 614,\n",
       " 360: 615,\n",
       " 361: 616,\n",
       " 366: 617,\n",
       " 370: 618,\n",
       " 371: 619,\n",
       " 374: 620,\n",
       " 377: 621,\n",
       " 378: 622,\n",
       " 381: 623,\n",
       " 382: 624,\n",
       " 383: 625,\n",
       " 405: 626,\n",
       " 412: 627,\n",
       " 415: 628,\n",
       " 416: 629,\n",
       " 419: 630,\n",
       " 426: 631,\n",
       " 432: 632,\n",
       " 434: 633,\n",
       " 435: 634,\n",
       " 437: 635,\n",
       " 440: 636,\n",
       " 445: 637,\n",
       " 454: 638,\n",
       " 455: 639,\n",
       " 458: 640,\n",
       " 460: 641,\n",
       " 466: 642,\n",
       " 468: 643,\n",
       " 469: 644,\n",
       " 472: 645,\n",
       " 477: 646,\n",
       " 485: 647,\n",
       " 489: 648,\n",
       " 490: 649,\n",
       " 491: 650,\n",
       " 493: 651,\n",
       " 494: 652,\n",
       " 497: 653,\n",
       " 502: 654,\n",
       " 505: 655,\n",
       " 508: 656,\n",
       " 510: 657,\n",
       " 516: 658,\n",
       " 520: 659,\n",
       " 524: 660,\n",
       " 532: 661,\n",
       " 536: 662,\n",
       " 537: 663,\n",
       " 540: 664,\n",
       " 542: 665,\n",
       " 546: 666,\n",
       " 548: 667,\n",
       " 569: 668,\n",
       " 575: 669,\n",
       " 587: 670,\n",
       " 606: 671,\n",
       " 609: 672,\n",
       " 616: 673,\n",
       " 628: 674,\n",
       " 631: 675,\n",
       " 637: 676,\n",
       " 640: 677,\n",
       " 662: 678,\n",
       " 667: 679,\n",
       " 694: 680,\n",
       " 697: 681,\n",
       " 700: 682,\n",
       " 704: 683,\n",
       " 709: 684,\n",
       " 710: 685,\n",
       " 711: 686,\n",
       " 719: 687,\n",
       " 747: 688,\n",
       " 762: 689,\n",
       " 765: 690,\n",
       " 775: 691,\n",
       " 783: 692,\n",
       " 795: 693,\n",
       " 799: 694,\n",
       " 801: 695,\n",
       " 802: 696,\n",
       " 818: 697,\n",
       " 830: 698,\n",
       " 835: 699,\n",
       " 837: 700,\n",
       " 838: 701,\n",
       " 839: 702,\n",
       " 842: 703,\n",
       " 848: 704,\n",
       " 852: 705,\n",
       " 867: 706,\n",
       " 880: 707,\n",
       " 881: 708,\n",
       " 888: 709,\n",
       " 891: 710,\n",
       " 979: 711,\n",
       " 981: 712,\n",
       " 986: 713,\n",
       " 991: 714,\n",
       " 996: 715,\n",
       " 999: 716,\n",
       " 1004: 717,\n",
       " 1006: 718,\n",
       " 1061: 719,\n",
       " 1064: 720,\n",
       " 1082: 721,\n",
       " 750: 722,\n",
       " 924: 723,\n",
       " 1101: 724,\n",
       " 1246: 725,\n",
       " 1584: 726,\n",
       " 1610: 727,\n",
       " 1682: 728,\n",
       " 1784: 729,\n",
       " 1917: 730,\n",
       " 2671: 731,\n",
       " 2688: 732,\n",
       " 2701: 733,\n",
       " 2717: 734,\n",
       " 3114: 735,\n",
       " 3354: 736,\n",
       " 3623: 737,\n",
       " 3869: 738,\n",
       " 3916: 739,\n",
       " 3977: 740,\n",
       " 3994: 741,\n",
       " 4018: 742,\n",
       " 4223: 743,\n",
       " 4306: 744,\n",
       " 4310: 745,\n",
       " 4370: 746,\n",
       " 4643: 747,\n",
       " 4700: 748,\n",
       " 4844: 749,\n",
       " 4874: 750,\n",
       " 4886: 751,\n",
       " 4963: 752,\n",
       " 4993: 753,\n",
       " 4995: 754,\n",
       " 5218: 755,\n",
       " 5349: 756,\n",
       " 5378: 757,\n",
       " 5445: 758,\n",
       " 5459: 759,\n",
       " 5464: 760,\n",
       " 5502: 761,\n",
       " 5618: 762,\n",
       " 5816: 763,\n",
       " 5952: 764,\n",
       " 5989: 765,\n",
       " 5991: 766,\n",
       " 6333: 767,\n",
       " 6365: 768,\n",
       " 6534: 769,\n",
       " 6539: 770,\n",
       " 6863: 771,\n",
       " 6934: 772,\n",
       " 7143: 773,\n",
       " 7153: 774,\n",
       " 7155: 775,\n",
       " 7445: 776,\n",
       " 8207: 777,\n",
       " 8360: 778,\n",
       " 8368: 779,\n",
       " 8373: 780,\n",
       " 8528: 781,\n",
       " 8636: 782,\n",
       " 8665: 783,\n",
       " 8666: 784,\n",
       " 8783: 785,\n",
       " 8808: 786,\n",
       " 8865: 787,\n",
       " 8870: 788,\n",
       " 8907: 789,\n",
       " 8908: 790,\n",
       " 8949: 791,\n",
       " 8957: 792,\n",
       " 8958: 793,\n",
       " 8961: 794,\n",
       " 8965: 795,\n",
       " 8970: 796,\n",
       " 8972: 797,\n",
       " 8984: 798,\n",
       " 27741: 799,\n",
       " 30812: 800,\n",
       " 30816: 801,\n",
       " 31878: 802,\n",
       " 32029: 803,\n",
       " 32031: 804,\n",
       " 32296: 805,\n",
       " 32587: 806,\n",
       " 33162: 807,\n",
       " 33493: 808,\n",
       " 33794: 809,\n",
       " 33836: 810,\n",
       " 34048: 811,\n",
       " 34319: 812,\n",
       " 37741: 813,\n",
       " 38388: 814,\n",
       " 42002: 815,\n",
       " 45499: 816,\n",
       " 45517: 817,\n",
       " 45668: 818,\n",
       " 45730: 819,\n",
       " 46530: 820,\n",
       " 48783: 821,\n",
       " 48997: 822,\n",
       " 49272: 823,\n",
       " 49278: 824,\n",
       " 49286: 825,\n",
       " 49824: 826,\n",
       " 586: 827,\n",
       " 187: 828,\n",
       " 627: 829,\n",
       " 922: 830,\n",
       " 1037: 831,\n",
       " 1095: 832,\n",
       " 1674: 833,\n",
       " 1987: 834,\n",
       " 2011: 835,\n",
       " 2023: 836,\n",
       " 2300: 837,\n",
       " 2877: 838,\n",
       " 2901: 839,\n",
       " 3173: 840,\n",
       " 3328: 841,\n",
       " 3735: 842,\n",
       " 4131: 843,\n",
       " 4558: 844,\n",
       " 5447: 845,\n",
       " 5451: 846,\n",
       " 5481: 847,\n",
       " 5507: 848,\n",
       " 5841: 849,\n",
       " 5843: 850,\n",
       " 5872: 851,\n",
       " 5890: 852,\n",
       " 5891: 853,\n",
       " 5893: 854,\n",
       " 5902: 855,\n",
       " 5956: 856,\n",
       " 5962: 857,\n",
       " 5965: 858,\n",
       " 5988: 859,\n",
       " 6001: 860,\n",
       " 6044: 861,\n",
       " 1028: 862,\n",
       " 1088: 863,\n",
       " 1247: 864,\n",
       " 1307: 865,\n",
       " 3882: 866,\n",
       " 4447: 867,\n",
       " 5066: 868,\n",
       " 5377: 869,\n",
       " 5620: 870,\n",
       " 5943: 871,\n",
       " 5957: 872,\n",
       " 6155: 873,\n",
       " 6266: 874,\n",
       " 6377: 875,\n",
       " 6535: 876,\n",
       " 6942: 877,\n",
       " 7149: 878,\n",
       " 7151: 879,\n",
       " 7154: 880,\n",
       " 7169: 881,\n",
       " 7293: 882,\n",
       " 7375: 883,\n",
       " 7451: 884,\n",
       " 7458: 885,\n",
       " 8529: 886,\n",
       " 8533: 887,\n",
       " 8869: 888,\n",
       " 8969: 889,\n",
       " 30749: 890,\n",
       " 31433: 891,\n",
       " 31685: 892,\n",
       " 33145: 893,\n",
       " 33679: 894,\n",
       " 40629: 895,\n",
       " 40819: 896,\n",
       " 41285: 897,\n",
       " 47099: 898,\n",
       " 51662: 899,\n",
       " 51705: 900,\n",
       " 51834: 901,\n",
       " 54286: 902,\n",
       " 56367: 903,\n",
       " 56949: 904,\n",
       " 58047: 905,\n",
       " 59333: 906,\n",
       " 59421: 907,\n",
       " 60397: 908,\n",
       " 60950: 909,\n",
       " 61250: 910,\n",
       " 63113: 911,\n",
       " 63992: 912,\n",
       " 64969: 913,\n",
       " 66203: 914,\n",
       " 68954: 915,\n",
       " 69406: 916,\n",
       " 69844: 917,\n",
       " 70183: 918,\n",
       " 70293: 919,\n",
       " 71579: 920,\n",
       " 72011: 921,\n",
       " 72330: 922,\n",
       " 72407: 923,\n",
       " 72720: 924,\n",
       " 72737: 925,\n",
       " 72998: 926,\n",
       " 73017: 927,\n",
       " 74450: 928,\n",
       " 77841: 929,\n",
       " 78772: 930,\n",
       " 79091: 931,\n",
       " 80549: 932,\n",
       " 81784: 933,\n",
       " 81845: 934,\n",
       " 81847: 935,\n",
       " 82167: 936,\n",
       " 82499: 937,\n",
       " 84374: 938,\n",
       " 86548: 939,\n",
       " 87222: 940,\n",
       " 88163: 941,\n",
       " 88810: 942,\n",
       " 91104: 943,\n",
       " 92259: 944,\n",
       " 94070: 945,\n",
       " 95167: 946,\n",
       " 95449: 947,\n",
       " 95510: 948,\n",
       " 95543: 949,\n",
       " 96079: 950,\n",
       " 97024: 951,\n",
       " 97938: 952,\n",
       " 98203: 953,\n",
       " 103335: 954,\n",
       " 103339: 955,\n",
       " 104374: 956,\n",
       " 105211: 957,\n",
       " 106489: 958,\n",
       " 106696: 959,\n",
       " 107141: 960,\n",
       " 109374: 961,\n",
       " 109853: 962,\n",
       " 112006: 963,\n",
       " 113275: 964,\n",
       " 113394: 965,\n",
       " 119145: 966,\n",
       " 129428: 967,\n",
       " 136020: 968,\n",
       " 137595: 969,\n",
       " 140110: 970,\n",
       " 44: 971,\n",
       " 376: 972,\n",
       " 511: 973,\n",
       " 529: 974,\n",
       " 1100: 975,\n",
       " 1358: 976,\n",
       " 1370: 977,\n",
       " 1385: 978,\n",
       " 1438: 979,\n",
       " 1518: 980,\n",
       " 1586: 981,\n",
       " 1604: 982,\n",
       " 1608: 983,\n",
       " 1616: 984,\n",
       " 1687: 985,\n",
       " 1693: 986,\n",
       " 1721: 987,\n",
       " 1840: 988,\n",
       " 1882: 989,\n",
       " 1918: 990,\n",
       " 2002: 991,\n",
       " 2027: 992,\n",
       " 1357: 993,\n",
       " 1405: 994,\n",
       " 1876: 995,\n",
       " 2072: 996,\n",
       " 2100: 997,\n",
       " 2421: 998,\n",
       " 2485: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                      title   \n",
       "0           1                           Toy Story (1995)  \\\n",
       "1           2                             Jumanji (1995)   \n",
       "2           3                    Grumpier Old Men (1995)   \n",
       "3           4                   Waiting to Exhale (1995)   \n",
       "4           5         Father of the Bride Part II (1995)   \n",
       "...       ...                                        ...   \n",
       "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "9738   193583               No Game No Life: Zero (2017)   \n",
       "9739   193585                               Flint (2017)   \n",
       "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                           genres  \n",
       "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                      Adventure|Children|Fantasy  \n",
       "2                                  Comedy|Romance  \n",
       "3                            Comedy|Drama|Romance  \n",
       "4                                          Comedy  \n",
       "...                                           ...  \n",
       "9737              Action|Animation|Comedy|Fantasy  \n",
       "9738                     Animation|Comedy|Fantasy  \n",
       "9739                                        Drama  \n",
       "9740                             Action|Animation  \n",
       "9741                                       Comedy  \n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId_x</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>new_movieId</th>\n",
       "      <th>movieId_y</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>2117</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1180302224</td>\n",
       "      <td>4063.0</td>\n",
       "      <td>4063</td>\n",
       "      <td>Prelude to a Kiss (1992)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>2117</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1107164430</td>\n",
       "      <td>4063.0</td>\n",
       "      <td>4063</td>\n",
       "      <td>Prelude to a Kiss (1992)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>598</td>\n",
       "      <td>2117</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1519116110</td>\n",
       "      <td>4063.0</td>\n",
       "      <td>4063</td>\n",
       "      <td>Prelude to a Kiss (1992)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>764</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1158982929</td>\n",
       "      <td>5048.0</td>\n",
       "      <td>5048</td>\n",
       "      <td>Snow Dogs (2002)</td>\n",
       "      <td>Adventure|Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>214</td>\n",
       "      <td>764</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1260908646</td>\n",
       "      <td>5048.0</td>\n",
       "      <td>5048</td>\n",
       "      <td>Snow Dogs (2002)</td>\n",
       "      <td>Adventure|Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47160</th>\n",
       "      <td>17</td>\n",
       "      <td>1354</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1478631983</td>\n",
       "      <td>4262.0</td>\n",
       "      <td>4262</td>\n",
       "      <td>Scarface (1983)</td>\n",
       "      <td>Action|Crime|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47161</th>\n",
       "      <td>88</td>\n",
       "      <td>3802</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1520409384</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>1762</td>\n",
       "      <td>Deep Rising (1998)</td>\n",
       "      <td>Action|Horror|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47162</th>\n",
       "      <td>88</td>\n",
       "      <td>3906</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1520409021</td>\n",
       "      <td>4921.0</td>\n",
       "      <td>4921</td>\n",
       "      <td>Little Women (1933)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47163</th>\n",
       "      <td>49</td>\n",
       "      <td>2858</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1514240284</td>\n",
       "      <td>184.0</td>\n",
       "      <td>184</td>\n",
       "      <td>Nadja (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47164</th>\n",
       "      <td>88</td>\n",
       "      <td>3822</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1520409653</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100</td>\n",
       "      <td>Splash (1984)</td>\n",
       "      <td>Comedy|Fantasy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47165 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId_x  rating   timestamp  new_movieId  movieId_y   \n",
       "0         186       2117     4.0  1180302224       4063.0       4063  \\\n",
       "1          22       2117     4.0  1107164430       4063.0       4063   \n",
       "2         598       2117     3.0  1519116110       4063.0       4063   \n",
       "3          74        764     4.0  1158982929       5048.0       5048   \n",
       "4         214        764     4.0  1260908646       5048.0       5048   \n",
       "...       ...        ...     ...         ...          ...        ...   \n",
       "47160      17       1354     4.0  1478631983       4262.0       4262   \n",
       "47161      88       3802     5.0  1520409384       1762.0       1762   \n",
       "47162      88       3906     4.0  1520409021       4921.0       4921   \n",
       "47163      49       2858     1.0  1514240284        184.0        184   \n",
       "47164      88       3822     5.0  1520409653       2100.0       2100   \n",
       "\n",
       "                          title                     genres  \n",
       "0      Prelude to a Kiss (1992)       Comedy|Drama|Romance  \n",
       "1      Prelude to a Kiss (1992)       Comedy|Drama|Romance  \n",
       "2      Prelude to a Kiss (1992)       Comedy|Drama|Romance  \n",
       "3              Snow Dogs (2002)  Adventure|Children|Comedy  \n",
       "4              Snow Dogs (2002)  Adventure|Children|Comedy  \n",
       "...                         ...                        ...  \n",
       "47160           Scarface (1983)         Action|Crime|Drama  \n",
       "47161        Deep Rising (1998)       Action|Horror|Sci-Fi  \n",
       "47162       Little Women (1933)              Drama|Romance  \n",
       "47163              Nadja (1994)                      Drama  \n",
       "47164             Splash (1984)     Comedy|Fantasy|Romance  \n",
       "\n",
       "[47165 rows x 8 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = dataset.copy()\n",
    "df_test['new_movieId'] = df_test['movieId'].map(movie_ids_map)\n",
    "merged_df = pd.merge(df_test, movie_title, left_on='new_movieId', right_on='movieId')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1923076923076925"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df[\"userId\"] == 2][\"rating\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_for_user(model, user_id, dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a list of the 5 movies that have the highest ratings among the unrated movies\n",
    "    of user `user_id`, along with a list of their predicted ratings.\n",
    "    \n",
    "    Input :\n",
    "        model : keras.models.Model : A trained matrix factorization model\n",
    "        user_id : int : The user id to use\n",
    "        dataset : DataFrame : The whole dataset, useful to find the movies \n",
    "            the user `user_id` has already rated\n",
    "    \n",
    "    Output :\n",
    "        five_best_movie_ids : list : The five movie ids among unrated movies by user `user_id` \n",
    "            that have the highest predicted ratings, in order\n",
    "        five_best_ratings : list : The corresponding five ratings\n",
    "    \"\"\"\n",
    "    \n",
    "    pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "    \n",
    "    df = pivoted_dataset[(pivoted_dataset.index >= 0) & (pivoted_dataset.index <= 9)]\n",
    "    df = df.stack(dropna=False).reset_index()\n",
    "    df = df.rename(columns={0:'rating'})\n",
    "    df['new_movieId'] = df['movieId'].map(movie_ids_map)\n",
    "    df = df[df['rating'].isna()]\n",
    "    \n",
    "    \n",
    "    df = df[df[\"userId\"] == user_id]\n",
    "    \n",
    "    X = [df[\"userId\"].to_numpy(), df[\"movieId\"].to_numpy()]\n",
    "    \n",
    "    \n",
    "    y_pred = best_model.predict(X)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "    \n",
    "    # Get the indices of the 5 maximum values in the array\n",
    "    max_indices = np.argsort(y_pred)[-5:][::-1]\n",
    "    \n",
    "    max_rating = y_pred[max_indices]\n",
    "    corresponding_movies = X[1][max_indices]\n",
    "    \n",
    "    movies = movie_title[movie_title[\"movieId\"].isin(corresponding_movies)][\"title\"]\n",
    "   \n",
    "    return max_rating, movies\n",
    "    # return five_best_movie_ids, five_best_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 0\n",
      "Max ratings :\n",
      "[5.1995797 4.9747977 4.972244  4.9603553 4.953046 ]\n",
      "Movies : \n",
      "198          Eat Drink Man Woman (Yin shi nan nu) (1994)\n",
      "204    Far From Home: The Adventures of Yellow Dog (1...\n",
      "607                                    Striptease (1996)\n",
      "784                 Robin Hood: Prince of Thieves (1991)\n",
      "854               Return of the Pink Panther, The (1975)\n",
      "Name: title, dtype: object\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 1\n",
      "Max ratings :\n",
      "[3.7197561 3.6988497 3.692965  3.6911883 3.6677058]\n",
      "Movies : \n",
      "3         Waiting to Exhale (1995)\n",
      "15                   Casino (1995)\n",
      "19              Money Train (1995)\n",
      "139    Doom Generation, The (1995)\n",
      "Name: title, dtype: object\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 2\n",
      "Max ratings :\n",
      "[2.229318  2.199158  2.1764073 2.1670256 2.1582003]\n",
      "Movies : \n",
      "2         Grumpier Old Men (1995)\n",
      "38         Dead Presidents (1995)\n",
      "62     From Dusk Till Dawn (1996)\n",
      "253               Outbreak (1995)\n",
      "416        Jimmy Hollywood (1994)\n",
      "Name: title, dtype: object\n",
      "298/298 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 3\n",
      "Max ratings :\n",
      "[4.2685037 4.1520867 4.1235924 4.0619917 4.0554605]\n",
      "Movies : \n",
      "41                To Die For (1995)\n",
      "607               Striptease (1996)\n",
      "1602         Sixteen Candles (1984)\n",
      "1795    Prince of Egypt, The (1998)\n",
      "Name: title, dtype: object\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 4\n",
      "Max ratings :\n",
      "[3.8598511 3.8476942 3.74992   3.6003506 3.5741777]\n",
      "Movies : \n",
      "14         Cutthroat Island (1995)\n",
      "19              Money Train (1995)\n",
      "32                     Babe (1995)\n",
      "62      From Dusk Till Dawn (1996)\n",
      "139    Doom Generation, The (1995)\n",
      "Name: title, dtype: object\n",
      "295/295 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 5\n",
      "Max ratings :\n",
      "[4.329933 4.177293 4.08513  4.083076 4.053998]\n",
      "Movies : \n",
      "14       Cutthroat Island (1995)\n",
      "62    From Dusk Till Dawn (1996)\n",
      "Name: title, dtype: object\n",
      "300/300 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 6\n",
      "Max ratings :\n",
      "[4.6452827 4.371536  4.329762  4.325963  4.2986517]\n",
      "Movies : \n",
      "15                                   Casino (1995)\n",
      "27                               Persuasion (1995)\n",
      "62                      From Dusk Till Dawn (1996)\n",
      "139                    Doom Generation, The (1995)\n",
      "198    Eat Drink Man Woman (Yin shi nan nu) (1994)\n",
      "Name: title, dtype: object\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 7\n",
      "Max ratings :\n",
      "[3.6901348 3.5544815 3.5493867 3.5058491 3.4598632]\n",
      "Movies : \n",
      "14         Cutthroat Island (1995)\n",
      "62      From Dusk Till Dawn (1996)\n",
      "139    Doom Generation, The (1995)\n",
      "416         Jimmy Hollywood (1994)\n",
      "Name: title, dtype: object\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 8\n",
      "Max ratings :\n",
      "[4.2394114 4.009088  3.9837146 3.9718528 3.9458447]\n",
      "Movies : \n",
      "3                         Waiting to Exhale (1995)\n",
      "14                         Cutthroat Island (1995)\n",
      "15                                   Casino (1995)\n",
      "32                                     Babe (1995)\n",
      "198    Eat Drink Man Woman (Yin shi nan nu) (1994)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for user in range(9):\n",
    "    max_rating, movies = get_top5_for_user(best_model, user, dataset)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(f\"User id --> {user}\")\n",
    "    print(f\"Max ratings :\")\n",
    "    print(max_rating)\n",
    "    print(f\"Movies : \")\n",
    "    print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at what is going on in the embedding space of the movies that we learnt. Our brain cannot picture anything beyond 3 dimensions, and we learnt high dimensional embeddings (k=15 or 30), so we are going to project the movies embeddings on a 2D plane, first with PCA, and then with another algorithm made for visualizing high dimensional spaces called t-sne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already studied PCA, it is a useful technique for dimensionality reduction, but also simply for visualization. Don't forget to scale your embeddings first. To access the embeddings values of your keras model, have a look at the *get_weights()* function.\n",
    "\n",
    "Compute a PCA on all your movies embeddings, get the 2 first principal components, and do a scatter plot of all the movies on a 2D plane, where each movie is a point defined by the two values of the two first principal components of the PCA from its embedding. Add the titles of the movies to each point of the plot (use plotly to do so it will be clearer), and try to see if you can interpret the axes of the PCA through to different movie genres, like in Figure 3 from the article *Matrix Factorization Techniques for Recommender Systems*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with t-sne, an algorithm specialized for visualizing high dimensional spaces, you can read more about it there : https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TOFILL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-sne in general tends to preserve local similarities better than PCA. In any case, it's always interesting to try both for visualizing high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can export your embedding and upload them on https://projector.tensorflow.org/ to visualize the embeddings in 3D. You can also use the movies genres from the *movies.csv* file to make one plot for each movie genre and try to see if some parts of the embedding space are representative of a movie genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL PARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies to yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that ask you to rate 20 movies, then add your own ratings to the dataset, retrain the model, and compute your own top-5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rate_my_movies(my_user_id, dataset, nb_movies, nb_to_rate, movie_ids_map):\n",
    "    \"\"\"\n",
    "    Returns a dataframe in the same format as the dataset dataframe, with\n",
    "    ratings entered by the user for `nb_to_rate` random movies\n",
    "    \n",
    "    Input :\n",
    "        my_user_id : int : The user_id of the new ratings\n",
    "        dataset : DataFrame : The whole dataset \n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        nb_to_rate : int : Number of movies to rate\n",
    "        movie_ids_map : dict : The mapping of original file userId to a new index starting at 0.\n",
    "    \n",
    "    Output : \n",
    "        my_ratings : DataFrame : A dataframe with the same column as `dataset` containing\n",
    "            the new ratings entered by the user\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    return my_ratings\n",
    "\n",
    "\n",
    "my_user_id = len(user_ids_map)\n",
    "\n",
    "my_ratings = rate_my_movies(my_user_id, dataset, nb_movies, 20, movie_ids_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_me = pd.concat([dataset, my_ratings], axis = 0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_with_me = [dataset_with_me[\"userId\"].to_numpy(), dataset_with_me[\"movieId\"].to_numpy()]\n",
    "y_with_me = dataset_with_me[\"rating\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model=get_mf_bias_l2_reg_model(nb_users + 1, nb_movies, k = best_params['k'], lambda_ = best_params['lambda_'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "best_model.fit(X_with_me, y_with_me, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_best_movie_ids, five_best_ratings =  get_top5_for_user(best_model, my_user_id, dataset)\n",
    "for i in range(5):\n",
    "    print('\\t' + ml_movie_id_to_title[ inverse_movie_ids_map[ five_best_movie_ids[i] ] ] + \n",
    "          '; predicted rating : ' + str(five_best_ratings[i]) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the movie embeddings to predict the movies genre with multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the goal of predicting missing rating, the matrix factorization techniques also produces vectorial representation of movies and users: their embeddings, what we just visualized for the movies. With a big enough dataset, these embeddings actually are good abstract representations of the movies and of the users, and can be reused as features for other tasks, such as classification.\n",
    "\n",
    "In the *movies.csv*, there is a column that gives the genres of each movie. Let's try to predict the genres of the movies from the embeddings we learnt. As you can see, each movie can have more than one genre, so in classification terms, more than one class. We can achieve that with *multilabel classification*. You can read more about it there: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "Load the movies genre, encode them as binary classes and use the classes imported below to train a multilabel classifier that uses the movie embeddings as features, and the movie genres as classes. Use the *OneVsRestClassifier* with a simple *LinearSVC* without any hyper-parameter tuning. Finally print the test accuracy, F1, precision and recall for each class, as well as the number of time each class appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rare classes, you should get a very high accuracy, with a very low F1. Indeed these classes are really imbalanced : there are a few positives, hence the classifier is largely biased toward the negatives, and rarely predict a positive for these classes. This is why accuracy is generally a bad measure with imbalanced dataset : the high number of true negatives makes the accuracy number high, while our model is actually barely capable of predicting true positives.\n",
    "\n",
    "Let's compare our classifier performance with a *DummyClassifier*, the dummy classifier takes the ratio $r = \\frac{nb\\_positives}{nb\\_positives + nb\\_negatives}$ as the probability to predict a positive, and then do it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply respecting the class balance, even at random, produces better F1 on most classes. One way to compensate for class imbalance is to tell the classifier to weight more the true samples at training time, accordingly with the ratio $r$ between true and false samples. With scikit-learn SVM implementation, you can use the argument *class_weight* for setting the weight of the positive and negative samples at training time. See : https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "But if you just want to set the class weights accordingly with the ratio between positives and negatives, you can just set *class_weight = â€˜balancedâ€™*. Test it with the LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is now much better than with the dummy classifier, however is is still not very convincing. This is quite normal given the size of the dataset we are using, which is pretty small to get really meaningful embeddings. But with bigger datasets, reusing embeddings as features for auxiliary tasks such as classification is actually a very effective way of doing so when there is no other informations about the items we try to classify. Here the items are the movies, the dataset doesn't provide more information about them, but one could imagine fetching from internet textual descriptions of the movies and use them as features alongside the embeddings to improve the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the different SGD algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the notebook we used the 'adam' `optimizer` to train our model, which is a variation of SGD. Keras proposes different variations of SGD: https://keras.io/optimizers/ . This article gif images gives an intuitive view of their different behavior : https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39\n",
    "\n",
    "Try a few ones with our model and see how the training and testing loss evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the global bias $\\mu$  parameter to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we didn't added the global bias $\\mu$ to our model yet (Equations (4-5) from Koren's paper). Use your best google skills to find a way to add an embedding layer that does that.\n",
    "\n",
    "Hint : Use a constant `Input` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own Stochastic Gradient Descent for Matrix Factorization with numpy instead of Keras (very optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know everything to implement your own matrix factorization SGD model, all with numpy arrays. Start without the biases again, and without mini-batches. The gradient update equations are described in page 4 of Koren's paper. Let's initialize your $p$ and $q$ embeddings with a gaussian sampling. Print the RMSE at the beginning of each epoch, and finally compute the RMSE of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "\n",
    "P = normal(size = (nb_users,k))\n",
    "Q = normal(size = (nb_movies,k))\n",
    "\n",
    "gamma = 0.1\n",
    "lambda_ = 0.00001\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for j in range(train.shape[0]):\n",
    "        u = train['userId'].iloc[j]\n",
    "        i = train['movieId'].iloc[j]\n",
    "        r_ui = train['rating'].iloc[j]\n",
    "        \n",
    "        #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
