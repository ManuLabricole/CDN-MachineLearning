{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:07:44.008543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources** :\n",
    "\n",
    "*Matrix Factorization techniques for Recommender Systems*, Koren (2009)    \n",
    "https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf\n",
    "\n",
    "Hands on Machine Learning with scikit-learn and tensorflow:             \n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "The movieLens dataset:                                                \n",
    "https://grouplens.org/datasets/movielens/ \n",
    "\n",
    "Keras Functional API doc :                                            \n",
    "https://keras.io/guides/functional_api/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems : collaborative filtering via matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you wonder how Netflix is able to recommend you movies despite it doesn't know anything about you but the ratings you gave to the movies you watched ? This is what we are going to explore during this 3 days machine learning module.\n",
    "\n",
    "First off, let's learn about what are recommender system, collaborative filtering and matrix factorization techniques, which are all very well introduced in Koren's 2009 famous article : *Matrix Factorization techniques for Recommender Systems* : https://www.inf.unibz.it/~ricci/ISR/papers/ieeecomputer.pdf . Read the 4 first pages (up to section *adding biases* included). \n",
    "\n",
    "Through this notebook we are going to re-implement the model described in the pages you read, and apply it to a classic movie ratings dataset coming from the website *movieLens*. To do so, we will use a powerful deep learning python library called *Keras*, that makes it easy to train complex models based on linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module, we are going to use the movieLens dataset, that contains data from the movie recommending website movielens. The data is a subset of ratings from 0 to 5 given by some users of the website to a subset of movies. You can read more about it here (we are using the latest small dataset) : https://grouplens.org/datasets/movielens/ , and in the *README* file that is in the *data/ml-latest-small/* folder.\n",
    "\n",
    "Load the ratings data from the `ratings.csv` file into a dataframe. The userId and movieId provided in the file don't start from 0, and are not contiguous (i.e. there are missing indexes).\n",
    "\n",
    "Re-index the user and movie ids to indexes going from 0 to `nb_users` and 0 to `nb_movies` respectively, by building two dictionnaries `user_ids_map` and `movie_ids_map` that maps the file ids to your new ids. \n",
    "And finally, split the rows of this dataframe in a random 90%/10% train/test sets.\n",
    "\n",
    "To do so, fill the `get_train_test_sets` function below, and respect the returned objects structures that are described in the docstring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd :  /Users/manulabricole/Documents/CDN/MachineLearning/ML4/notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "path = \"/Users/manulabricole/Documents/CDN/MachineLearning/ML4/data/ml-latest-small/ratings.csv\"\n",
    "print(\"cwd : \",cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/ml-latest-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              1\n",
       "1              3\n",
       "2              6\n",
       "3             47\n",
       "4             50\n",
       "           ...  \n",
       "100831    166534\n",
       "100832    168248\n",
       "100833    168250\n",
       "100834    168252\n",
       "100835    170875\n",
       "Name: movieId, Length: 100836, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"movieId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 3: 1,\n",
       " 6: 2,\n",
       " 47: 3,\n",
       " 50: 4,\n",
       " 70: 5,\n",
       " 101: 6,\n",
       " 110: 7,\n",
       " 151: 8,\n",
       " 157: 9,\n",
       " 163: 10,\n",
       " 216: 11,\n",
       " 223: 12,\n",
       " 231: 13,\n",
       " 235: 14,\n",
       " 260: 15,\n",
       " 296: 16,\n",
       " 316: 17,\n",
       " 333: 18,\n",
       " 349: 19,\n",
       " 356: 20,\n",
       " 362: 21,\n",
       " 367: 22,\n",
       " 423: 23,\n",
       " 441: 24,\n",
       " 457: 25,\n",
       " 480: 26,\n",
       " 500: 27,\n",
       " 527: 28,\n",
       " 543: 29,\n",
       " 552: 30,\n",
       " 553: 31,\n",
       " 590: 32,\n",
       " 592: 33,\n",
       " 593: 34,\n",
       " 596: 35,\n",
       " 608: 36,\n",
       " 648: 37,\n",
       " 661: 38,\n",
       " 673: 39,\n",
       " 733: 40,\n",
       " 736: 41,\n",
       " 780: 42,\n",
       " 804: 43,\n",
       " 919: 44,\n",
       " 923: 45,\n",
       " 940: 46,\n",
       " 943: 47,\n",
       " 954: 48,\n",
       " 1009: 49,\n",
       " 1023: 50,\n",
       " 1024: 51,\n",
       " 1025: 52,\n",
       " 1029: 53,\n",
       " 1030: 54,\n",
       " 1031: 55,\n",
       " 1032: 56,\n",
       " 1042: 57,\n",
       " 1049: 58,\n",
       " 1060: 59,\n",
       " 1073: 60,\n",
       " 1080: 61,\n",
       " 1089: 62,\n",
       " 1090: 63,\n",
       " 1092: 64,\n",
       " 1097: 65,\n",
       " 1127: 66,\n",
       " 1136: 67,\n",
       " 1196: 68,\n",
       " 1197: 69,\n",
       " 1198: 70,\n",
       " 1206: 71,\n",
       " 1208: 72,\n",
       " 1210: 73,\n",
       " 1213: 74,\n",
       " 1214: 75,\n",
       " 1219: 76,\n",
       " 1220: 77,\n",
       " 1222: 78,\n",
       " 1224: 79,\n",
       " 1226: 80,\n",
       " 1240: 81,\n",
       " 1256: 82,\n",
       " 1258: 83,\n",
       " 1265: 84,\n",
       " 1270: 85,\n",
       " 1275: 86,\n",
       " 1278: 87,\n",
       " 1282: 88,\n",
       " 1291: 89,\n",
       " 1298: 90,\n",
       " 1348: 91,\n",
       " 1377: 92,\n",
       " 1396: 93,\n",
       " 1408: 94,\n",
       " 1445: 95,\n",
       " 1473: 96,\n",
       " 1500: 97,\n",
       " 1517: 98,\n",
       " 1552: 99,\n",
       " 1573: 100,\n",
       " 1580: 101,\n",
       " 1587: 102,\n",
       " 1617: 103,\n",
       " 1620: 104,\n",
       " 1625: 105,\n",
       " 1644: 106,\n",
       " 1676: 107,\n",
       " 1732: 108,\n",
       " 1777: 109,\n",
       " 1793: 110,\n",
       " 1804: 111,\n",
       " 1805: 112,\n",
       " 1920: 113,\n",
       " 1927: 114,\n",
       " 1954: 115,\n",
       " 1967: 116,\n",
       " 2000: 117,\n",
       " 2005: 118,\n",
       " 2012: 119,\n",
       " 2018: 120,\n",
       " 2028: 121,\n",
       " 2033: 122,\n",
       " 2046: 123,\n",
       " 2048: 124,\n",
       " 2054: 125,\n",
       " 2058: 126,\n",
       " 2078: 127,\n",
       " 2090: 128,\n",
       " 2093: 129,\n",
       " 2094: 130,\n",
       " 2096: 131,\n",
       " 2099: 132,\n",
       " 2105: 133,\n",
       " 2115: 134,\n",
       " 2116: 135,\n",
       " 2137: 136,\n",
       " 2139: 137,\n",
       " 2141: 138,\n",
       " 2143: 139,\n",
       " 2161: 140,\n",
       " 2174: 141,\n",
       " 2193: 142,\n",
       " 2253: 143,\n",
       " 2268: 144,\n",
       " 2273: 145,\n",
       " 2291: 146,\n",
       " 2329: 147,\n",
       " 2338: 148,\n",
       " 2353: 149,\n",
       " 2366: 150,\n",
       " 2387: 151,\n",
       " 2389: 152,\n",
       " 2395: 153,\n",
       " 2406: 154,\n",
       " 2414: 155,\n",
       " 2427: 156,\n",
       " 2450: 157,\n",
       " 2459: 158,\n",
       " 2470: 159,\n",
       " 2478: 160,\n",
       " 2492: 161,\n",
       " 2502: 162,\n",
       " 2528: 163,\n",
       " 2529: 164,\n",
       " 2542: 165,\n",
       " 2571: 166,\n",
       " 2580: 167,\n",
       " 2596: 168,\n",
       " 2616: 169,\n",
       " 2617: 170,\n",
       " 2628: 171,\n",
       " 2640: 172,\n",
       " 2641: 173,\n",
       " 2644: 174,\n",
       " 2648: 175,\n",
       " 2654: 176,\n",
       " 2657: 177,\n",
       " 2692: 178,\n",
       " 2700: 179,\n",
       " 2716: 180,\n",
       " 2761: 181,\n",
       " 2797: 182,\n",
       " 2826: 183,\n",
       " 2858: 184,\n",
       " 2872: 185,\n",
       " 2899: 186,\n",
       " 2916: 187,\n",
       " 2944: 188,\n",
       " 2947: 189,\n",
       " 2948: 190,\n",
       " 2949: 191,\n",
       " 2959: 192,\n",
       " 2985: 193,\n",
       " 2987: 194,\n",
       " 2991: 195,\n",
       " 2993: 196,\n",
       " 2997: 197,\n",
       " 3033: 198,\n",
       " 3034: 199,\n",
       " 3052: 200,\n",
       " 3053: 201,\n",
       " 3062: 202,\n",
       " 3147: 203,\n",
       " 3168: 204,\n",
       " 3176: 205,\n",
       " 3243: 206,\n",
       " 3247: 207,\n",
       " 3253: 208,\n",
       " 3273: 209,\n",
       " 3386: 210,\n",
       " 3439: 211,\n",
       " 3440: 212,\n",
       " 3441: 213,\n",
       " 3448: 214,\n",
       " 3450: 215,\n",
       " 3479: 216,\n",
       " 3489: 217,\n",
       " 3527: 218,\n",
       " 3578: 219,\n",
       " 3617: 220,\n",
       " 3639: 221,\n",
       " 3671: 222,\n",
       " 3702: 223,\n",
       " 3703: 224,\n",
       " 3729: 225,\n",
       " 3740: 226,\n",
       " 3744: 227,\n",
       " 3793: 228,\n",
       " 3809: 229,\n",
       " 4006: 230,\n",
       " 5060: 231,\n",
       " 318: 232,\n",
       " 1704: 233,\n",
       " 6874: 234,\n",
       " 8798: 235,\n",
       " 46970: 236,\n",
       " 48516: 237,\n",
       " 58559: 238,\n",
       " 60756: 239,\n",
       " 68157: 240,\n",
       " 71535: 241,\n",
       " 74458: 242,\n",
       " 77455: 243,\n",
       " 79132: 244,\n",
       " 80489: 245,\n",
       " 80906: 246,\n",
       " 86345: 247,\n",
       " 89774: 248,\n",
       " 91529: 249,\n",
       " 91658: 250,\n",
       " 99114: 251,\n",
       " 106782: 252,\n",
       " 109487: 253,\n",
       " 112552: 254,\n",
       " 114060: 255,\n",
       " 115713: 256,\n",
       " 122882: 257,\n",
       " 131724: 258,\n",
       " 31: 259,\n",
       " 647: 260,\n",
       " 688: 261,\n",
       " 720: 262,\n",
       " 849: 263,\n",
       " 914: 264,\n",
       " 1093: 265,\n",
       " 1124: 266,\n",
       " 1263: 267,\n",
       " 1272: 268,\n",
       " 1302: 269,\n",
       " 1371: 270,\n",
       " 2080: 271,\n",
       " 2288: 272,\n",
       " 2424: 273,\n",
       " 2851: 274,\n",
       " 3024: 275,\n",
       " 3210: 276,\n",
       " 3949: 277,\n",
       " 4518: 278,\n",
       " 5048: 279,\n",
       " 5181: 280,\n",
       " 5746: 281,\n",
       " 5764: 282,\n",
       " 5919: 283,\n",
       " 6238: 284,\n",
       " 6835: 285,\n",
       " 7899: 286,\n",
       " 7991: 287,\n",
       " 26409: 288,\n",
       " 70946: 289,\n",
       " 72378: 290,\n",
       " 21: 291,\n",
       " 32: 292,\n",
       " 45: 293,\n",
       " 52: 294,\n",
       " 58: 295,\n",
       " 106: 296,\n",
       " 125: 297,\n",
       " 126: 298,\n",
       " 162: 299,\n",
       " 171: 300,\n",
       " 176: 301,\n",
       " 190: 302,\n",
       " 215: 303,\n",
       " 222: 304,\n",
       " 232: 305,\n",
       " 247: 306,\n",
       " 265: 307,\n",
       " 319: 308,\n",
       " 342: 309,\n",
       " 345: 310,\n",
       " 348: 311,\n",
       " 351: 312,\n",
       " 357: 313,\n",
       " 368: 314,\n",
       " 417: 315,\n",
       " 450: 316,\n",
       " 475: 317,\n",
       " 492: 318,\n",
       " 509: 319,\n",
       " 538: 320,\n",
       " 539: 321,\n",
       " 588: 322,\n",
       " 595: 323,\n",
       " 599: 324,\n",
       " 708: 325,\n",
       " 759: 326,\n",
       " 800: 327,\n",
       " 892: 328,\n",
       " 898: 329,\n",
       " 899: 330,\n",
       " 902: 331,\n",
       " 904: 332,\n",
       " 908: 333,\n",
       " 910: 334,\n",
       " 912: 335,\n",
       " 920: 336,\n",
       " 930: 337,\n",
       " 937: 338,\n",
       " 1046: 339,\n",
       " 1057: 340,\n",
       " 1077: 341,\n",
       " 1079: 342,\n",
       " 1084: 343,\n",
       " 1086: 344,\n",
       " 1094: 345,\n",
       " 1103: 346,\n",
       " 1179: 347,\n",
       " 1183: 348,\n",
       " 1188: 349,\n",
       " 1199: 350,\n",
       " 1203: 351,\n",
       " 1211: 352,\n",
       " 1225: 353,\n",
       " 1250: 354,\n",
       " 1259: 355,\n",
       " 1266: 356,\n",
       " 1279: 357,\n",
       " 1283: 358,\n",
       " 1288: 359,\n",
       " 1304: 360,\n",
       " 1391: 361,\n",
       " 1449: 362,\n",
       " 1466: 363,\n",
       " 1597: 364,\n",
       " 1641: 365,\n",
       " 1719: 366,\n",
       " 1733: 367,\n",
       " 1734: 368,\n",
       " 1834: 369,\n",
       " 1860: 370,\n",
       " 1883: 371,\n",
       " 1885: 372,\n",
       " 1892: 373,\n",
       " 1895: 374,\n",
       " 1907: 375,\n",
       " 1914: 376,\n",
       " 1916: 377,\n",
       " 1923: 378,\n",
       " 1947: 379,\n",
       " 1966: 380,\n",
       " 1968: 381,\n",
       " 2019: 382,\n",
       " 2076: 383,\n",
       " 2109: 384,\n",
       " 2145: 385,\n",
       " 2150: 386,\n",
       " 2186: 387,\n",
       " 2203: 388,\n",
       " 2204: 389,\n",
       " 2282: 390,\n",
       " 2324: 391,\n",
       " 2336: 392,\n",
       " 2351: 393,\n",
       " 2359: 394,\n",
       " 2390: 395,\n",
       " 2467: 396,\n",
       " 2583: 397,\n",
       " 2599: 398,\n",
       " 2683: 399,\n",
       " 2712: 400,\n",
       " 2762: 401,\n",
       " 2763: 402,\n",
       " 2770: 403,\n",
       " 2791: 404,\n",
       " 2843: 405,\n",
       " 2874: 406,\n",
       " 2921: 407,\n",
       " 2926: 408,\n",
       " 2973: 409,\n",
       " 3044: 410,\n",
       " 3060: 411,\n",
       " 3079: 412,\n",
       " 3083: 413,\n",
       " 3160: 414,\n",
       " 3175: 415,\n",
       " 3204: 416,\n",
       " 3255: 417,\n",
       " 3317: 418,\n",
       " 3358: 419,\n",
       " 3365: 420,\n",
       " 3408: 421,\n",
       " 3481: 422,\n",
       " 3508: 423,\n",
       " 3538: 424,\n",
       " 3591: 425,\n",
       " 3788: 426,\n",
       " 3851: 427,\n",
       " 3897: 428,\n",
       " 3911: 429,\n",
       " 3967: 430,\n",
       " 3996: 431,\n",
       " 4002: 432,\n",
       " 4014: 433,\n",
       " 4020: 434,\n",
       " 4021: 435,\n",
       " 4027: 436,\n",
       " 4029: 437,\n",
       " 4033: 438,\n",
       " 4034: 439,\n",
       " 4074: 440,\n",
       " 4121: 441,\n",
       " 4144: 442,\n",
       " 4166: 443,\n",
       " 4226: 444,\n",
       " 4239: 445,\n",
       " 4246: 446,\n",
       " 4252: 447,\n",
       " 4260: 448,\n",
       " 4273: 449,\n",
       " 4308: 450,\n",
       " 4347: 451,\n",
       " 4381: 452,\n",
       " 4641: 453,\n",
       " 4741: 454,\n",
       " 4765: 455,\n",
       " 4881: 456,\n",
       " 4896: 457,\n",
       " 4902: 458,\n",
       " 4967: 459,\n",
       " 34: 460,\n",
       " 36: 461,\n",
       " 39: 462,\n",
       " 150: 463,\n",
       " 153: 464,\n",
       " 253: 465,\n",
       " 261: 466,\n",
       " 266: 467,\n",
       " 290: 468,\n",
       " 300: 469,\n",
       " 344: 470,\n",
       " 364: 471,\n",
       " 380: 472,\n",
       " 410: 473,\n",
       " 474: 474,\n",
       " 515: 475,\n",
       " 531: 476,\n",
       " 534: 477,\n",
       " 589: 478,\n",
       " 594: 479,\n",
       " 597: 480,\n",
       " 2: 481,\n",
       " 4: 482,\n",
       " 5: 483,\n",
       " 7: 484,\n",
       " 8: 485,\n",
       " 10: 486,\n",
       " 11: 487,\n",
       " 13: 488,\n",
       " 15: 489,\n",
       " 16: 490,\n",
       " 17: 491,\n",
       " 19: 492,\n",
       " 22: 493,\n",
       " 24: 494,\n",
       " 25: 495,\n",
       " 26: 496,\n",
       " 27: 497,\n",
       " 41: 498,\n",
       " 43: 499,\n",
       " 46: 500,\n",
       " 54: 501,\n",
       " 60: 502,\n",
       " 61: 503,\n",
       " 62: 504,\n",
       " 65: 505,\n",
       " 66: 506,\n",
       " 76: 507,\n",
       " 79: 508,\n",
       " 86: 509,\n",
       " 87: 510,\n",
       " 88: 511,\n",
       " 89: 512,\n",
       " 92: 513,\n",
       " 93: 514,\n",
       " 95: 515,\n",
       " 100: 516,\n",
       " 102: 517,\n",
       " 104: 518,\n",
       " 105: 519,\n",
       " 112: 520,\n",
       " 113: 521,\n",
       " 135: 522,\n",
       " 140: 523,\n",
       " 141: 524,\n",
       " 145: 525,\n",
       " 146: 526,\n",
       " 158: 527,\n",
       " 159: 528,\n",
       " 160: 529,\n",
       " 161: 530,\n",
       " 165: 531,\n",
       " 168: 532,\n",
       " 170: 533,\n",
       " 174: 534,\n",
       " 177: 535,\n",
       " 179: 536,\n",
       " 180: 537,\n",
       " 181: 538,\n",
       " 185: 539,\n",
       " 186: 540,\n",
       " 189: 541,\n",
       " 191: 542,\n",
       " 195: 543,\n",
       " 196: 544,\n",
       " 201: 545,\n",
       " 204: 546,\n",
       " 205: 547,\n",
       " 207: 548,\n",
       " 208: 549,\n",
       " 209: 550,\n",
       " 210: 551,\n",
       " 212: 552,\n",
       " 217: 553,\n",
       " 218: 554,\n",
       " 219: 555,\n",
       " 224: 556,\n",
       " 225: 557,\n",
       " 230: 558,\n",
       " 234: 559,\n",
       " 236: 560,\n",
       " 237: 561,\n",
       " 239: 562,\n",
       " 240: 563,\n",
       " 243: 564,\n",
       " 248: 565,\n",
       " 250: 566,\n",
       " 251: 567,\n",
       " 252: 568,\n",
       " 254: 569,\n",
       " 256: 570,\n",
       " 257: 571,\n",
       " 258: 572,\n",
       " 262: 573,\n",
       " 267: 574,\n",
       " 270: 575,\n",
       " 271: 576,\n",
       " 273: 577,\n",
       " 274: 578,\n",
       " 276: 579,\n",
       " 277: 580,\n",
       " 279: 581,\n",
       " 281: 582,\n",
       " 282: 583,\n",
       " 288: 584,\n",
       " 289: 585,\n",
       " 291: 586,\n",
       " 292: 587,\n",
       " 293: 588,\n",
       " 302: 589,\n",
       " 303: 590,\n",
       " 304: 591,\n",
       " 310: 592,\n",
       " 312: 593,\n",
       " 313: 594,\n",
       " 314: 595,\n",
       " 315: 596,\n",
       " 317: 597,\n",
       " 327: 598,\n",
       " 329: 599,\n",
       " 330: 600,\n",
       " 332: 601,\n",
       " 336: 602,\n",
       " 337: 603,\n",
       " 339: 604,\n",
       " 340: 605,\n",
       " 343: 606,\n",
       " 347: 607,\n",
       " 350: 608,\n",
       " 352: 609,\n",
       " 353: 610,\n",
       " 354: 611,\n",
       " 355: 612,\n",
       " 358: 613,\n",
       " 359: 614,\n",
       " 360: 615,\n",
       " 361: 616,\n",
       " 366: 617,\n",
       " 370: 618,\n",
       " 371: 619,\n",
       " 374: 620,\n",
       " 377: 621,\n",
       " 378: 622,\n",
       " 381: 623,\n",
       " 382: 624,\n",
       " 383: 625,\n",
       " 405: 626,\n",
       " 412: 627,\n",
       " 415: 628,\n",
       " 416: 629,\n",
       " 419: 630,\n",
       " 426: 631,\n",
       " 432: 632,\n",
       " 434: 633,\n",
       " 435: 634,\n",
       " 437: 635,\n",
       " 440: 636,\n",
       " 445: 637,\n",
       " 454: 638,\n",
       " 455: 639,\n",
       " 458: 640,\n",
       " 460: 641,\n",
       " 466: 642,\n",
       " 468: 643,\n",
       " 469: 644,\n",
       " 472: 645,\n",
       " 477: 646,\n",
       " 485: 647,\n",
       " 489: 648,\n",
       " 490: 649,\n",
       " 491: 650,\n",
       " 493: 651,\n",
       " 494: 652,\n",
       " 497: 653,\n",
       " 502: 654,\n",
       " 505: 655,\n",
       " 508: 656,\n",
       " 510: 657,\n",
       " 516: 658,\n",
       " 520: 659,\n",
       " 524: 660,\n",
       " 532: 661,\n",
       " 536: 662,\n",
       " 537: 663,\n",
       " 540: 664,\n",
       " 542: 665,\n",
       " 546: 666,\n",
       " 548: 667,\n",
       " 569: 668,\n",
       " 575: 669,\n",
       " 587: 670,\n",
       " 606: 671,\n",
       " 609: 672,\n",
       " 616: 673,\n",
       " 628: 674,\n",
       " 631: 675,\n",
       " 637: 676,\n",
       " 640: 677,\n",
       " 662: 678,\n",
       " 667: 679,\n",
       " 694: 680,\n",
       " 697: 681,\n",
       " 700: 682,\n",
       " 704: 683,\n",
       " 709: 684,\n",
       " 710: 685,\n",
       " 711: 686,\n",
       " 719: 687,\n",
       " 747: 688,\n",
       " 762: 689,\n",
       " 765: 690,\n",
       " 775: 691,\n",
       " 783: 692,\n",
       " 795: 693,\n",
       " 799: 694,\n",
       " 801: 695,\n",
       " 802: 696,\n",
       " 818: 697,\n",
       " 830: 698,\n",
       " 835: 699,\n",
       " 837: 700,\n",
       " 838: 701,\n",
       " 839: 702,\n",
       " 842: 703,\n",
       " 848: 704,\n",
       " 852: 705,\n",
       " 867: 706,\n",
       " 880: 707,\n",
       " 881: 708,\n",
       " 888: 709,\n",
       " 891: 710,\n",
       " 979: 711,\n",
       " 981: 712,\n",
       " 986: 713,\n",
       " 991: 714,\n",
       " 996: 715,\n",
       " 999: 716,\n",
       " 1004: 717,\n",
       " 1006: 718,\n",
       " 1061: 719,\n",
       " 1064: 720,\n",
       " 1082: 721,\n",
       " 750: 722,\n",
       " 924: 723,\n",
       " 1101: 724,\n",
       " 1246: 725,\n",
       " 1584: 726,\n",
       " 1610: 727,\n",
       " 1682: 728,\n",
       " 1784: 729,\n",
       " 1917: 730,\n",
       " 2671: 731,\n",
       " 2688: 732,\n",
       " 2701: 733,\n",
       " 2717: 734,\n",
       " 3114: 735,\n",
       " 3354: 736,\n",
       " 3623: 737,\n",
       " 3869: 738,\n",
       " 3916: 739,\n",
       " 3977: 740,\n",
       " 3994: 741,\n",
       " 4018: 742,\n",
       " 4223: 743,\n",
       " 4306: 744,\n",
       " 4310: 745,\n",
       " 4370: 746,\n",
       " 4643: 747,\n",
       " 4700: 748,\n",
       " 4844: 749,\n",
       " 4874: 750,\n",
       " 4886: 751,\n",
       " 4963: 752,\n",
       " 4993: 753,\n",
       " 4995: 754,\n",
       " 5218: 755,\n",
       " 5349: 756,\n",
       " 5378: 757,\n",
       " 5445: 758,\n",
       " 5459: 759,\n",
       " 5464: 760,\n",
       " 5502: 761,\n",
       " 5618: 762,\n",
       " 5816: 763,\n",
       " 5952: 764,\n",
       " 5989: 765,\n",
       " 5991: 766,\n",
       " 6333: 767,\n",
       " 6365: 768,\n",
       " 6534: 769,\n",
       " 6539: 770,\n",
       " 6863: 771,\n",
       " 6934: 772,\n",
       " 7143: 773,\n",
       " 7153: 774,\n",
       " 7155: 775,\n",
       " 7445: 776,\n",
       " 8207: 777,\n",
       " 8360: 778,\n",
       " 8368: 779,\n",
       " 8373: 780,\n",
       " 8528: 781,\n",
       " 8636: 782,\n",
       " 8665: 783,\n",
       " 8666: 784,\n",
       " 8783: 785,\n",
       " 8808: 786,\n",
       " 8865: 787,\n",
       " 8870: 788,\n",
       " 8907: 789,\n",
       " 8908: 790,\n",
       " 8949: 791,\n",
       " 8957: 792,\n",
       " 8958: 793,\n",
       " 8961: 794,\n",
       " 8965: 795,\n",
       " 8970: 796,\n",
       " 8972: 797,\n",
       " 8984: 798,\n",
       " 27741: 799,\n",
       " 30812: 800,\n",
       " 30816: 801,\n",
       " 31878: 802,\n",
       " 32029: 803,\n",
       " 32031: 804,\n",
       " 32296: 805,\n",
       " 32587: 806,\n",
       " 33162: 807,\n",
       " 33493: 808,\n",
       " 33794: 809,\n",
       " 33836: 810,\n",
       " 34048: 811,\n",
       " 34319: 812,\n",
       " 37741: 813,\n",
       " 38388: 814,\n",
       " 42002: 815,\n",
       " 45499: 816,\n",
       " 45517: 817,\n",
       " 45668: 818,\n",
       " 45730: 819,\n",
       " 46530: 820,\n",
       " 48783: 821,\n",
       " 48997: 822,\n",
       " 49272: 823,\n",
       " 49278: 824,\n",
       " 49286: 825,\n",
       " 49824: 826,\n",
       " 586: 827,\n",
       " 187: 828,\n",
       " 627: 829,\n",
       " 922: 830,\n",
       " 1037: 831,\n",
       " 1095: 832,\n",
       " 1674: 833,\n",
       " 1987: 834,\n",
       " 2011: 835,\n",
       " 2023: 836,\n",
       " 2300: 837,\n",
       " 2877: 838,\n",
       " 2901: 839,\n",
       " 3173: 840,\n",
       " 3328: 841,\n",
       " 3735: 842,\n",
       " 4131: 843,\n",
       " 4558: 844,\n",
       " 5447: 845,\n",
       " 5451: 846,\n",
       " 5481: 847,\n",
       " 5507: 848,\n",
       " 5841: 849,\n",
       " 5843: 850,\n",
       " 5872: 851,\n",
       " 5890: 852,\n",
       " 5891: 853,\n",
       " 5893: 854,\n",
       " 5902: 855,\n",
       " 5956: 856,\n",
       " 5962: 857,\n",
       " 5965: 858,\n",
       " 5988: 859,\n",
       " 6001: 860,\n",
       " 6044: 861,\n",
       " 1028: 862,\n",
       " 1088: 863,\n",
       " 1247: 864,\n",
       " 1307: 865,\n",
       " 3882: 866,\n",
       " 4447: 867,\n",
       " 5066: 868,\n",
       " 5377: 869,\n",
       " 5620: 870,\n",
       " 5943: 871,\n",
       " 5957: 872,\n",
       " 6155: 873,\n",
       " 6266: 874,\n",
       " 6377: 875,\n",
       " 6535: 876,\n",
       " 6942: 877,\n",
       " 7149: 878,\n",
       " 7151: 879,\n",
       " 7154: 880,\n",
       " 7169: 881,\n",
       " 7293: 882,\n",
       " 7375: 883,\n",
       " 7451: 884,\n",
       " 7458: 885,\n",
       " 8529: 886,\n",
       " 8533: 887,\n",
       " 8869: 888,\n",
       " 8969: 889,\n",
       " 30749: 890,\n",
       " 31433: 891,\n",
       " 31685: 892,\n",
       " 33145: 893,\n",
       " 33679: 894,\n",
       " 40629: 895,\n",
       " 40819: 896,\n",
       " 41285: 897,\n",
       " 47099: 898,\n",
       " 51662: 899,\n",
       " 51705: 900,\n",
       " 51834: 901,\n",
       " 54286: 902,\n",
       " 56367: 903,\n",
       " 56949: 904,\n",
       " 58047: 905,\n",
       " 59333: 906,\n",
       " 59421: 907,\n",
       " 60397: 908,\n",
       " 60950: 909,\n",
       " 61250: 910,\n",
       " 63113: 911,\n",
       " 63992: 912,\n",
       " 64969: 913,\n",
       " 66203: 914,\n",
       " 68954: 915,\n",
       " 69406: 916,\n",
       " 69844: 917,\n",
       " 70183: 918,\n",
       " 70293: 919,\n",
       " 71579: 920,\n",
       " 72011: 921,\n",
       " 72330: 922,\n",
       " 72407: 923,\n",
       " 72720: 924,\n",
       " 72737: 925,\n",
       " 72998: 926,\n",
       " 73017: 927,\n",
       " 74450: 928,\n",
       " 77841: 929,\n",
       " 78772: 930,\n",
       " 79091: 931,\n",
       " 80549: 932,\n",
       " 81784: 933,\n",
       " 81845: 934,\n",
       " 81847: 935,\n",
       " 82167: 936,\n",
       " 82499: 937,\n",
       " 84374: 938,\n",
       " 86548: 939,\n",
       " 87222: 940,\n",
       " 88163: 941,\n",
       " 88810: 942,\n",
       " 91104: 943,\n",
       " 92259: 944,\n",
       " 94070: 945,\n",
       " 95167: 946,\n",
       " 95449: 947,\n",
       " 95510: 948,\n",
       " 95543: 949,\n",
       " 96079: 950,\n",
       " 97024: 951,\n",
       " 97938: 952,\n",
       " 98203: 953,\n",
       " 103335: 954,\n",
       " 103339: 955,\n",
       " 104374: 956,\n",
       " 105211: 957,\n",
       " 106489: 958,\n",
       " 106696: 959,\n",
       " 107141: 960,\n",
       " 109374: 961,\n",
       " 109853: 962,\n",
       " 112006: 963,\n",
       " 113275: 964,\n",
       " 113394: 965,\n",
       " 119145: 966,\n",
       " 129428: 967,\n",
       " 136020: 968,\n",
       " 137595: 969,\n",
       " 140110: 970,\n",
       " 44: 971,\n",
       " 376: 972,\n",
       " 511: 973,\n",
       " 529: 974,\n",
       " 1100: 975,\n",
       " 1358: 976,\n",
       " 1370: 977,\n",
       " 1385: 978,\n",
       " 1438: 979,\n",
       " 1518: 980,\n",
       " 1586: 981,\n",
       " 1604: 982,\n",
       " 1608: 983,\n",
       " 1616: 984,\n",
       " 1687: 985,\n",
       " 1693: 986,\n",
       " 1721: 987,\n",
       " 1840: 988,\n",
       " 1882: 989,\n",
       " 1918: 990,\n",
       " 2002: 991,\n",
       " 2027: 992,\n",
       " 1357: 993,\n",
       " 1405: 994,\n",
       " 1876: 995,\n",
       " 2072: 996,\n",
       " 2100: 997,\n",
       " 2421: 998,\n",
       " 2485: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieId_unique = data[\"movieId\"].unique()\n",
    "movie_ids_map = {movie_id: index for index, movie_id in enumerate(movieId_unique)}\n",
    "movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId  movieId  rating  timestamp\n",
       "False   False    False   False        100836\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_sets(data_path, train_prop = 0.9, rdmS=42):\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    userId_unique = data[\"userId\"].unique()\n",
    "    movieId_unique = data[\"movieId\"].unique()\n",
    "    \n",
    "    user_ids_map = {user_id: index for index, user_id in enumerate(userId_unique)}\n",
    "    movie_ids_map = {movie_id: index for index, movie_id in enumerate(movieId_unique)}\n",
    "    \n",
    "    data[\"userId\"] = data[\"userId\"].map(user_ids_map)\n",
    "    data[\"movieId\"] = data[\"movieId\"].map(movie_ids_map)\n",
    "    \n",
    "    data = data.sample(frac=1)\n",
    "    \n",
    "    train, test = train_test_split(data, train_size=train_prop, random_state=rdmS)\n",
    "    \n",
    "    nb_users = len(userId_unique)\n",
    "    nb_movies = len(movieId_unique)\n",
    "    \n",
    "    return train, test, nb_users, nb_movies, user_ids_map, movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9724 movies, 610 users, and 100836 ratings\n"
     ]
    }
   ],
   "source": [
    "ratings_s_path =  '../data/ml-latest-small/ratings.csv'\n",
    "train, test, nb_users, nb_movies, user_ids_map, movie_ids_map = get_train_test_sets(ratings_s_path)\n",
    "dataset = pd.concat((train,test), axis = 0)\n",
    "\n",
    "print(\"There are %i movies, %i users, and %i ratings\" % (nb_movies, nb_users, dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [train[\"userId\"].to_numpy(), train[\"movieId\"].to_numpy()]\n",
    "y_train = train[\"rating\"].to_numpy()\n",
    "\n",
    "X_test = [test[\"userId\"].to_numpy(), test[\"movieId\"].to_numpy()]\n",
    "y_test = test[\"rating\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90752"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the ratings distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['rating'].hist(bins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset there are a lot of missing values, because not all the user/movie pairs have an associated rating. Indeed, each user rates only a few movies ! The goal of this notebook is to predict (some of) the missing user/movie ratings.\n",
    "\n",
    "Print how many movies each of the 5 first users have rated, and print the percentage of available ratings in the whole dataset (i.e. the ratio between number of ratings and all the possible users/movies combinations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"rating\"].isna().value_counts().values[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--> There are no nan in rating columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Id : 1 --> 29 movies rated\n",
      "User Id : 2 --> 39 movies rated\n",
      "User Id : 3 --> 216 movies rated\n",
      "User Id : 4 --> 44 movies rated\n",
      "User Id : 5 --> 314 movies rated\n"
     ]
    }
   ],
   "source": [
    "for i in [1, 2, 3, 4, 5]:\n",
    "    films = len(dataset[dataset[\"userId\"] == i])\n",
    "    print(f\"User Id : {i} --> {films} movies rated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage : 1.7\n"
     ]
    }
   ],
   "source": [
    "combination = nb_users * nb_movies\n",
    "ratings_num = len(dataset[\"rating\"])\n",
    "print(f\"Pourcentage : {round(ratings_num/combination*100, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only 1.7% of ratings that are available, which is normal as each hasn't rated all the movies. To see the dataset in a matrix form with all the missing ratings, use the `Dataframe.pivot()` function, with the `userId` as index, the `movieId` as columns, and the ratings for the `values` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9714</th>\n",
       "      <th>9715</th>\n",
       "      <th>9716</th>\n",
       "      <th>9717</th>\n",
       "      <th>9718</th>\n",
       "      <th>9719</th>\n",
       "      <th>9720</th>\n",
       "      <th>9721</th>\n",
       "      <th>9722</th>\n",
       "      <th>9723</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  0     1     2     3     4     5     6     7     8     9     ...   \n",
       "userId                                                               ...   \n",
       "0         4.0   4.0   4.0   5.0   5.0   3.0   5.0   4.0   5.0   5.0  ...  \\\n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3         NaN   NaN   NaN   2.0   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         4.0   NaN   NaN   NaN   4.0   NaN   NaN   4.0   NaN   NaN  ...   \n",
       "\n",
       "movieId  9714  9715  9716  9717  9718  9719  9720  9721  9722  9723  \n",
       "userId                                                               \n",
       "0         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[5 rows x 9724 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "pivoted_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the ratings of user 1. To do so, use the *movies.csv* file and your `movie_ids_map` dictionnary to find the movie title from the new movie indexes, and print the real movie title associated to each rating of user 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_title = pd.read_csv(\"../data/ml-latest-small/movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_user_1 = pivoted_dataset.loc[1,:]\n",
    "movie_user_1 = movie_user_1.dropna()\n",
    "merged_df = pd.merge(movie_user_1, movie_title, on='movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Cure, The (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Eat Drink Man Woman (Yin shi nan nu) (1994)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>233</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Exotica (1994)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Exit to Eden (1994)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>235</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Ed Wood (1994)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>French Kiss (1995)</td>\n",
       "      <td>Action|Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>237</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Forget Paris (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>238</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Far From Home: The Adventures of Yellow Dog (1...</td>\n",
       "      <td>Adventure|Children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>239</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Goofy Movie, A (1995)</td>\n",
       "      <td>Animation|Children|Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>240</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Hideaway (1995)</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>241</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fluke (1995)</td>\n",
       "      <td>Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>242</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Farinelli: il castrato (1994)</td>\n",
       "      <td>Drama|Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>243</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Gordy (1995)</td>\n",
       "      <td>Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>246</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Hoop Dreams (1994)</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>247</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Heavenly Creatures (1994)</td>\n",
       "      <td>Crime|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Houseguest (1994)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>249</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Immortal Beloved (1994)</td>\n",
       "      <td>Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>250</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Heavyweights (Heavy Weights) (1995)</td>\n",
       "      <td>Children|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>251</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Hunted, The (1995)</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I.Q. (1994)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>253</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Interview with the Vampire: The Vampire Chroni...</td>\n",
       "      <td>Drama|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>254</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Jefferson in Paris (1995)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>255</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Jerky Boys, The (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>256</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Junior (1994)</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>257</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Just Cause (1995)</td>\n",
       "      <td>Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>258</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Kid in King Arthur's Court, A (1995)</td>\n",
       "      <td>Adventure|Children|Comedy|Fantasy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId    1                                              title   \n",
       "0        18  4.0                                  Four Rooms (1995)  \\\n",
       "1       219  4.0                                   Cure, The (1995)   \n",
       "2       232  3.0        Eat Drink Man Woman (Yin shi nan nu) (1994)   \n",
       "3       233  4.5                                     Exotica (1994)   \n",
       "4       234  4.0                                Exit to Eden (1994)   \n",
       "5       235  3.5                                     Ed Wood (1994)   \n",
       "6       236  4.0                                 French Kiss (1995)   \n",
       "7       237  4.0                                Forget Paris (1995)   \n",
       "8       238  4.5  Far From Home: The Adventures of Yellow Dog (1...   \n",
       "9       239  5.0                              Goofy Movie, A (1995)   \n",
       "10      240  4.5                                    Hideaway (1995)   \n",
       "11      241  3.0                                       Fluke (1995)   \n",
       "12      242  4.0                      Farinelli: il castrato (1994)   \n",
       "13      243  3.0                                       Gordy (1995)   \n",
       "14      246  5.0                                 Hoop Dreams (1994)   \n",
       "15      247  4.0                          Heavenly Creatures (1994)   \n",
       "16      248  5.0                                  Houseguest (1994)   \n",
       "17      249  3.5                            Immortal Beloved (1994)   \n",
       "18      250  2.5                Heavyweights (Heavy Weights) (1995)   \n",
       "19      251  3.5                                 Hunted, The (1995)   \n",
       "20      252  5.0                                        I.Q. (1994)   \n",
       "21      253  3.0  Interview with the Vampire: The Vampire Chroni...   \n",
       "22      254  4.0                          Jefferson in Paris (1995)   \n",
       "23      255  2.0                             Jerky Boys, The (1995)   \n",
       "24      256  3.5                                      Junior (1994)   \n",
       "25      257  5.0                                  Just Cause (1995)   \n",
       "26      258  5.0               Kid in King Arthur's Court, A (1995)   \n",
       "\n",
       "                                       genres  \n",
       "0                                      Comedy  \n",
       "1                                       Drama  \n",
       "2                        Comedy|Drama|Romance  \n",
       "3                                       Drama  \n",
       "4                                      Comedy  \n",
       "5                                Comedy|Drama  \n",
       "6                       Action|Comedy|Romance  \n",
       "7                              Comedy|Romance  \n",
       "8                          Adventure|Children  \n",
       "9           Animation|Children|Comedy|Romance  \n",
       "10                                   Thriller  \n",
       "11                             Children|Drama  \n",
       "12                              Drama|Musical  \n",
       "13                    Children|Comedy|Fantasy  \n",
       "14                                Documentary  \n",
       "15                                Crime|Drama  \n",
       "16                                     Comedy  \n",
       "17                              Drama|Romance  \n",
       "18                            Children|Comedy  \n",
       "19                                     Action  \n",
       "20                             Comedy|Romance  \n",
       "21                               Drama|Horror  \n",
       "22                                      Drama  \n",
       "23                                     Comedy  \n",
       "24                              Comedy|Sci-Fi  \n",
       "25                           Mystery|Thriller  \n",
       "26  Adventure|Children|Comedy|Fantasy|Romance  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a python library made for easily designing complex models such as deep learning models, in this module we are going to use just a few features from it to implement our simple matrix factorization model, as it makes a good introduction to the library before the next module about deep learning where you will also be using Keras.\n",
    "\n",
    "The following function `get_mf_model` implements the model described in equation (2) in Koren's paper (without the $+\\lambda(\\ldots)$ part for the moment). So it basically tries to find the $p_u \\in \\mathbb{R}^k$ and $q_i \\in \\mathbb{R}^k$ vectors that minimizes the squared loss between their dot product $p_u^Tq_i$, and the observed ratings $r_{ui}$, from random initialization of $p_u$ and $q_i$. In machine learning terms, $p_u$ and $q_i$ are called the *embeddings* of the user $u$ and of the movie $i$ respectively. Their size $k$ is an hyper-parameter of the model, which is called the *rank* of the factorization.\n",
    "\n",
    "To do so, it uses the functional API from Keras (the other API proposed is the sequential one, but is not adapted for this model), you can read about it here : https://keras.io/guides/functional_api/ .\n",
    "\n",
    "Keras, unlike Numpy, uses a different progamming paradigm. Numpy uses an *imperative* programming style (like python in general), meaning that when you execute `x.dot(y)`, the dot product is actually calculated. Keras however, uses a *declarative* (also called *symbolic*) programming style, meaning that when you write `Dot()([x, y])`, you tell Keras than when you will call the *fit* function of your model in the future, you will want to do a dot product between the future values that *x* and *y* will have. And this is what Keras is about, it allows you to build your own model as a sequence of operations, describing each input and output, and then later fit it and predict with it.\n",
    "\n",
    "Let's not get in too many details, but retain that the `get_mf_model` function below is not actually executing the model, it creates it, and returns an object of the class `keras.models.Model` that has been instructed with your model operations, and this object can then be trained with the classic `fit` and `predict` functions. \n",
    "\n",
    "Read carefully the comments in the code of the function to understand the different steps in the model creation process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERAS : 2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"KERAS : {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/vs/63szyqqn4dq91nzk275599p40000gn/T/ipykernel_34446/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:08:00.105491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-12 12:08:00.105593: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Reshape, Dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:08:24.961297: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-12 12:08:24.961368: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 1, 30) dtype=float32 (created by layer 'p_u__user_embedding')>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_embedddings_2 = 30\n",
    "p_u = Embedding(nb_users, dim_embedddings_2, name=\"p_u__user_embedding\")(u)\n",
    "p_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_mf_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a simple matrix factorization model from\n",
    "    the number of user, the number of movies, and the size of the embeddings k.\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization\n",
    "        \n",
    "    \"\"\"\n",
    "    dim_embedddings = k\n",
    "    \n",
    "    #Inputs:\n",
    "    #First we describe the input of the model, that is the training data that we will give it as X\n",
    "    #In our case, the input are just the user index u and the movie index i.\n",
    "    #So we declare two inputs of size one:\n",
    "    \n",
    "    u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    #Then let's declare our variable, the embeddings p and q.\n",
    "    #First with the users, we declare that we have nb_users embeddings, each of size dim_embeddings.\n",
    "    #An embedding object is indexed by calling it with the index parameter like a function,\n",
    "    #so we add a `(u)` at the end to tell keras we want it to be indexed \n",
    "    #by the user ids we will pass at training time as inputs.\n",
    "    \n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    \n",
    "    #Unfortunatly, when indexing an embeddings it keeps [1,k] matrix shape instead\n",
    "    #of just a [k] vector, so we have to tell Keras that we just want a vector by\n",
    "    #redefining its shape:\n",
    "    \n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # Same thing for the movie embeddings:\n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    #Then the dot product between the two indexed embeddings, \n",
    "    #we'll understand the axes = 1 part later.\n",
    "    r_hat = Dot(axes = 1)([q_i, p_u])\n",
    "\n",
    "    #We define our model by giving its input and outputs, in our case\n",
    "    #the user and movie ids will be the inputs, and the output will be\n",
    "    #the estimated rating r_hat, that is the dot product of the \n",
    "    #corresponding embeddings.\n",
    "    model = Model(inputs=[u, i], outputs=r_hat)\n",
    "    \n",
    "    #Finally, we define the loss and metric to use, in our case the mean squared error,\n",
    "    #along with the optimization method, we'll understand what is 'adam' later also.\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u = Input(shape=(1,), dtype='int32', name = \"u__user_id\")\n",
    "p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "\n",
    "r_hat = Dot(axes = 1)([q_i, p_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "mf_model = get_mf_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows us to have a textual overview of the model we defined with the *summary()* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_users * K : 18300\n",
      "nb_movies * K : 291720\n"
     ]
    }
   ],
   "source": [
    "print(f\"nb_users * K : {nb_users*30}\")\n",
    "print(f\"nb_movies * K : {nb_movies*30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 310,020\n",
      "Trainable params: 310,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Each of the keras objects we defined in our model is called a *layer*, and we find them in order in the first column. The *Param #* column gives the number of trainable parameters of the layer, in our case these are just the embeddings, and they should be equal to $nb\\_users \\times k$ and $nb\\_movies \\times k$. The *Connected to* column tells for each layer which layers are inputs for this layer (you can safely ignore the `[0][0]` for this module).\n",
    "\n",
    "Finally the *Output Shape* column gives us the shape of the layer, each layer being a *tensor*. A tensor is the generalization of matrices to more than two dimensions. So a matrix is a 2D-tensor and a vector is a 1D-tensor, and each layer can be a matrix, a vector, or a higher-order tensor. The output shape we see is indeed the expected one at each layer, except there is this `None` in first dimension, why is that ?\n",
    "\n",
    "To understand it, we have to get into how Keras is actually minimizing the mean squared loss of our model. In general, when in comes to minimizing error functions on big datasets, a generic method is to use Stocastic Gradient Descent (SGD), briefly described in page 4 of Koren's article. \n",
    "\n",
    "Read about gradient descent, SGD and its variant mini-batch SGD in Chapter 4 of *Hands on ML ...* (pages 111-120):\n",
    "https://drive.google.com/file/d/1t0rc3x5YQBgLXVLET6BzR4jn5vzMI_m0/view?usp=sharing\n",
    "\n",
    "This is what Keras does when it fits the model, it initializes the $q_i$ and $p_u$ embedding vectors randomly, and then perform mini-batch SGD to find the minimum mean squared error on the training set. Since mini-batching means considering multiple training samples at the same time, Keras keeps the first dimension of each layer to stack the samples of each batch, this is why `None` is written, the actual batch_size being set at training time when calling the `fit` function. This is also why we had to set `axes=1` when calling the `Dot` layer in the `get_mf_model` function, because the first dimension (axe 0) of each layer is kept for the batches. And about the `optimizer='adam'`, it is just a variation of mini-batch SGD that is faster, we'll get into more details about SGD variations in the optional parts of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally train our matrix factorization model on our movieLens data. The `epochs` parameter controls the number of iterations of the SGD algorithm, that is the number of times it is going to pass on each training rating and update the embeddings accordingly. Let's keep it at 20 for the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5595 - mse: 0.5595\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5477 - mse: 0.5477\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5364 - mse: 0.5364\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5252 - mse: 0.5252\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5138 - mse: 0.5138\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5026 - mse: 0.5026\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4910 - mse: 0.4910\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4789 - mse: 0.4789\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4669 - mse: 0.4669\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4551 - mse: 0.4551\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4428 - mse: 0.4428\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4306 - mse: 0.4306\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4184 - mse: 0.4184\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4059 - mse: 0.4059\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3939 - mse: 0.3939\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3819 - mse: 0.3819\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3699 - mse: 0.3699\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3580 - mse: 0.3580\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3465 - mse: 0.3465\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.3351 - mse: 0.3351\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now try to predict the test ratings, and report our root mean squared error like in other regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45/316 [===>..........................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:10:06.705208: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 3ms/step\n",
      " Test RMSE : 1.0802678268342425 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "y_pred = mf_model.predict(X_test)\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 1.1/1.2 RMSE, we can probably do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the same model on your GPU and on your CPU, and compare the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the `tensorflow` python library for the computation part, if you have installed your GPU drivers and the GPU version of keras, then it will run on your GPU by default. We can force tensorflow to use the cpu instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPUs :  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "------------------------------------------------------------------\n",
      "Num CPUs Available:  1\n",
      "CPUs :  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPUs : \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"CPUs : \", tf.config.list_physical_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 13.3321 - mse: 13.3321\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 11.8761 - mse: 11.8761\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 6.1857 - mse: 6.1857\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.7879 - mse: 2.7879\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.8157 - mse: 1.8157\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.3781 - mse: 1.3781\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1267 - mse: 1.1267\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9661 - mse: 0.9661\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8561 - mse: 0.8561\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7774 - mse: 0.7774\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7188 - mse: 0.7188\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6738 - mse: 0.6738\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6382 - mse: 0.6382\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6094 - mse: 0.6094\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5853 - mse: 0.5853\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5648 - mse: 0.5648\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5469 - mse: 0.5469\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5311 - mse: 0.5311\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5165 - mse: 0.5165\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5032 - mse: 0.5032\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    mf_model = get_mf_model(nb_users, nb_movies, k)\n",
    "    history = mf_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix farctorization model is a really simple model, with not enough operations to parallelize on the GPU, this is why the training time is quite similar for this model. However with deep networks models the training time can be up to 10x times faster on a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding user and movie bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enhance our matrix factorization model and add the user and movie biases to the rating estimation function as in equation (4) of Koren's paper ; except we will for the moment forget about the global bias $\\mu$ as it is not so intuitive to implement in Keras. Fill the function below to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Add\n",
    "\n",
    "def get_mf_bias_model(nb_users, nb_movies, k):\n",
    "    \"\"\"\n",
    "    Build a smatrix factorization model with user and movie biases\n",
    "    \n",
    "    Input:\n",
    "        nb_users : int : The number of unique users\n",
    "        nb_movies : int : The number of unique movies\n",
    "        k : int : The size of the embeddings\n",
    "        \n",
    "    Output:\n",
    "        model : keras.models.Model : A keras model that implements matrix factorization with biases\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    \n",
    "    p_u = Embedding(nb_users, dim_embedddings, name=\"p_u__user_embedding\")(u)\n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # User bias embeddings\n",
    "    bu = Embedding(nb_users, 1, name=\"bu__user_bias\")(u)\n",
    "    bu = Reshape((1,), name=\"bu__user_bias_reshaped\")(bu)\n",
    "    \n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    q_i = Embedding(nb_movies, dim_embedddings, name=\"q_i__movie_embedding\")(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    # Movie bias embeddings\n",
    "    bi = Embedding(nb_movies, 1, name=\"bi__movie_bias\")(i)\n",
    "    bi = Reshape((1,), name=\"bi__movie_bias_reshaped\")(bi)\n",
    "    \n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    # Add user and movie biases\n",
    "    d = Add()([d, bu, bi])\n",
    "    \n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " bu__user_bias (Embedding)      (None, 1, 1)         610         ['u__user_id[0][0]']             \n",
      "                                                                                                  \n",
      " bi__movie_bias (Embedding)     (None, 1, 1)         9724        ['i__movie_id[0][0]']            \n",
      "                                                                                                  \n",
      " dot_5 (Dot)                    (None, 1)            0           ['p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " bu__user_bias_reshaped (Reshap  (None, 1)           0           ['bu__user_bias[0][0]']          \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bi__movie_bias_reshaped (Resha  (None, 1)           0           ['bi__movie_bias[0][0]']         \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 1)            0           ['dot_5[0][0]',                  \n",
      "                                                                  'bu__user_bias_reshaped[0][0]', \n",
      "                                                                  'bi__movie_bias_reshaped[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 12.6349 - mse: 12.6349\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 9.7952 - mse: 9.7952\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 4.2914 - mse: 4.2914\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.0750 - mse: 2.0750\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.4469 - mse: 1.4469\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1505 - mse: 1.1505\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9795 - mse: 0.9795\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8699 - mse: 0.8699\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7944 - mse: 0.7944\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7397 - mse: 0.7397\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6988 - mse: 0.6988\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6667 - mse: 0.6667\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6418 - mse: 0.6418\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6209 - mse: 0.6209\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6028 - mse: 0.6028\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5871 - mse: 0.5871\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5726 - mse: 0.5726\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5591 - mse: 0.5591\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5461 - mse: 0.5461\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5335 - mse: 0.5335\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    mf_bias_model = get_mf_bias_model(nb_users, nb_movies, k)\n",
    "    history = mf_bias_model.fit(X_train, y_train, epochs=20, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41/316 [==>...........................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:10:19.994654: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 4ms/step\n",
      " Test RMSE : 1.0180034484751832 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a lower RMSE, about 1.0/1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the moment we have omitted the regularization of the embeddings and bias parameters, as described in equation (5) of Koren's paper. We are now going to add them to the model, have a look at https://keras.io/layers/embeddings/ and https://keras.io/regularizers/ to see how to do this with keras. Fill the function below to implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "def get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_):\n",
    "    \n",
    "    dim_embedddings = k\n",
    "    \n",
    "    # User embeddings\n",
    "    u = Input(shape=(1,), dtype='int32', name = 'u__user_id')\n",
    "    \n",
    "    p_u = Embedding(\n",
    "        nb_users, \n",
    "        dim_embedddings, \n",
    "        name=\"p_u__user_embedding\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(u)\n",
    "    \n",
    "    p_u = Reshape((dim_embedddings,), name=\"p_u__user_embedding_reshaped\")(p_u)\n",
    "    \n",
    "    # User bias embeddings\n",
    "    bu = Embedding(\n",
    "        nb_users, \n",
    "        1, \n",
    "        name=\"bu__user_bias\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(u)\n",
    "    bu = Reshape((1,), name=\"bu__user_bias_reshaped\")(bu)\n",
    "    \n",
    "    \n",
    "    # Movie embeddings\n",
    "    i = Input(shape=(1,), dtype='int32', name = 'i__movie_id')\n",
    "    \n",
    "    q_i = Embedding(\n",
    "        nb_movies, \n",
    "        dim_embedddings, \n",
    "        name=\"q_i__movie_embedding\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(i)\n",
    "    q_i = Reshape((dim_embedddings,), name=\"q_i__movie_embedding_reshaped\")(q_i)\n",
    "    \n",
    "    # Movie bias embeddings\n",
    "    bi = Embedding(\n",
    "        nb_movies, \n",
    "        1, \n",
    "        name=\"bi__movie_bias\",\n",
    "        embeddings_regularizer=regularizers.l2(lambda_)\n",
    "    )(i)\n",
    "    bi = Reshape((1,), name=\"bi__movie_bias_reshaped\")(bi)\n",
    "    \n",
    "    # Dot product\n",
    "    d = Dot(axes = 1)([p_u, q_i])\n",
    "    \n",
    "    # Add user and movie biases\n",
    "    d = Add()([d, bu, bi])\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[u, i], outputs=d)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[\"mse\"])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " u__user_id (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " i__movie_id (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding (Embedding  (None, 1, 30)       18300       ['u__user_id[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " q_i__movie_embedding (Embeddin  (None, 1, 30)       291720      ['i__movie_id[0][0]']            \n",
      " g)                                                                                               \n",
      "                                                                                                  \n",
      " p_u__user_embedding_reshaped (  (None, 30)          0           ['p_u__user_embedding[0][0]']    \n",
      " Reshape)                                                                                         \n",
      "                                                                                                  \n",
      " q_i__movie_embedding_reshaped   (None, 30)          0           ['q_i__movie_embedding[0][0]']   \n",
      " (Reshape)                                                                                        \n",
      "                                                                                                  \n",
      " bu__user_bias (Embedding)      (None, 1, 1)         610         ['u__user_id[0][0]']             \n",
      "                                                                                                  \n",
      " bi__movie_bias (Embedding)     (None, 1, 1)         9724        ['i__movie_id[0][0]']            \n",
      "                                                                                                  \n",
      " dot_7 (Dot)                    (None, 1)            0           ['p_u__user_embedding_reshaped[0]\n",
      "                                                                 [0]',                            \n",
      "                                                                  'q_i__movie_embedding_reshaped[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " bu__user_bias_reshaped (Reshap  (None, 1)           0           ['bu__user_bias[0][0]']          \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bi__movie_bias_reshaped (Resha  (None, 1)           0           ['bi__movie_bias[0][0]']         \n",
      " pe)                                                                                              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 1)            0           ['dot_7[0][0]',                  \n",
      "                                                                  'bu__user_bias_reshaped[0][0]', \n",
      "                                                                  'bi__movie_bias_reshaped[0][0]']\n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 320,354\n",
      "Trainable params: 320,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "mf_bias_reg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 12.6652 - mse: 12.6652\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 9.9273 - mse: 9.9273\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 4.4371 - mse: 4.4371\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 2.1028 - mse: 2.1028\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.4531 - mse: 1.4531\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1521 - mse: 1.1521\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9807 - mse: 0.9807\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8726 - mse: 0.8726\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7989 - mse: 0.7989\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7457 - mse: 0.7456\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7056 - mse: 0.7056\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6748 - mse: 0.6748\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6493 - mse: 0.6493\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6286 - mse: 0.6286\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6105 - mse: 0.6105\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5944 - mse: 0.5944\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5801 - mse: 0.5800\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5660 - mse: 0.5660\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5524 - mse: 0.5523\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5391 - mse: 0.5391\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "    history = mf_bias_reg_model.fit(X_train, y_train, epochs=20, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41/316 [==>...........................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:10:28.636322: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 1s 4ms/step\n",
      " Test RMSE : 1.0132317365705723 \n"
     ]
    }
   ],
   "source": [
    "y_pred = mf_bias_reg_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\" Test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a slightly better RMSE, but sometimes regularization is very important for achieving good test performances, in depends on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of setting manually the maximum number of epochs, we prefer to use *early stopping*. When training with early stopping, keras keeps a given validation set though the parameter `validation_split`, on which it is going to monitor a performance measure you give it (here the `mse`) at every epoch, and continue optimization while the mse on the validation set keeps going down, and stops it when it goes back up. This mechanism is an easy way to avoid over-fitting, you can read more about it there : https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "\n",
    "In general when using early stopping we setup a high number of maximum epochs, that is never reach because the optimization is stopped by early stopping first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_bias_reg_model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_list.append(early_stopping)\n",
    "cb_list.append(tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:10:30.415694: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/160 [============================>.] - ETA: 0s - loss: 12.7522 - mse: 12.7522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:10:32.129008: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2s 11ms/step - loss: 12.7414 - mse: 12.7414 - val_loss: 12.0131 - val_mse: 12.0131\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 10.5868 - mse: 10.5868 - val_loss: 8.3091 - val_mse: 8.3091\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 5.5000 - mse: 5.5000 - val_loss: 3.5498 - val_mse: 3.5498\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 2.4882 - mse: 2.4882 - val_loss: 2.1948 - val_mse: 2.1948\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 1.6342 - mse: 1.6341 - val_loss: 1.7216 - val_mse: 1.7216\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 1.2622 - mse: 1.2622 - val_loss: 1.4827 - val_mse: 1.4827\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 1.0540 - mse: 1.0540 - val_loss: 1.3461 - val_mse: 1.3461\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.9244 - mse: 0.9243 - val_loss: 1.2599 - val_mse: 1.2599\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.8369 - mse: 0.8369 - val_loss: 1.2045 - val_mse: 1.2044\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.7748 - mse: 0.7748 - val_loss: 1.1671 - val_mse: 1.1671\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.7285 - mse: 0.7285 - val_loss: 1.1408 - val_mse: 1.1408\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.6929 - mse: 0.6929 - val_loss: 1.1239 - val_mse: 1.1239\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.6639 - mse: 0.6639 - val_loss: 1.1113 - val_mse: 1.1113\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.6405 - mse: 0.6405 - val_loss: 1.1007 - val_mse: 1.1007\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.6206 - mse: 0.6206 - val_loss: 1.0958 - val_mse: 1.0958\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.6030 - mse: 0.6030 - val_loss: 1.0909 - val_mse: 1.0909\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 1s 9ms/step - loss: 0.5873 - mse: 0.5873 - val_loss: 1.0878 - val_mse: 1.0878\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.5727 - mse: 0.5727 - val_loss: 1.0857 - val_mse: 1.0856\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5587 - mse: 0.5587 - val_loss: 1.0830 - val_mse: 1.0830\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5455 - mse: 0.5455 - val_loss: 1.0841 - val_mse: 1.0841\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.5327 - mse: 0.5327 - val_loss: 1.0831 - val_mse: 1.0831\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.5202 - mse: 0.5201 - val_loss: 1.0823 - val_mse: 1.0823\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.5074 - mse: 0.5074 - val_loss: 1.0823 - val_mse: 1.0823\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.4947 - mse: 0.4947 - val_loss: 1.0836 - val_mse: 1.0836\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.4820 - mse: 0.4820 - val_loss: 1.0861 - val_mse: 1.0861\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 2s 9ms/step - loss: 0.4691 - mse: 0.4691 - val_loss: 1.0853 - val_mse: 1.0853\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4564 - mse: 0.4564 - val_loss: 1.0858 - val_mse: 1.0858\n",
      "Epoch 28/500\n",
      "157/160 [============================>.] - ETA: 0s - loss: 0.4435 - mse: 0.4435Restoring model weights from the end of the best epoch: 23.\n",
      "160/160 [==============================] - 2s 10ms/step - loss: 0.4438 - mse: 0.4438 - val_loss: 1.0873 - val_mse: 1.0873\n",
      "Epoch 28: early stopping\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    history = mf_bias_reg_model.fit(\n",
    "        \n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=500, \n",
    "        batch_size=512, \n",
    "        validation_split=0.1, \n",
    "        callbacks=cb_list\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23600), started 14:53:30 ago. (Use '!kill 23600' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-662a94f69224aebf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-662a94f69224aebf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the training stops before 500 epochs, when the validation MSE stops decreasing during 5 consecutive epochs (the patience value = 5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search embedding size and regularization factor with early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the moment we didn't grid search our model hyper-parameters, such as `k` and `lambda_`. There exists some scikit-learn wrappers for keras models in order to use scikit grid search functions, unfortunately they only work with single input keras models, which is not our case as we have two inputs: the user and the movie indexes.\n",
    "\n",
    "So let's implement your own grid search function for the two parameters `k` and `lambda_`. With big enough datasets, it is not necessary to do a cross-validation for each hyper-parameter combination, and we can simply split the training set into a sub-training set and a validation set to test our hyper-parameters. It does work because the validation set is big enough to see enough data variations, and with very big datasets, it is anyway not possible anymore to do a full cross-validation as it takes too much time to train. \n",
    "\n",
    "Fill in the `grid_search` function below and use early stopping with a validation split (just like above), and retrieve the validation RMSE (you can get the MSE from the `history` variable that is returned by the `fit` method (and then take the `sqrt` of that)) for all the hyper-parameter combinations from the `param_grid` dictionary of hyper-parameter values. Call the `get_model_function` parameter (yes, you can pass functions as parameters!) to generate each model, and return the hyper-parameters that give the lowest RMSE on the 10% validation set, the RMSE value, and the best corresponding trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "mse\n",
      "val_loss\n",
      "val_mse\n"
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(history.history):\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "for i, value in enumerate(history.history[\"mse\"]):\n",
    "    epoch_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.082297682762146"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mse = history.history['mse']\n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "min(val_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=epoch_list, y=val_mse)\n",
    "sns.lineplot(x=epoch_list, y=train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(data, param_grid, get_model_function, nb_users, nb_movies, validation_size = 0.1):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_mse', patience=5, verbose=1, restore_best_weights=True)\n",
    "    \n",
    "    score_list = []\n",
    "    param_list = []\n",
    "    model_list = []\n",
    "    \n",
    "    best_score = np.inf\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    \n",
    "    # Here we will make a loop for each parameters tested\n",
    "\n",
    "    with tf.device('/CPU:0'):\n",
    "        for k, lambda_ in product(ks, lambdas_):\n",
    "            print(\"°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\")\n",
    "            print(\"°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\")\n",
    "            print(f\"TESTING... {k} | {lambda_}\")\n",
    "            print(\"\")\n",
    "            model = get_mf_bias_l2_reg_model(nb_users, nb_movies, k, lambda_)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, \n",
    "                y_train, \n",
    "                epochs=500, \n",
    "                batch_size=512, \n",
    "                validation_split = 0.1,\n",
    "                callbacks=[early_stopping]\n",
    "            )\n",
    "            \n",
    "            \n",
    "            val_mse = history.history['val_mse']\n",
    "            min_mse = min(val_mse)\n",
    "            \n",
    "            score_list.append(min(val_mse))\n",
    "            param_list.append((k, lambda_))\n",
    "            model_list.append(model)\n",
    "            \n",
    "            print(f\"k={k}, lambda_={lambda_}, val_mse={val_mse}\")\n",
    "        \n",
    "    min_index = score_list.index(min(score_list))\n",
    "\n",
    "    best_score = score_list[min_index]\n",
    "    best_params = {'k': param_list[min_index][0], 'lambda':param_list[min_index][1]}\n",
    "    best_model = model_list[min_index]\n",
    "    \n",
    "    \n",
    "\n",
    "    return best_params, best_score, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambdas_ = [0.0002, 0.00005, 0.00002]\n",
    "ks = [15,30]\n",
    "\n",
    "param_grid = dict(k=ks, lambda_=lambdas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 15 | 0.0002\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 12.7485 - mse: 12.7396 - val_loss: 12.0629 - val_mse: 12.0534\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 11.0153 - mse: 10.9701 - val_loss: 9.4675 - val_mse: 9.3518\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.4928 - mse: 7.2439 - val_loss: 5.8057 - val_mse: 5.4058\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 4.5844 - mse: 4.0361 - val_loss: 4.0079 - val_mse: 3.3236\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.4490 - mse: 2.6637 - val_loss: 3.4144 - val_mse: 2.5425\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.0272 - mse: 2.0913 - val_loss: 3.1546 - val_mse: 2.1616\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8155 - mse: 1.7785 - val_loss: 3.0102 - val_mse: 1.9321\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6904 - mse: 1.5803 - val_loss: 2.9194 - val_mse: 1.7804\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6077 - mse: 1.4452 - val_loss: 2.8549 - val_mse: 1.6717\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5484 - mse: 1.3488 - val_loss: 2.8062 - val_mse: 1.5909\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5028 - mse: 1.2766 - val_loss: 2.7665 - val_mse: 1.5297\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4655 - mse: 1.2205 - val_loss: 2.7319 - val_mse: 1.4802\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4334 - mse: 1.1769 - val_loss: 2.7007 - val_mse: 1.4405\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4046 - mse: 1.1427 - val_loss: 2.6707 - val_mse: 1.4065\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3779 - mse: 1.1127 - val_loss: 2.6420 - val_mse: 1.3776\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3529 - mse: 1.0891 - val_loss: 2.6153 - val_mse: 1.3541\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3290 - mse: 1.0700 - val_loss: 2.5871 - val_mse: 1.3303\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3058 - mse: 1.0519 - val_loss: 2.5616 - val_mse: 1.3121\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2836 - mse: 1.0380 - val_loss: 2.5361 - val_mse: 1.2943\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2613 - mse: 1.0242 - val_loss: 2.5093 - val_mse: 1.2769\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2401 - mse: 1.0126 - val_loss: 2.4841 - val_mse: 1.2619\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2188 - mse: 1.0016 - val_loss: 2.4589 - val_mse: 1.2476\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1981 - mse: 0.9924 - val_loss: 2.4343 - val_mse: 1.2342\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1776 - mse: 0.9831 - val_loss: 2.4097 - val_mse: 1.2207\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1578 - mse: 0.9748 - val_loss: 2.3846 - val_mse: 1.2086\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1383 - mse: 0.9677 - val_loss: 2.3597 - val_mse: 1.1947\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1193 - mse: 0.9600 - val_loss: 2.3373 - val_mse: 1.1843\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1008 - mse: 0.9538 - val_loss: 2.3149 - val_mse: 1.1732\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0826 - mse: 0.9475 - val_loss: 2.2916 - val_mse: 1.1615\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0653 - mse: 0.9409 - val_loss: 2.2709 - val_mse: 1.1529\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0483 - mse: 0.9355 - val_loss: 2.2493 - val_mse: 1.1425\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0317 - mse: 0.9306 - val_loss: 2.2281 - val_mse: 1.1331\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0157 - mse: 0.9253 - val_loss: 2.2089 - val_mse: 1.1232\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0003 - mse: 0.9197 - val_loss: 2.1901 - val_mse: 1.1152\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9852 - mse: 0.9150 - val_loss: 2.1717 - val_mse: 1.1071\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9708 - mse: 0.9104 - val_loss: 2.1536 - val_mse: 1.0983\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9569 - mse: 0.9057 - val_loss: 2.1370 - val_mse: 1.0905\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9438 - mse: 0.9020 - val_loss: 2.1207 - val_mse: 1.0828\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9310 - mse: 0.8974 - val_loss: 2.1047 - val_mse: 1.0751\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9189 - mse: 0.8934 - val_loss: 2.0901 - val_mse: 1.0680\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9070 - mse: 0.8889 - val_loss: 2.0751 - val_mse: 1.0617\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8956 - mse: 0.8858 - val_loss: 2.0615 - val_mse: 1.0542\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8848 - mse: 0.8810 - val_loss: 2.0481 - val_mse: 1.0486\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8742 - mse: 0.8778 - val_loss: 2.0362 - val_mse: 1.0432\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8644 - mse: 0.8743 - val_loss: 2.0239 - val_mse: 1.0370\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8550 - mse: 0.8707 - val_loss: 2.0122 - val_mse: 1.0315\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8457 - mse: 0.8676 - val_loss: 2.0013 - val_mse: 1.0255\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8370 - mse: 0.8641 - val_loss: 1.9908 - val_mse: 1.0207\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8285 - mse: 0.8611 - val_loss: 1.9804 - val_mse: 1.0150\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8205 - mse: 0.8566 - val_loss: 1.9711 - val_mse: 1.0101\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8128 - mse: 0.8540 - val_loss: 1.9621 - val_mse: 1.0052\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8053 - mse: 0.8501 - val_loss: 1.9536 - val_mse: 1.0008\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7983 - mse: 0.8473 - val_loss: 1.9464 - val_mse: 0.9975\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7915 - mse: 0.8441 - val_loss: 1.9381 - val_mse: 0.9925\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7850 - mse: 0.8406 - val_loss: 1.9317 - val_mse: 0.9891\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7787 - mse: 0.8381 - val_loss: 1.9253 - val_mse: 0.9859\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7729 - mse: 0.8344 - val_loss: 1.9185 - val_mse: 0.9815\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7672 - mse: 0.8314 - val_loss: 1.9129 - val_mse: 0.9783\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7617 - mse: 0.8283 - val_loss: 1.9064 - val_mse: 0.9742\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7565 - mse: 0.8253 - val_loss: 1.9015 - val_mse: 0.9712\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7515 - mse: 0.8218 - val_loss: 1.8961 - val_mse: 0.9676\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7468 - mse: 0.8190 - val_loss: 1.8921 - val_mse: 0.9650\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7424 - mse: 0.8163 - val_loss: 1.8877 - val_mse: 0.9616\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7381 - mse: 0.8124 - val_loss: 1.8839 - val_mse: 0.9595\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7339 - mse: 0.8095 - val_loss: 1.8804 - val_mse: 0.9573\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7300 - mse: 0.8071 - val_loss: 1.8764 - val_mse: 0.9538\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7264 - mse: 0.8044 - val_loss: 1.8730 - val_mse: 0.9508\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7227 - mse: 0.8010 - val_loss: 1.8704 - val_mse: 0.9494\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7194 - mse: 0.7984 - val_loss: 1.8676 - val_mse: 0.9470\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7163 - mse: 0.7956 - val_loss: 1.8648 - val_mse: 0.9446\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7130 - mse: 0.7930 - val_loss: 1.8622 - val_mse: 0.9425\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7102 - mse: 0.7903 - val_loss: 1.8601 - val_mse: 0.9404\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7074 - mse: 0.7879 - val_loss: 1.8585 - val_mse: 0.9390\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7048 - mse: 0.7852 - val_loss: 1.8560 - val_mse: 0.9362\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7024 - mse: 0.7827 - val_loss: 1.8542 - val_mse: 0.9346\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7000 - mse: 0.7805 - val_loss: 1.8526 - val_mse: 0.9332\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6976 - mse: 0.7780 - val_loss: 1.8508 - val_mse: 0.9312\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6955 - mse: 0.7756 - val_loss: 1.8497 - val_mse: 0.9297\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6935 - mse: 0.7731 - val_loss: 1.8482 - val_mse: 0.9280\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6916 - mse: 0.7708 - val_loss: 1.8473 - val_mse: 0.9272\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6898 - mse: 0.7694 - val_loss: 1.8462 - val_mse: 0.9260\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6880 - mse: 0.7676 - val_loss: 1.8448 - val_mse: 0.9240\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6864 - mse: 0.7650 - val_loss: 1.8442 - val_mse: 0.9226\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6848 - mse: 0.7633 - val_loss: 1.8432 - val_mse: 0.9213\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6833 - mse: 0.7610 - val_loss: 1.8428 - val_mse: 0.9205\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6818 - mse: 0.7591 - val_loss: 1.8421 - val_mse: 0.9195\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6805 - mse: 0.7579 - val_loss: 1.8416 - val_mse: 0.9187\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6791 - mse: 0.7555 - val_loss: 1.8410 - val_mse: 0.9179\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6779 - mse: 0.7542 - val_loss: 1.8404 - val_mse: 0.9165\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6768 - mse: 0.7525 - val_loss: 1.8399 - val_mse: 0.9155\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6756 - mse: 0.7511 - val_loss: 1.8394 - val_mse: 0.9143\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6746 - mse: 0.7490 - val_loss: 1.8390 - val_mse: 0.9137\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6735 - mse: 0.7480 - val_loss: 1.8384 - val_mse: 0.9125\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6725 - mse: 0.7465 - val_loss: 1.8381 - val_mse: 0.9119\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6717 - mse: 0.7451 - val_loss: 1.8383 - val_mse: 0.9116\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6708 - mse: 0.7434 - val_loss: 1.8377 - val_mse: 0.9101\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6700 - mse: 0.7421 - val_loss: 1.8379 - val_mse: 0.9100\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6693 - mse: 0.7409 - val_loss: 1.8378 - val_mse: 0.9094\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6685 - mse: 0.7395 - val_loss: 1.8377 - val_mse: 0.9088\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6678 - mse: 0.7385 - val_loss: 1.8376 - val_mse: 0.9080\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6670 - mse: 0.7368 - val_loss: 1.8379 - val_mse: 0.9076\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6664 - mse: 0.7356 - val_loss: 1.8378 - val_mse: 0.9072\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6657 - mse: 0.7346 - val_loss: 1.8373 - val_mse: 0.9062\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6652 - mse: 0.7339 - val_loss: 1.8374 - val_mse: 0.9058\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6646 - mse: 0.7319 - val_loss: 1.8378 - val_mse: 0.9055\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6640 - mse: 0.7312 - val_loss: 1.8378 - val_mse: 0.9050\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6635 - mse: 0.7306 - val_loss: 1.8378 - val_mse: 0.9046\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6630 - mse: 0.7294 - val_loss: 1.8380 - val_mse: 0.9041\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6626 - mse: 0.7278 - val_loss: 1.8383 - val_mse: 0.9038\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6620 - mse: 0.7271 - val_loss: 1.8383 - val_mse: 0.9033\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6616 - mse: 0.7258 - val_loss: 1.8382 - val_mse: 0.9027\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6612 - mse: 0.7254 - val_loss: 1.8387 - val_mse: 0.9031\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6608 - mse: 0.7243 - val_loss: 1.8391 - val_mse: 0.9030\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6604 - mse: 0.7230 - val_loss: 1.8390 - val_mse: 0.9022\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6601 - mse: 0.7227 - val_loss: 1.8393 - val_mse: 0.9018\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6597 - mse: 0.7217 - val_loss: 1.8396 - val_mse: 0.9016\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6593 - mse: 0.7206 - val_loss: 1.8398 - val_mse: 0.9011\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6589 - mse: 0.7200 - val_loss: 1.8402 - val_mse: 0.9010\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6587 - mse: 0.7190 - val_loss: 1.8404 - val_mse: 0.9006\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6584 - mse: 0.7179 - val_loss: 1.8405 - val_mse: 0.9002\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6580 - mse: 0.7176 - val_loss: 1.8410 - val_mse: 0.9002\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6577 - mse: 0.7167 - val_loss: 1.8413 - val_mse: 0.9000\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6576 - mse: 0.7156 - val_loss: 1.8413 - val_mse: 0.8993\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6572 - mse: 0.7146 - val_loss: 1.8421 - val_mse: 0.8999\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6571 - mse: 0.7144 - val_loss: 1.8420 - val_mse: 0.8993\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6568 - mse: 0.7138 - val_loss: 1.8425 - val_mse: 0.8995\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6566 - mse: 0.7129 - val_loss: 1.8427 - val_mse: 0.8989\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6563 - mse: 0.7120 - val_loss: 1.8432 - val_mse: 0.8990\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6562 - mse: 0.7117 - val_loss: 1.8432 - val_mse: 0.8987\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6559 - mse: 0.7110 - val_loss: 1.8435 - val_mse: 0.8985\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6558 - mse: 0.7103 - val_loss: 1.8434 - val_mse: 0.8981\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6555 - mse: 0.7097 - val_loss: 1.8440 - val_mse: 0.8979\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6554 - mse: 0.7091 - val_loss: 1.8445 - val_mse: 0.8979\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6551 - mse: 0.7082 - val_loss: 1.8452 - val_mse: 0.8984\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6550 - mse: 0.7075 - val_loss: 1.8454 - val_mse: 0.8981\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6549 - mse: 0.7072 - val_loss: 1.8456 - val_mse: 0.8979\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6547 - mse: 0.7068 - val_loss: 1.8462 - val_mse: 0.8980\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6545 - mse: 0.7060 - val_loss: 1.8461 - val_mse: 0.8976\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6544 - mse: 0.7055 - val_loss: 1.8464 - val_mse: 0.8972\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6542 - mse: 0.7048 - val_loss: 1.8468 - val_mse: 0.8977\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6541 - mse: 0.7044 - val_loss: 1.8471 - val_mse: 0.8972\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6540 - mse: 0.7036 - val_loss: 1.8472 - val_mse: 0.8971\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6538 - mse: 0.7033 - val_loss: 1.8477 - val_mse: 0.8973\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6537 - mse: 0.7027 - val_loss: 1.8480 - val_mse: 0.8969\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6536 - mse: 0.7020 - val_loss: 1.8479 - val_mse: 0.8966\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6535 - mse: 0.7023 - val_loss: 1.8484 - val_mse: 0.8969\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6534 - mse: 0.7012 - val_loss: 1.8488 - val_mse: 0.8967\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6533 - mse: 0.7009 - val_loss: 1.8489 - val_mse: 0.8966\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6531 - mse: 0.7005 - val_loss: 1.8492 - val_mse: 0.8965\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6531 - mse: 0.7001 - val_loss: 1.8498 - val_mse: 0.8966\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6529 - mse: 0.6996 - val_loss: 1.8500 - val_mse: 0.8966\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6528 - mse: 0.6989 - val_loss: 1.8500 - val_mse: 0.8963\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6528 - mse: 0.6988 - val_loss: 1.8500 - val_mse: 0.8962\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6527 - mse: 0.6981 - val_loss: 1.8505 - val_mse: 0.8960\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6525 - mse: 0.6977 - val_loss: 1.8509 - val_mse: 0.8962\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6525 - mse: 0.6976 - val_loss: 1.8513 - val_mse: 0.8962\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6524 - mse: 0.6969 - val_loss: 1.8515 - val_mse: 0.8962\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6523 - mse: 0.6967 - val_loss: 1.8518 - val_mse: 0.8965\n",
      "Epoch 159/500\n",
      "142/160 [=========================>....] - ETA: 0s - loss: 1.6493 - mse: 0.6931Restoring model weights from the end of the best epoch: 154.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6523 - mse: 0.6962 - val_loss: 1.8522 - val_mse: 0.8964\n",
      "Epoch 159: early stopping\n",
      "k=15, lambda_=0.0002, val_mse=[12.053409576416016, 9.351776123046875, 5.405755996704102, 3.3236324787139893, 2.5424959659576416, 2.1615612506866455, 1.9321246147155762, 1.7804317474365234, 1.6716924905776978, 1.5909043550491333, 1.529667854309082, 1.4802218675613403, 1.4404926300048828, 1.4065444469451904, 1.377620816230774, 1.3540960550308228, 1.3302890062332153, 1.3120962381362915, 1.2943150997161865, 1.276890516281128, 1.2619060277938843, 1.2476271390914917, 1.2341912984848022, 1.220660924911499, 1.2085901498794556, 1.1947062015533447, 1.1843278408050537, 1.173231601715088, 1.1614800691604614, 1.1529324054718018, 1.1424534320831299, 1.13312566280365, 1.1231786012649536, 1.115208625793457, 1.1071207523345947, 1.0982937812805176, 1.09049391746521, 1.0828144550323486, 1.075114369392395, 1.0679965019226074, 1.0617154836654663, 1.0542292594909668, 1.048628807067871, 1.0431950092315674, 1.0369617938995361, 1.0315340757369995, 1.025455117225647, 1.020687222480774, 1.0150260925292969, 1.0100773572921753, 1.005232334136963, 1.000791311264038, 0.9975016713142395, 0.9924662709236145, 0.9891336560249329, 0.9858849048614502, 0.9815148115158081, 0.9782506227493286, 0.9742242693901062, 0.9711546897888184, 0.9676336646080017, 0.9649800658226013, 0.9616444110870361, 0.9594889879226685, 0.9572676420211792, 0.9537815451622009, 0.950835645198822, 0.9494419097900391, 0.9469970464706421, 0.9445506930351257, 0.942529022693634, 0.9404463768005371, 0.9389593601226807, 0.9361832141876221, 0.9345970749855042, 0.9331513047218323, 0.9312112927436829, 0.9296587705612183, 0.9280139207839966, 0.9271650910377502, 0.9260017275810242, 0.9239774942398071, 0.9225867390632629, 0.9212838411331177, 0.9204552173614502, 0.9194740653038025, 0.918735921382904, 0.9178725481033325, 0.9165290594100952, 0.9155118465423584, 0.9143349528312683, 0.9136557579040527, 0.9125279188156128, 0.9119496941566467, 0.9116335511207581, 0.9100828766822815, 0.9099639654159546, 0.9093688726425171, 0.9087551832199097, 0.9079734086990356, 0.9076377749443054, 0.9071839451789856, 0.906171977519989, 0.9058308005332947, 0.9054734706878662, 0.9050042033195496, 0.9046206474304199, 0.9041099548339844, 0.9037728905677795, 0.903259813785553, 0.9026743769645691, 0.9031212329864502, 0.902980387210846, 0.9021955728530884, 0.9017883539199829, 0.9015569686889648, 0.9010950922966003, 0.900953471660614, 0.9005690813064575, 0.9001770615577698, 0.9001869559288025, 0.9000434875488281, 0.8992727398872375, 0.8999153971672058, 0.8993340134620667, 0.8995003700256348, 0.8989235162734985, 0.8990312218666077, 0.8987288475036621, 0.898504376411438, 0.8980881571769714, 0.8979431986808777, 0.8979319930076599, 0.8983907699584961, 0.8981112241744995, 0.8979155421257019, 0.8979631066322327, 0.8976190090179443, 0.8971885442733765, 0.8976691961288452, 0.8971909284591675, 0.8971476554870605, 0.8973162174224854, 0.8969386219978333, 0.8965736031532288, 0.8968613743782043, 0.8967076539993286, 0.8965712189674377, 0.8965274095535278, 0.8965548276901245, 0.896615743637085, 0.8962632417678833, 0.8961953520774841, 0.8959901928901672, 0.8961747884750366, 0.8961957097053528, 0.8962090611457825, 0.8965492844581604, 0.8963875770568848]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 15 | 5e-05\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 12.7369 - mse: 12.7338 - val_loss: 12.0256 - val_mse: 12.0218\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.7771 - mse: 10.7609 - val_loss: 8.9188 - val_mse: 8.8791\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 6.6954 - mse: 6.6151 - val_loss: 4.9062 - val_mse: 4.7809\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.6541 - mse: 3.4863 - val_loss: 3.1026 - val_mse: 2.8956\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5038 - mse: 2.2668 - val_loss: 2.4713 - val_mse: 2.2073\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0370 - mse: 1.7519 - val_loss: 2.1662 - val_mse: 1.8612\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7836 - mse: 1.4619 - val_loss: 1.9885 - val_mse: 1.6510\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6276 - mse: 1.2764 - val_loss: 1.8742 - val_mse: 1.5104\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5245 - mse: 1.1494 - val_loss: 1.7979 - val_mse: 1.4120\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4529 - mse: 1.0574 - val_loss: 1.7447 - val_mse: 1.3403\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4013 - mse: 0.9891 - val_loss: 1.7050 - val_mse: 1.2850\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3631 - mse: 0.9367 - val_loss: 1.6770 - val_mse: 1.2444\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3336 - mse: 0.8954 - val_loss: 1.6531 - val_mse: 1.2095\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3107 - mse: 0.8625 - val_loss: 1.6357 - val_mse: 1.1833\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2918 - mse: 0.8354 - val_loss: 1.6206 - val_mse: 1.1608\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2769 - mse: 0.8140 - val_loss: 1.6079 - val_mse: 1.1418\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2642 - mse: 0.7957 - val_loss: 1.5973 - val_mse: 1.1266\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2532 - mse: 0.7804 - val_loss: 1.5862 - val_mse: 1.1114\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2435 - mse: 0.7672 - val_loss: 1.5765 - val_mse: 1.0987\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2349 - mse: 0.7559 - val_loss: 1.5683 - val_mse: 1.0883\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2271 - mse: 0.7461 - val_loss: 1.5607 - val_mse: 1.0792\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2199 - mse: 0.7378 - val_loss: 1.5540 - val_mse: 1.0713\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2129 - mse: 0.7301 - val_loss: 1.5465 - val_mse: 1.0633\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2064 - mse: 0.7232 - val_loss: 1.5387 - val_mse: 1.0554\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1997 - mse: 0.7166 - val_loss: 1.5313 - val_mse: 1.0486\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1934 - mse: 0.7111 - val_loss: 1.5240 - val_mse: 1.0420\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1873 - mse: 0.7056 - val_loss: 1.5170 - val_mse: 1.0358\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1809 - mse: 0.7003 - val_loss: 1.5097 - val_mse: 1.0294\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1748 - mse: 0.6954 - val_loss: 1.5029 - val_mse: 1.0240\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1682 - mse: 0.6901 - val_loss: 1.4960 - val_mse: 1.0188\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1616 - mse: 0.6850 - val_loss: 1.4894 - val_mse: 1.0136\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1550 - mse: 0.6800 - val_loss: 1.4831 - val_mse: 1.0092\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1480 - mse: 0.6750 - val_loss: 1.4773 - val_mse: 1.0049\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1412 - mse: 0.6697 - val_loss: 1.4710 - val_mse: 1.0004\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1340 - mse: 0.6645 - val_loss: 1.4644 - val_mse: 0.9954\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1265 - mse: 0.6586 - val_loss: 1.4584 - val_mse: 0.9915\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1191 - mse: 0.6528 - val_loss: 1.4523 - val_mse: 0.9870\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1116 - mse: 0.6474 - val_loss: 1.4467 - val_mse: 0.9831\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1039 - mse: 0.6413 - val_loss: 1.4415 - val_mse: 0.9797\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0958 - mse: 0.6350 - val_loss: 1.4353 - val_mse: 0.9753\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0879 - mse: 0.6284 - val_loss: 1.4306 - val_mse: 0.9723\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0797 - mse: 0.6222 - val_loss: 1.4258 - val_mse: 0.9687\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0717 - mse: 0.6154 - val_loss: 1.4208 - val_mse: 0.9654\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0633 - mse: 0.6087 - val_loss: 1.4163 - val_mse: 0.9621\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0548 - mse: 0.6015 - val_loss: 1.4121 - val_mse: 0.9591\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0464 - mse: 0.5940 - val_loss: 1.4080 - val_mse: 0.9563\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0379 - mse: 0.5870 - val_loss: 1.4040 - val_mse: 0.9532\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0296 - mse: 0.5795 - val_loss: 1.4001 - val_mse: 0.9507\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0210 - mse: 0.5720 - val_loss: 1.3976 - val_mse: 0.9486\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0126 - mse: 0.5643 - val_loss: 1.3930 - val_mse: 0.9451\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0042 - mse: 0.5569 - val_loss: 1.3894 - val_mse: 0.9422\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9961 - mse: 0.5493 - val_loss: 1.3871 - val_mse: 0.9407\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9879 - mse: 0.5418 - val_loss: 1.3837 - val_mse: 0.9378\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9800 - mse: 0.5344 - val_loss: 1.3810 - val_mse: 0.9356\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9723 - mse: 0.5273 - val_loss: 1.3779 - val_mse: 0.9329\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9645 - mse: 0.5197 - val_loss: 1.3767 - val_mse: 0.9321\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9571 - mse: 0.5127 - val_loss: 1.3731 - val_mse: 0.9287\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9497 - mse: 0.5054 - val_loss: 1.3715 - val_mse: 0.9274\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9425 - mse: 0.4989 - val_loss: 1.3696 - val_mse: 0.9257\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9357 - mse: 0.4920 - val_loss: 1.3670 - val_mse: 0.9234\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9288 - mse: 0.4854 - val_loss: 1.3660 - val_mse: 0.9225\n",
      "Epoch 62/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9222 - mse: 0.4790 - val_loss: 1.3632 - val_mse: 0.9197\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9160 - mse: 0.4726 - val_loss: 1.3621 - val_mse: 0.9188\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9097 - mse: 0.4666 - val_loss: 1.3600 - val_mse: 0.9166\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9036 - mse: 0.4603 - val_loss: 1.3583 - val_mse: 0.9148\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8976 - mse: 0.4543 - val_loss: 1.3575 - val_mse: 0.9140\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8920 - mse: 0.4487 - val_loss: 1.3565 - val_mse: 0.9131\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8866 - mse: 0.4434 - val_loss: 1.3542 - val_mse: 0.9106\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8812 - mse: 0.4378 - val_loss: 1.3538 - val_mse: 0.9101\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8760 - mse: 0.4325 - val_loss: 1.3544 - val_mse: 0.9106\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8710 - mse: 0.4273 - val_loss: 1.3528 - val_mse: 0.9089\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8660 - mse: 0.4222 - val_loss: 1.3516 - val_mse: 0.9079\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8614 - mse: 0.4178 - val_loss: 1.3513 - val_mse: 0.9071\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8567 - mse: 0.4126 - val_loss: 1.3512 - val_mse: 0.9070\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8524 - mse: 0.4083 - val_loss: 1.3512 - val_mse: 0.9068\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8482 - mse: 0.4038 - val_loss: 1.3508 - val_mse: 0.9064\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8442 - mse: 0.3996 - val_loss: 1.3507 - val_mse: 0.9061\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8401 - mse: 0.3956 - val_loss: 1.3502 - val_mse: 0.9054\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8362 - mse: 0.3915 - val_loss: 1.3495 - val_mse: 0.9046\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8325 - mse: 0.3876 - val_loss: 1.3500 - val_mse: 0.9049\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8289 - mse: 0.3839 - val_loss: 1.3496 - val_mse: 0.9043\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8254 - mse: 0.3804 - val_loss: 1.3493 - val_mse: 0.9037\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8221 - mse: 0.3768 - val_loss: 1.3496 - val_mse: 0.9039\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8189 - mse: 0.3733 - val_loss: 1.3489 - val_mse: 0.9032\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8157 - mse: 0.3702 - val_loss: 1.3493 - val_mse: 0.9035\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8128 - mse: 0.3670 - val_loss: 1.3495 - val_mse: 0.9034\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8099 - mse: 0.3640 - val_loss: 1.3497 - val_mse: 0.9037\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8071 - mse: 0.3612 - val_loss: 1.3491 - val_mse: 0.9026\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8043 - mse: 0.3580 - val_loss: 1.3503 - val_mse: 0.9038\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8016 - mse: 0.3553 - val_loss: 1.3492 - val_mse: 0.9027\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7991 - mse: 0.3528 - val_loss: 1.3498 - val_mse: 0.9031\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7966 - mse: 0.3502 - val_loss: 1.3501 - val_mse: 0.9034\n",
      "Epoch 93/500\n",
      "133/160 [=======================>......] - ETA: 0s - loss: 0.7900 - mse: 0.3434Restoring model weights from the end of the best epoch: 88.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7941 - mse: 0.3476 - val_loss: 1.3500 - val_mse: 0.9033\n",
      "Epoch 93: early stopping\n",
      "k=15, lambda_=5e-05, val_mse=[12.021756172180176, 8.879074096679688, 4.780898571014404, 2.895555257797241, 2.2072930335998535, 1.861220359802246, 1.6509904861450195, 1.5103579759597778, 1.411962628364563, 1.3402934074401855, 1.2850397825241089, 1.2444416284561157, 1.2095361948013306, 1.183282494544983, 1.1608246564865112, 1.1418185234069824, 1.1265630722045898, 1.1114414930343628, 1.0986506938934326, 1.0882818698883057, 1.0791621208190918, 1.071340560913086, 1.0633248090744019, 1.055395245552063, 1.0485708713531494, 1.041964054107666, 1.0358294248580933, 1.0293861627578735, 1.0240238904953003, 1.018761396408081, 1.0136380195617676, 1.0091922283172607, 1.0049259662628174, 1.0003905296325684, 0.9954135417938232, 0.9915294051170349, 0.9869909286499023, 0.9831154346466064, 0.9797057509422302, 0.9752986431121826, 0.9723097681999207, 0.9686955809593201, 0.9653922915458679, 0.9620745182037354, 0.9590756297111511, 0.9563201069831848, 0.9532423615455627, 0.9506765007972717, 0.9486179351806641, 0.9450994729995728, 0.9422277808189392, 0.9407052397727966, 0.9378209710121155, 0.9356412291526794, 0.9329298734664917, 0.932051420211792, 0.9287382960319519, 0.927375316619873, 0.9256715178489685, 0.9233981370925903, 0.9225339293479919, 0.9197343587875366, 0.9188365340232849, 0.9165936708450317, 0.9148446321487427, 0.9139624238014221, 0.9130578637123108, 0.9106419682502747, 0.910138726234436, 0.9105989933013916, 0.9088807702064514, 0.9078677296638489, 0.9071288704872131, 0.9070321321487427, 0.9068347811698914, 0.9064094424247742, 0.9061254262924194, 0.9054319262504578, 0.904615581035614, 0.9049102067947388, 0.9042777419090271, 0.9036628007888794, 0.9038738012313843, 0.903190553188324, 0.9034584760665894, 0.9034150242805481, 0.9036703109741211, 0.902556300163269, 0.9037755727767944, 0.9026980996131897, 0.9031333923339844, 0.9034117460250854, 0.9032588601112366]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 15 | 2e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 2ms/step - loss: 12.7454 - mse: 12.7439 - val_loss: 12.0422 - val_mse: 12.0405\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.9362 - mse: 10.9303 - val_loss: 9.2519 - val_mse: 9.2379\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 7.0356 - mse: 7.0062 - val_loss: 5.1097 - val_mse: 5.0623\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.6936 - mse: 3.6281 - val_loss: 3.0088 - val_mse: 2.9262\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3565 - mse: 2.2606 - val_loss: 2.2853 - val_mse: 2.1775\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8284 - mse: 1.7109 - val_loss: 1.9395 - val_mse: 1.8130\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5444 - mse: 1.4104 - val_loss: 1.7400 - val_mse: 1.5986\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3680 - mse: 1.2202 - val_loss: 1.6113 - val_mse: 1.4574\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2505 - mse: 1.0912 - val_loss: 1.5233 - val_mse: 1.3587\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1681 - mse: 0.9987 - val_loss: 1.4626 - val_mse: 1.2887\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1080 - mse: 0.9299 - val_loss: 1.4179 - val_mse: 1.2359\n",
      "Epoch 12/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0630 - mse: 0.8774 - val_loss: 1.3843 - val_mse: 1.1952\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0282 - mse: 0.8360 - val_loss: 1.3595 - val_mse: 1.1642\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0009 - mse: 0.8028 - val_loss: 1.3403 - val_mse: 1.1396\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9788 - mse: 0.7756 - val_loss: 1.3248 - val_mse: 1.1193\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9610 - mse: 0.7533 - val_loss: 1.3125 - val_mse: 1.1027\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9459 - mse: 0.7343 - val_loss: 1.3022 - val_mse: 1.0886\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9331 - mse: 0.7179 - val_loss: 1.2942 - val_mse: 1.0774\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9216 - mse: 0.7033 - val_loss: 1.2872 - val_mse: 1.0675\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9118 - mse: 0.6908 - val_loss: 1.2817 - val_mse: 1.0594\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9027 - mse: 0.6793 - val_loss: 1.2758 - val_mse: 1.0514\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8941 - mse: 0.6687 - val_loss: 1.2718 - val_mse: 1.0455\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8863 - mse: 0.6591 - val_loss: 1.2679 - val_mse: 1.0400\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8788 - mse: 0.6500 - val_loss: 1.2650 - val_mse: 1.0356\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8717 - mse: 0.6416 - val_loss: 1.2605 - val_mse: 1.0298\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8643 - mse: 0.6331 - val_loss: 1.2593 - val_mse: 1.0276\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8574 - mse: 0.6254 - val_loss: 1.2557 - val_mse: 1.0231\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8507 - mse: 0.6178 - val_loss: 1.2530 - val_mse: 1.0197\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8441 - mse: 0.6105 - val_loss: 1.2507 - val_mse: 1.0168\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8376 - mse: 0.6036 - val_loss: 1.2497 - val_mse: 1.0154\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8307 - mse: 0.5963 - val_loss: 1.2473 - val_mse: 1.0128\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8242 - mse: 0.5896 - val_loss: 1.2454 - val_mse: 1.0107\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8176 - mse: 0.5830 - val_loss: 1.2439 - val_mse: 1.0091\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8110 - mse: 0.5762 - val_loss: 1.2421 - val_mse: 1.0072\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8043 - mse: 0.5695 - val_loss: 1.2402 - val_mse: 1.0053\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7975 - mse: 0.5629 - val_loss: 1.2382 - val_mse: 1.0035\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7908 - mse: 0.5562 - val_loss: 1.2370 - val_mse: 1.0024\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7840 - mse: 0.5495 - val_loss: 1.2362 - val_mse: 1.0018\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7770 - mse: 0.5428 - val_loss: 1.2342 - val_mse: 1.0001\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7700 - mse: 0.5360 - val_loss: 1.2325 - val_mse: 0.9986\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7632 - mse: 0.5295 - val_loss: 1.2319 - val_mse: 0.9982\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7559 - mse: 0.5223 - val_loss: 1.2302 - val_mse: 0.9968\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7488 - mse: 0.5156 - val_loss: 1.2301 - val_mse: 0.9969\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7417 - mse: 0.5087 - val_loss: 1.2292 - val_mse: 0.9962\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7345 - mse: 0.5017 - val_loss: 1.2289 - val_mse: 0.9962\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7273 - mse: 0.4948 - val_loss: 1.2277 - val_mse: 0.9951\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7202 - mse: 0.4878 - val_loss: 1.2270 - val_mse: 0.9945\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7132 - mse: 0.4809 - val_loss: 1.2269 - val_mse: 0.9946\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7062 - mse: 0.4742 - val_loss: 1.2271 - val_mse: 0.9950\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6989 - mse: 0.4670 - val_loss: 1.2266 - val_mse: 0.9946\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6919 - mse: 0.4600 - val_loss: 1.2269 - val_mse: 0.9949\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6852 - mse: 0.4534 - val_loss: 1.2258 - val_mse: 0.9939\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6783 - mse: 0.4466 - val_loss: 1.2262 - val_mse: 0.9942\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6715 - mse: 0.4397 - val_loss: 1.2261 - val_mse: 0.9941\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6646 - mse: 0.4329 - val_loss: 1.2265 - val_mse: 0.9946\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6581 - mse: 0.4262 - val_loss: 1.2262 - val_mse: 0.9942\n",
      "Epoch 57/500\n",
      "145/160 [==========================>...] - ETA: 0s - loss: 0.6500 - mse: 0.4182Restoring model weights from the end of the best epoch: 52.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6517 - mse: 0.4199 - val_loss: 1.2264 - val_mse: 0.9944\n",
      "Epoch 57: early stopping\n",
      "k=15, lambda_=2e-05, val_mse=[12.040477752685547, 9.237908363342285, 5.062347412109375, 2.926156997680664, 2.177452564239502, 1.8130321502685547, 1.5985803604125977, 1.4573911428451538, 1.358669638633728, 1.288670301437378, 1.2359498739242554, 1.1952056884765625, 1.1642001867294312, 1.139564871788025, 1.11930513381958, 1.1026852130889893, 1.088649868965149, 1.077404260635376, 1.0675464868545532, 1.0593856573104858, 1.0513584613800049, 1.0455377101898193, 1.03995680809021, 1.0355886220932007, 1.0298078060150146, 1.0275824069976807, 1.023144006729126, 1.019675374031067, 1.016801118850708, 1.0154212713241577, 1.012783408164978, 1.0107275247573853, 1.0090855360031128, 1.007192850112915, 1.0052790641784668, 1.0035152435302734, 1.0023850202560425, 1.0017861127853394, 1.0000522136688232, 0.9985777735710144, 0.9981634020805359, 0.9968340396881104, 0.9968949556350708, 0.9961935877799988, 0.9961550831794739, 0.9950520992279053, 0.9944979548454285, 0.9946109652519226, 0.994999885559082, 0.994608461856842, 0.994890034198761, 0.9938891530036926, 0.9942355155944824, 0.9941447973251343, 0.9945822954177856, 0.9941973090171814, 0.994350790977478]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 30 | 0.0002\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7529 - mse: 12.7363 - val_loss: 12.0109 - val_mse: 11.9939\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.4394 - mse: 10.3514 - val_loss: 8.1469 - val_mse: 7.9249\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.8496 - mse: 5.4128 - val_loss: 4.4134 - val_mse: 3.7608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.6425 - mse: 2.8344 - val_loss: 3.4946 - val_mse: 2.5593\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 3.0721 - mse: 2.0514 - val_loss: 3.1992 - val_mse: 2.1053\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8388 - mse: 1.6908 - val_loss: 3.0573 - val_mse: 1.8618\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.7134 - mse: 1.4817 - val_loss: 2.9738 - val_mse: 1.7113\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6344 - mse: 1.3480 - val_loss: 2.9215 - val_mse: 1.6140\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5782 - mse: 1.2556 - val_loss: 2.8782 - val_mse: 1.5434\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.5344 - mse: 1.1904 - val_loss: 2.8444 - val_mse: 1.4928\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4976 - mse: 1.1402 - val_loss: 2.8127 - val_mse: 1.4539\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4654 - mse: 1.1047 - val_loss: 2.7841 - val_mse: 1.4221\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4361 - mse: 1.0752 - val_loss: 2.7553 - val_mse: 1.3961\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.4090 - mse: 1.0514 - val_loss: 2.7286 - val_mse: 1.3751\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3831 - mse: 1.0330 - val_loss: 2.7015 - val_mse: 1.3552\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3580 - mse: 1.0162 - val_loss: 2.6769 - val_mse: 1.3404\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3334 - mse: 1.0015 - val_loss: 2.6500 - val_mse: 1.3245\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.3097 - mse: 0.9900 - val_loss: 2.6221 - val_mse: 1.3079\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2866 - mse: 0.9783 - val_loss: 2.5953 - val_mse: 1.2934\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2640 - mse: 0.9686 - val_loss: 2.5700 - val_mse: 1.2803\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2415 - mse: 0.9587 - val_loss: 2.5426 - val_mse: 1.2666\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.2196 - mse: 0.9493 - val_loss: 2.5161 - val_mse: 1.2538\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1978 - mse: 0.9412 - val_loss: 2.4915 - val_mse: 1.2419\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1769 - mse: 0.9337 - val_loss: 2.4661 - val_mse: 1.2298\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1563 - mse: 0.9265 - val_loss: 2.4392 - val_mse: 1.2156\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1357 - mse: 0.9189 - val_loss: 2.4145 - val_mse: 1.2030\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.1161 - mse: 0.9120 - val_loss: 2.3911 - val_mse: 1.1926\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0963 - mse: 0.9043 - val_loss: 2.3664 - val_mse: 1.1808\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0778 - mse: 0.8979 - val_loss: 2.3435 - val_mse: 1.1699\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0596 - mse: 0.8923 - val_loss: 2.3213 - val_mse: 1.1601\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0417 - mse: 0.8859 - val_loss: 2.2970 - val_mse: 1.1480\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0243 - mse: 0.8812 - val_loss: 2.2763 - val_mse: 1.1375\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0074 - mse: 0.8735 - val_loss: 2.2553 - val_mse: 1.1275\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9914 - mse: 0.8684 - val_loss: 2.2339 - val_mse: 1.1179\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9756 - mse: 0.8642 - val_loss: 2.2153 - val_mse: 1.1085\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9608 - mse: 0.8589 - val_loss: 2.1949 - val_mse: 1.0972\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9460 - mse: 0.8526 - val_loss: 2.1780 - val_mse: 1.0911\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9320 - mse: 0.8500 - val_loss: 2.1611 - val_mse: 1.0823\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9183 - mse: 0.8440 - val_loss: 2.1441 - val_mse: 1.0731\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.9056 - mse: 0.8394 - val_loss: 2.1268 - val_mse: 1.0645\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8930 - mse: 0.8350 - val_loss: 2.1114 - val_mse: 1.0568\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8813 - mse: 0.8310 - val_loss: 2.0969 - val_mse: 1.0511\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8697 - mse: 0.8271 - val_loss: 2.0827 - val_mse: 1.0430\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8587 - mse: 0.8219 - val_loss: 2.0684 - val_mse: 1.0356\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8484 - mse: 0.8192 - val_loss: 2.0559 - val_mse: 1.0301\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8386 - mse: 0.8159 - val_loss: 2.0446 - val_mse: 1.0246\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8291 - mse: 0.8120 - val_loss: 2.0320 - val_mse: 1.0179\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8202 - mse: 0.8084 - val_loss: 2.0217 - val_mse: 1.0128\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8114 - mse: 0.8050 - val_loss: 2.0101 - val_mse: 1.0063\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8031 - mse: 0.8017 - val_loss: 2.0002 - val_mse: 1.0017\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7954 - mse: 0.7982 - val_loss: 1.9914 - val_mse: 0.9971\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7879 - mse: 0.7954 - val_loss: 1.9826 - val_mse: 0.9930\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7811 - mse: 0.7927 - val_loss: 1.9734 - val_mse: 0.9882\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7742 - mse: 0.7899 - val_loss: 1.9660 - val_mse: 0.9833\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7678 - mse: 0.7865 - val_loss: 1.9590 - val_mse: 0.9801\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7619 - mse: 0.7845 - val_loss: 1.9518 - val_mse: 0.9757\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7560 - mse: 0.7816 - val_loss: 1.9444 - val_mse: 0.9711\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7506 - mse: 0.7790 - val_loss: 1.9396 - val_mse: 0.9695\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7457 - mse: 0.7761 - val_loss: 1.9330 - val_mse: 0.9652\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7407 - mse: 0.7741 - val_loss: 1.9274 - val_mse: 0.9617\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7360 - mse: 0.7712 - val_loss: 1.9226 - val_mse: 0.9589\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7316 - mse: 0.7686 - val_loss: 1.9173 - val_mse: 0.9553\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7275 - mse: 0.7661 - val_loss: 1.9133 - val_mse: 0.9533\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7236 - mse: 0.7642 - val_loss: 1.9090 - val_mse: 0.9509\n",
      "Epoch 65/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7198 - mse: 0.7618 - val_loss: 1.9052 - val_mse: 0.9481\n",
      "Epoch 66/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7163 - mse: 0.7600 - val_loss: 1.9006 - val_mse: 0.9448\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7130 - mse: 0.7575 - val_loss: 1.8976 - val_mse: 0.9426\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7098 - mse: 0.7549 - val_loss: 1.8955 - val_mse: 0.9415\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7067 - mse: 0.7531 - val_loss: 1.8916 - val_mse: 0.9387\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7038 - mse: 0.7506 - val_loss: 1.8889 - val_mse: 0.9362\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7011 - mse: 0.7488 - val_loss: 1.8853 - val_mse: 0.9336\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6986 - mse: 0.7472 - val_loss: 1.8832 - val_mse: 0.9325\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6961 - mse: 0.7445 - val_loss: 1.8813 - val_mse: 0.9311\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6938 - mse: 0.7422 - val_loss: 1.8793 - val_mse: 0.9295\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6917 - mse: 0.7413 - val_loss: 1.8775 - val_mse: 0.9273\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6898 - mse: 0.7394 - val_loss: 1.8757 - val_mse: 0.9257\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6876 - mse: 0.7375 - val_loss: 1.8745 - val_mse: 0.9240\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6858 - mse: 0.7353 - val_loss: 1.8728 - val_mse: 0.9227\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6840 - mse: 0.7334 - val_loss: 1.8711 - val_mse: 0.9210\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6824 - mse: 0.7314 - val_loss: 1.8703 - val_mse: 0.9201\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6808 - mse: 0.7300 - val_loss: 1.8694 - val_mse: 0.9188\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6792 - mse: 0.7278 - val_loss: 1.8683 - val_mse: 0.9177\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6778 - mse: 0.7270 - val_loss: 1.8679 - val_mse: 0.9167\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6764 - mse: 0.7254 - val_loss: 1.8668 - val_mse: 0.9151\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6752 - mse: 0.7227 - val_loss: 1.8663 - val_mse: 0.9146\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6740 - mse: 0.7217 - val_loss: 1.8653 - val_mse: 0.9133\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6730 - mse: 0.7205 - val_loss: 1.8648 - val_mse: 0.9122\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6718 - mse: 0.7185 - val_loss: 1.8644 - val_mse: 0.9116\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6708 - mse: 0.7170 - val_loss: 1.8639 - val_mse: 0.9107\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6698 - mse: 0.7160 - val_loss: 1.8634 - val_mse: 0.9091\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6690 - mse: 0.7135 - val_loss: 1.8631 - val_mse: 0.9086\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6681 - mse: 0.7132 - val_loss: 1.8633 - val_mse: 0.9082\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6672 - mse: 0.7115 - val_loss: 1.8627 - val_mse: 0.9073\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6665 - mse: 0.7105 - val_loss: 1.8627 - val_mse: 0.9067\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6658 - mse: 0.7091 - val_loss: 1.8626 - val_mse: 0.9061\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6650 - mse: 0.7077 - val_loss: 1.8623 - val_mse: 0.9049\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6645 - mse: 0.7067 - val_loss: 1.8622 - val_mse: 0.9041\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6639 - mse: 0.7059 - val_loss: 1.8624 - val_mse: 0.9037\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6631 - mse: 0.7035 - val_loss: 1.8624 - val_mse: 0.9034\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6627 - mse: 0.7035 - val_loss: 1.8623 - val_mse: 0.9028\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6623 - mse: 0.7022 - val_loss: 1.8625 - val_mse: 0.9027\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6617 - mse: 0.7008 - val_loss: 1.8623 - val_mse: 0.9022\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6611 - mse: 0.7003 - val_loss: 1.8623 - val_mse: 0.9011\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6608 - mse: 0.6992 - val_loss: 1.8626 - val_mse: 0.9017\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6604 - mse: 0.6986 - val_loss: 1.8623 - val_mse: 0.9006\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6599 - mse: 0.6971 - val_loss: 1.8628 - val_mse: 0.9005\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6596 - mse: 0.6966 - val_loss: 1.8628 - val_mse: 0.9000\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6593 - mse: 0.6958 - val_loss: 1.8625 - val_mse: 0.8996\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6589 - mse: 0.6947 - val_loss: 1.8630 - val_mse: 0.8996\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6585 - mse: 0.6941 - val_loss: 1.8632 - val_mse: 0.8989\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6583 - mse: 0.6936 - val_loss: 1.8633 - val_mse: 0.8985\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6580 - mse: 0.6928 - val_loss: 1.8633 - val_mse: 0.8979\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6577 - mse: 0.6915 - val_loss: 1.8632 - val_mse: 0.8973\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6574 - mse: 0.6907 - val_loss: 1.8635 - val_mse: 0.8975\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6572 - mse: 0.6904 - val_loss: 1.8637 - val_mse: 0.8973\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6571 - mse: 0.6902 - val_loss: 1.8640 - val_mse: 0.8969\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6567 - mse: 0.6886 - val_loss: 1.8643 - val_mse: 0.8968\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6565 - mse: 0.6888 - val_loss: 1.8644 - val_mse: 0.8968\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6564 - mse: 0.6879 - val_loss: 1.8641 - val_mse: 0.8959\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6561 - mse: 0.6874 - val_loss: 1.8645 - val_mse: 0.8962\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6561 - mse: 0.6874 - val_loss: 1.8645 - val_mse: 0.8958\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6559 - mse: 0.6862 - val_loss: 1.8651 - val_mse: 0.8960\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6556 - mse: 0.6854 - val_loss: 1.8651 - val_mse: 0.8956\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6556 - mse: 0.6859 - val_loss: 1.8662 - val_mse: 0.8964\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6553 - mse: 0.6851 - val_loss: 1.8656 - val_mse: 0.8950\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6553 - mse: 0.6840 - val_loss: 1.8659 - val_mse: 0.8951\n",
      "Epoch 127/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6552 - mse: 0.6842 - val_loss: 1.8658 - val_mse: 0.8947\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6550 - mse: 0.6835 - val_loss: 1.8658 - val_mse: 0.8945\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6550 - mse: 0.6830 - val_loss: 1.8665 - val_mse: 0.8946\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6548 - mse: 0.6826 - val_loss: 1.8665 - val_mse: 0.8947\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6546 - mse: 0.6825 - val_loss: 1.8661 - val_mse: 0.8940\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6546 - mse: 0.6812 - val_loss: 1.8665 - val_mse: 0.8940\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6545 - mse: 0.6814 - val_loss: 1.8669 - val_mse: 0.8943\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6544 - mse: 0.6814 - val_loss: 1.8673 - val_mse: 0.8941\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6543 - mse: 0.6807 - val_loss: 1.8673 - val_mse: 0.8937\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6543 - mse: 0.6805 - val_loss: 1.8674 - val_mse: 0.8936\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6541 - mse: 0.6798 - val_loss: 1.8676 - val_mse: 0.8938\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6541 - mse: 0.6796 - val_loss: 1.8679 - val_mse: 0.8937\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6540 - mse: 0.6792 - val_loss: 1.8683 - val_mse: 0.8937\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6540 - mse: 0.6784 - val_loss: 1.8683 - val_mse: 0.8939\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6538 - mse: 0.6787 - val_loss: 1.8681 - val_mse: 0.8933\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6537 - mse: 0.6779 - val_loss: 1.8683 - val_mse: 0.8931\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6537 - mse: 0.6784 - val_loss: 1.8686 - val_mse: 0.8933\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6537 - mse: 0.6776 - val_loss: 1.8686 - val_mse: 0.8930\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6536 - mse: 0.6774 - val_loss: 1.8686 - val_mse: 0.8929\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6535 - mse: 0.6775 - val_loss: 1.8688 - val_mse: 0.8928\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6535 - mse: 0.6772 - val_loss: 1.8690 - val_mse: 0.8929\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6534 - mse: 0.6768 - val_loss: 1.8691 - val_mse: 0.8924\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6533 - mse: 0.6757 - val_loss: 1.8692 - val_mse: 0.8925\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6534 - mse: 0.6766 - val_loss: 1.8692 - val_mse: 0.8921\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6533 - mse: 0.6757 - val_loss: 1.8694 - val_mse: 0.8922\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6532 - mse: 0.6753 - val_loss: 1.8697 - val_mse: 0.8924\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6533 - mse: 0.6754 - val_loss: 1.8704 - val_mse: 0.8930\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6532 - mse: 0.6751 - val_loss: 1.8701 - val_mse: 0.8927\n",
      "Epoch 155/500\n",
      "138/160 [========================>.....] - ETA: 0s - loss: 1.6513 - mse: 0.6733Restoring model weights from the end of the best epoch: 150.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.6532 - mse: 0.6751 - val_loss: 1.8700 - val_mse: 0.8922\n",
      "Epoch 155: early stopping\n",
      "k=30, lambda_=0.0002, val_mse=[11.993925094604492, 7.924865245819092, 3.760822296142578, 2.5593340396881104, 2.1052844524383545, 1.8617656230926514, 1.7113486528396606, 1.6140071153640747, 1.543419361114502, 1.4927880764007568, 1.4538733959197998, 1.4221460819244385, 1.3961400985717773, 1.3750720024108887, 1.3551924228668213, 1.3403699398040771, 1.3244919776916504, 1.3078813552856445, 1.2933861017227173, 1.2803115844726562, 1.266610860824585, 1.2537879943847656, 1.2419447898864746, 1.2297933101654053, 1.215643048286438, 1.2030161619186401, 1.192566156387329, 1.1808421611785889, 1.1698672771453857, 1.1601004600524902, 1.147996425628662, 1.1375268697738647, 1.127539038658142, 1.1179091930389404, 1.1085187196731567, 1.0972294807434082, 1.0910911560058594, 1.0822787284851074, 1.073146104812622, 1.0645180940628052, 1.0567820072174072, 1.0510809421539307, 1.0429842472076416, 1.0356066226959229, 1.0301278829574585, 1.0245836973190308, 1.0178930759429932, 1.0127785205841064, 1.0063432455062866, 1.0017163753509521, 0.9970733523368835, 0.992996335029602, 0.9881975650787354, 0.9833269715309143, 0.9801332354545593, 0.9756670594215393, 0.9710763692855835, 0.9694520831108093, 0.9651728868484497, 0.9616706967353821, 0.9589174389839172, 0.955329179763794, 0.9532674551010132, 0.9508751034736633, 0.9481184482574463, 0.944824755191803, 0.9426010251045227, 0.9415313005447388, 0.9387015700340271, 0.9362176656723022, 0.9336140751838684, 0.9324898719787598, 0.931090235710144, 0.9294939041137695, 0.9273059368133545, 0.9256958365440369, 0.9240435361862183, 0.9227142930030823, 0.9210172295570374, 0.9201433062553406, 0.9188221096992493, 0.9177221059799194, 0.9167269468307495, 0.9151319265365601, 0.9145786762237549, 0.9133024215698242, 0.9122134447097778, 0.9115698933601379, 0.9107192158699036, 0.9091182351112366, 0.9086388945579529, 0.9081937670707703, 0.9072986245155334, 0.9067107439041138, 0.9061389565467834, 0.90486079454422, 0.904089093208313, 0.9036648273468018, 0.9033610820770264, 0.9028400778770447, 0.9027068018913269, 0.9022220373153687, 0.9011188745498657, 0.9017193913459778, 0.9006252884864807, 0.900532603263855, 0.899972140789032, 0.8995846509933472, 0.8995659351348877, 0.8989176154136658, 0.8985219597816467, 0.8979167938232422, 0.8972615003585815, 0.8974995017051697, 0.897284746170044, 0.8969354033470154, 0.89683997631073, 0.8968349099159241, 0.8959078192710876, 0.896155834197998, 0.8958261013031006, 0.8959991335868835, 0.8955755233764648, 0.8963776230812073, 0.8949974775314331, 0.8950738906860352, 0.8947351574897766, 0.8945041298866272, 0.8946447968482971, 0.8946844935417175, 0.8940021991729736, 0.8940134048461914, 0.8942826390266418, 0.8940691947937012, 0.8937442898750305, 0.8936466574668884, 0.8937657475471497, 0.893737256526947, 0.8936803340911865, 0.8938938975334167, 0.8932600021362305, 0.893130362033844, 0.8932561874389648, 0.8929873108863831, 0.8928847312927246, 0.8928027749061584, 0.8928911089897156, 0.8923731446266174, 0.8925251364707947, 0.8920504450798035, 0.8921641707420349, 0.8923777937889099, 0.8929685354232788, 0.8927268981933594, 0.8921857476234436]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 30 | 5e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7213 - mse: 12.7156 - val_loss: 11.9891 - val_mse: 11.9828\n",
      "Epoch 2/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 10.4716 - mse: 10.4478 - val_loss: 8.1293 - val_mse: 8.0713\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.4956 - mse: 5.3776 - val_loss: 3.7571 - val_mse: 3.5761\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.8069 - mse: 2.5766 - val_loss: 2.5721 - val_mse: 2.2998\n",
      "Epoch 5/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 2.0592 - mse: 1.7561 - val_loss: 2.1660 - val_mse: 1.8353\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7368 - mse: 1.3831 - val_loss: 1.9664 - val_mse: 1.5920\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.5609 - mse: 1.1690 - val_loss: 1.8580 - val_mse: 1.4498\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4524 - mse: 1.0306 - val_loss: 1.7926 - val_mse: 1.3576\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3805 - mse: 0.9345 - val_loss: 1.7518 - val_mse: 1.2956\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.3294 - mse: 0.8645 - val_loss: 1.7258 - val_mse: 1.2528\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2912 - mse: 0.8117 - val_loss: 1.7073 - val_mse: 1.2210\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2614 - mse: 0.7696 - val_loss: 1.6928 - val_mse: 1.1960\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2376 - mse: 0.7364 - val_loss: 1.6860 - val_mse: 1.1811\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2179 - mse: 0.7098 - val_loss: 1.6781 - val_mse: 1.1665\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2008 - mse: 0.6868 - val_loss: 1.6720 - val_mse: 1.1557\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1863 - mse: 0.6682 - val_loss: 1.6680 - val_mse: 1.1481\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1726 - mse: 0.6517 - val_loss: 1.6641 - val_mse: 1.1419\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1604 - mse: 0.6375 - val_loss: 1.6613 - val_mse: 1.1376\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1491 - mse: 0.6250 - val_loss: 1.6570 - val_mse: 1.1325\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1381 - mse: 0.6132 - val_loss: 1.6539 - val_mse: 1.1288\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1274 - mse: 0.6026 - val_loss: 1.6488 - val_mse: 1.1240\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1173 - mse: 0.5928 - val_loss: 1.6475 - val_mse: 1.1232\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1075 - mse: 0.5838 - val_loss: 1.6430 - val_mse: 1.1194\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0978 - mse: 0.5745 - val_loss: 1.6374 - val_mse: 1.1144\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0880 - mse: 0.5659 - val_loss: 1.6355 - val_mse: 1.1136\n",
      "Epoch 26/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0786 - mse: 0.5573 - val_loss: 1.6317 - val_mse: 1.1107\n",
      "Epoch 27/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0691 - mse: 0.5488 - val_loss: 1.6272 - val_mse: 1.1076\n",
      "Epoch 28/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0599 - mse: 0.5409 - val_loss: 1.6228 - val_mse: 1.1042\n",
      "Epoch 29/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0506 - mse: 0.5332 - val_loss: 1.6196 - val_mse: 1.1022\n",
      "Epoch 30/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0413 - mse: 0.5249 - val_loss: 1.6141 - val_mse: 1.0975\n",
      "Epoch 31/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0320 - mse: 0.5166 - val_loss: 1.6089 - val_mse: 1.0938\n",
      "Epoch 32/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0228 - mse: 0.5084 - val_loss: 1.6049 - val_mse: 1.0905\n",
      "Epoch 33/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0140 - mse: 0.5008 - val_loss: 1.5998 - val_mse: 1.0867\n",
      "Epoch 34/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0049 - mse: 0.4925 - val_loss: 1.5956 - val_mse: 1.0835\n",
      "Epoch 35/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9959 - mse: 0.4845 - val_loss: 1.5915 - val_mse: 1.0803\n",
      "Epoch 36/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9871 - mse: 0.4764 - val_loss: 1.5866 - val_mse: 1.0762\n",
      "Epoch 37/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9782 - mse: 0.4683 - val_loss: 1.5825 - val_mse: 1.0728\n",
      "Epoch 38/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9698 - mse: 0.4610 - val_loss: 1.5771 - val_mse: 1.0682\n",
      "Epoch 39/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9612 - mse: 0.4529 - val_loss: 1.5728 - val_mse: 1.0645\n",
      "Epoch 40/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9526 - mse: 0.4450 - val_loss: 1.5686 - val_mse: 1.0606\n",
      "Epoch 41/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9443 - mse: 0.4370 - val_loss: 1.5624 - val_mse: 1.0552\n",
      "Epoch 42/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9359 - mse: 0.4291 - val_loss: 1.5595 - val_mse: 1.0527\n",
      "Epoch 43/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9280 - mse: 0.4216 - val_loss: 1.5543 - val_mse: 1.0479\n",
      "Epoch 44/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9199 - mse: 0.4141 - val_loss: 1.5495 - val_mse: 1.0433\n",
      "Epoch 45/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9120 - mse: 0.4066 - val_loss: 1.5465 - val_mse: 1.0405\n",
      "Epoch 46/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9042 - mse: 0.3987 - val_loss: 1.5423 - val_mse: 1.0367\n",
      "Epoch 47/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8966 - mse: 0.3915 - val_loss: 1.5369 - val_mse: 1.0314\n",
      "Epoch 48/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8892 - mse: 0.3841 - val_loss: 1.5361 - val_mse: 1.0308\n",
      "Epoch 49/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8817 - mse: 0.3769 - val_loss: 1.5301 - val_mse: 1.0249\n",
      "Epoch 50/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8747 - mse: 0.3699 - val_loss: 1.5266 - val_mse: 1.0216\n",
      "Epoch 51/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8675 - mse: 0.3629 - val_loss: 1.5241 - val_mse: 1.0192\n",
      "Epoch 52/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8606 - mse: 0.3560 - val_loss: 1.5197 - val_mse: 1.0144\n",
      "Epoch 53/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8540 - mse: 0.3494 - val_loss: 1.5186 - val_mse: 1.0137\n",
      "Epoch 54/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8475 - mse: 0.3427 - val_loss: 1.5154 - val_mse: 1.0103\n",
      "Epoch 55/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8411 - mse: 0.3365 - val_loss: 1.5105 - val_mse: 1.0053\n",
      "Epoch 56/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8350 - mse: 0.3303 - val_loss: 1.5079 - val_mse: 1.0029\n",
      "Epoch 57/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8290 - mse: 0.3240 - val_loss: 1.5054 - val_mse: 1.0002\n",
      "Epoch 58/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8231 - mse: 0.3184 - val_loss: 1.5033 - val_mse: 0.9980\n",
      "Epoch 59/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8172 - mse: 0.3122 - val_loss: 1.4999 - val_mse: 0.9944\n",
      "Epoch 60/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8118 - mse: 0.3068 - val_loss: 1.4971 - val_mse: 0.9915\n",
      "Epoch 61/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8064 - mse: 0.3012 - val_loss: 1.4966 - val_mse: 0.9914\n",
      "Epoch 62/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8012 - mse: 0.2961 - val_loss: 1.4942 - val_mse: 0.9886\n",
      "Epoch 63/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7962 - mse: 0.2911 - val_loss: 1.4917 - val_mse: 0.9859\n",
      "Epoch 64/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7911 - mse: 0.2857 - val_loss: 1.4897 - val_mse: 0.9838\n",
      "Epoch 65/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7865 - mse: 0.2813 - val_loss: 1.4878 - val_mse: 0.9820\n",
      "Epoch 66/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7817 - mse: 0.2762 - val_loss: 1.4857 - val_mse: 0.9797\n",
      "Epoch 67/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7774 - mse: 0.2720 - val_loss: 1.4846 - val_mse: 0.9787\n",
      "Epoch 68/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7730 - mse: 0.2675 - val_loss: 1.4820 - val_mse: 0.9760\n",
      "Epoch 69/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7688 - mse: 0.2633 - val_loss: 1.4809 - val_mse: 0.9752\n",
      "Epoch 70/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7650 - mse: 0.2596 - val_loss: 1.4793 - val_mse: 0.9732\n",
      "Epoch 71/500\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.7608 - mse: 0.2551 - val_loss: 1.4775 - val_mse: 0.9717\n",
      "Epoch 72/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7572 - mse: 0.2519 - val_loss: 1.4765 - val_mse: 0.9707\n",
      "Epoch 73/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7534 - mse: 0.2480 - val_loss: 1.4742 - val_mse: 0.9685\n",
      "Epoch 74/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7501 - mse: 0.2447 - val_loss: 1.4725 - val_mse: 0.9666\n",
      "Epoch 75/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7466 - mse: 0.2412 - val_loss: 1.4706 - val_mse: 0.9648\n",
      "Epoch 76/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7432 - mse: 0.2380 - val_loss: 1.4701 - val_mse: 0.9645\n",
      "Epoch 77/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7402 - mse: 0.2350 - val_loss: 1.4688 - val_mse: 0.9633\n",
      "Epoch 78/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7370 - mse: 0.2321 - val_loss: 1.4675 - val_mse: 0.9619\n",
      "Epoch 79/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7340 - mse: 0.2290 - val_loss: 1.4655 - val_mse: 0.9603\n",
      "Epoch 80/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7311 - mse: 0.2263 - val_loss: 1.4644 - val_mse: 0.9594\n",
      "Epoch 81/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7283 - mse: 0.2238 - val_loss: 1.4637 - val_mse: 0.9587\n",
      "Epoch 82/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7256 - mse: 0.2211 - val_loss: 1.4627 - val_mse: 0.9579\n",
      "Epoch 83/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7231 - mse: 0.2189 - val_loss: 1.4609 - val_mse: 0.9563\n",
      "Epoch 84/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7206 - mse: 0.2166 - val_loss: 1.4604 - val_mse: 0.9561\n",
      "Epoch 85/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7181 - mse: 0.2143 - val_loss: 1.4581 - val_mse: 0.9540\n",
      "Epoch 86/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7157 - mse: 0.2122 - val_loss: 1.4569 - val_mse: 0.9532\n",
      "Epoch 87/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7134 - mse: 0.2103 - val_loss: 1.4560 - val_mse: 0.9522\n",
      "Epoch 88/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7112 - mse: 0.2083 - val_loss: 1.4544 - val_mse: 0.9509\n",
      "Epoch 89/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7090 - mse: 0.2060 - val_loss: 1.4539 - val_mse: 0.9508\n",
      "Epoch 90/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7070 - mse: 0.2046 - val_loss: 1.4524 - val_mse: 0.9495\n",
      "Epoch 91/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7049 - mse: 0.2027 - val_loss: 1.4515 - val_mse: 0.9488\n",
      "Epoch 92/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7030 - mse: 0.2011 - val_loss: 1.4508 - val_mse: 0.9485\n",
      "Epoch 93/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7011 - mse: 0.1994 - val_loss: 1.4500 - val_mse: 0.9481\n",
      "Epoch 94/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6992 - mse: 0.1979 - val_loss: 1.4478 - val_mse: 0.9462\n",
      "Epoch 95/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6975 - mse: 0.1966 - val_loss: 1.4467 - val_mse: 0.9452\n",
      "Epoch 96/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6957 - mse: 0.1949 - val_loss: 1.4455 - val_mse: 0.9444\n",
      "Epoch 97/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6940 - mse: 0.1935 - val_loss: 1.4448 - val_mse: 0.9440\n",
      "Epoch 98/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6925 - mse: 0.1923 - val_loss: 1.4435 - val_mse: 0.9430\n",
      "Epoch 99/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6908 - mse: 0.1910 - val_loss: 1.4425 - val_mse: 0.9424\n",
      "Epoch 100/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6893 - mse: 0.1896 - val_loss: 1.4418 - val_mse: 0.9419\n",
      "Epoch 101/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6878 - mse: 0.1886 - val_loss: 1.4398 - val_mse: 0.9403\n",
      "Epoch 102/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6864 - mse: 0.1876 - val_loss: 1.4393 - val_mse: 0.9401\n",
      "Epoch 103/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6849 - mse: 0.1862 - val_loss: 1.4388 - val_mse: 0.9398\n",
      "Epoch 104/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6836 - mse: 0.1853 - val_loss: 1.4372 - val_mse: 0.9385\n",
      "Epoch 105/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6823 - mse: 0.1843 - val_loss: 1.4360 - val_mse: 0.9377\n",
      "Epoch 106/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6809 - mse: 0.1832 - val_loss: 1.4344 - val_mse: 0.9365\n",
      "Epoch 107/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6797 - mse: 0.1824 - val_loss: 1.4344 - val_mse: 0.9366\n",
      "Epoch 108/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6784 - mse: 0.1814 - val_loss: 1.4331 - val_mse: 0.9358\n",
      "Epoch 109/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6773 - mse: 0.1807 - val_loss: 1.4315 - val_mse: 0.9345\n",
      "Epoch 110/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6761 - mse: 0.1795 - val_loss: 1.4305 - val_mse: 0.9337\n",
      "Epoch 111/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6750 - mse: 0.1788 - val_loss: 1.4308 - val_mse: 0.9343\n",
      "Epoch 112/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6738 - mse: 0.1779 - val_loss: 1.4289 - val_mse: 0.9329\n",
      "Epoch 113/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6727 - mse: 0.1772 - val_loss: 1.4277 - val_mse: 0.9319\n",
      "Epoch 114/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6717 - mse: 0.1765 - val_loss: 1.4271 - val_mse: 0.9316\n",
      "Epoch 115/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6707 - mse: 0.1760 - val_loss: 1.4258 - val_mse: 0.9304\n",
      "Epoch 116/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6697 - mse: 0.1749 - val_loss: 1.4246 - val_mse: 0.9295\n",
      "Epoch 117/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6687 - mse: 0.1744 - val_loss: 1.4233 - val_mse: 0.9284\n",
      "Epoch 118/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6678 - mse: 0.1736 - val_loss: 1.4229 - val_mse: 0.9285\n",
      "Epoch 119/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6669 - mse: 0.1731 - val_loss: 1.4227 - val_mse: 0.9286\n",
      "Epoch 120/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6660 - mse: 0.1725 - val_loss: 1.4212 - val_mse: 0.9275\n",
      "Epoch 121/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6651 - mse: 0.1719 - val_loss: 1.4207 - val_mse: 0.9272\n",
      "Epoch 122/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6643 - mse: 0.1713 - val_loss: 1.4197 - val_mse: 0.9264\n",
      "Epoch 123/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6634 - mse: 0.1707 - val_loss: 1.4192 - val_mse: 0.9259\n",
      "Epoch 124/500\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6626 - mse: 0.1702 - val_loss: 1.4178 - val_mse: 0.9250\n",
      "Epoch 125/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6618 - mse: 0.1696 - val_loss: 1.4179 - val_mse: 0.9253\n",
      "Epoch 126/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6611 - mse: 0.1689 - val_loss: 1.4163 - val_mse: 0.9241\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6603 - mse: 0.1686 - val_loss: 1.4158 - val_mse: 0.9237\n",
      "Epoch 128/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6596 - mse: 0.1681 - val_loss: 1.4148 - val_mse: 0.9230\n",
      "Epoch 129/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6588 - mse: 0.1674 - val_loss: 1.4139 - val_mse: 0.9224\n",
      "Epoch 130/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6583 - mse: 0.1674 - val_loss: 1.4139 - val_mse: 0.9225\n",
      "Epoch 131/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6576 - mse: 0.1667 - val_loss: 1.4137 - val_mse: 0.9226\n",
      "Epoch 132/500\n",
      "160/160 [==============================] - 0s 3ms/step - loss: 0.6569 - mse: 0.1663 - val_loss: 1.4127 - val_mse: 0.9217\n",
      "Epoch 133/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6562 - mse: 0.1658 - val_loss: 1.4116 - val_mse: 0.9210\n",
      "Epoch 134/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6556 - mse: 0.1654 - val_loss: 1.4110 - val_mse: 0.9204\n",
      "Epoch 135/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6550 - mse: 0.1650 - val_loss: 1.4098 - val_mse: 0.9194\n",
      "Epoch 136/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6544 - mse: 0.1645 - val_loss: 1.4098 - val_mse: 0.9199\n",
      "Epoch 137/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6539 - mse: 0.1643 - val_loss: 1.4088 - val_mse: 0.9189\n",
      "Epoch 138/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6532 - mse: 0.1639 - val_loss: 1.4081 - val_mse: 0.9184\n",
      "Epoch 139/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6527 - mse: 0.1636 - val_loss: 1.4080 - val_mse: 0.9184\n",
      "Epoch 140/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6522 - mse: 0.1631 - val_loss: 1.4071 - val_mse: 0.9178\n",
      "Epoch 141/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6517 - mse: 0.1629 - val_loss: 1.4071 - val_mse: 0.9180\n",
      "Epoch 142/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6511 - mse: 0.1624 - val_loss: 1.4063 - val_mse: 0.9172\n",
      "Epoch 143/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6506 - mse: 0.1621 - val_loss: 1.4055 - val_mse: 0.9167\n",
      "Epoch 144/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6502 - mse: 0.1619 - val_loss: 1.4057 - val_mse: 0.9168\n",
      "Epoch 145/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6497 - mse: 0.1615 - val_loss: 1.4047 - val_mse: 0.9161\n",
      "Epoch 146/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6492 - mse: 0.1609 - val_loss: 1.4045 - val_mse: 0.9160\n",
      "Epoch 147/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6488 - mse: 0.1609 - val_loss: 1.4038 - val_mse: 0.9156\n",
      "Epoch 148/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6483 - mse: 0.1605 - val_loss: 1.4033 - val_mse: 0.9151\n",
      "Epoch 149/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6479 - mse: 0.1601 - val_loss: 1.4026 - val_mse: 0.9148\n",
      "Epoch 150/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6474 - mse: 0.1601 - val_loss: 1.4020 - val_mse: 0.9142\n",
      "Epoch 151/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6471 - mse: 0.1598 - val_loss: 1.4011 - val_mse: 0.9134\n",
      "Epoch 152/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6467 - mse: 0.1596 - val_loss: 1.4012 - val_mse: 0.9136\n",
      "Epoch 153/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6463 - mse: 0.1591 - val_loss: 1.4006 - val_mse: 0.9130\n",
      "Epoch 154/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6459 - mse: 0.1588 - val_loss: 1.4003 - val_mse: 0.9129\n",
      "Epoch 155/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6455 - mse: 0.1587 - val_loss: 1.4006 - val_mse: 0.9134\n",
      "Epoch 156/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6451 - mse: 0.1585 - val_loss: 1.3996 - val_mse: 0.9127\n",
      "Epoch 157/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6448 - mse: 0.1583 - val_loss: 1.3996 - val_mse: 0.9126\n",
      "Epoch 158/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6445 - mse: 0.1579 - val_loss: 1.3988 - val_mse: 0.9120\n",
      "Epoch 159/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6442 - mse: 0.1577 - val_loss: 1.3993 - val_mse: 0.9126\n",
      "Epoch 160/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6438 - mse: 0.1574 - val_loss: 1.3989 - val_mse: 0.9122\n",
      "Epoch 161/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6435 - mse: 0.1573 - val_loss: 1.3986 - val_mse: 0.9122\n",
      "Epoch 162/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6432 - mse: 0.1571 - val_loss: 1.3981 - val_mse: 0.9117\n",
      "Epoch 163/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6428 - mse: 0.1569 - val_loss: 1.3972 - val_mse: 0.9109\n",
      "Epoch 164/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6425 - mse: 0.1564 - val_loss: 1.3976 - val_mse: 0.9113\n",
      "Epoch 165/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6422 - mse: 0.1563 - val_loss: 1.3969 - val_mse: 0.9106\n",
      "Epoch 166/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6420 - mse: 0.1562 - val_loss: 1.3966 - val_mse: 0.9105\n",
      "Epoch 167/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6417 - mse: 0.1561 - val_loss: 1.3971 - val_mse: 0.9111\n",
      "Epoch 168/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6414 - mse: 0.1557 - val_loss: 1.3966 - val_mse: 0.9107\n",
      "Epoch 169/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6411 - mse: 0.1557 - val_loss: 1.3960 - val_mse: 0.9100\n",
      "Epoch 170/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6408 - mse: 0.1553 - val_loss: 1.3957 - val_mse: 0.9099\n",
      "Epoch 171/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6405 - mse: 0.1552 - val_loss: 1.3952 - val_mse: 0.9098\n",
      "Epoch 172/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6404 - mse: 0.1553 - val_loss: 1.3951 - val_mse: 0.9093\n",
      "Epoch 173/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6401 - mse: 0.1548 - val_loss: 1.3949 - val_mse: 0.9092\n",
      "Epoch 174/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6399 - mse: 0.1547 - val_loss: 1.3950 - val_mse: 0.9095\n",
      "Epoch 175/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6397 - mse: 0.1546 - val_loss: 1.3942 - val_mse: 0.9089\n",
      "Epoch 176/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6394 - mse: 0.1544 - val_loss: 1.3941 - val_mse: 0.9087\n",
      "Epoch 177/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6392 - mse: 0.1542 - val_loss: 1.3943 - val_mse: 0.9091\n",
      "Epoch 178/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6390 - mse: 0.1543 - val_loss: 1.3936 - val_mse: 0.9084\n",
      "Epoch 179/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6387 - mse: 0.1538 - val_loss: 1.3940 - val_mse: 0.9087\n",
      "Epoch 180/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6386 - mse: 0.1537 - val_loss: 1.3940 - val_mse: 0.9088\n",
      "Epoch 181/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6383 - mse: 0.1535 - val_loss: 1.3935 - val_mse: 0.9084\n",
      "Epoch 182/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6382 - mse: 0.1536 - val_loss: 1.3934 - val_mse: 0.9083\n",
      "Epoch 183/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6379 - mse: 0.1532 - val_loss: 1.3928 - val_mse: 0.9077\n",
      "Epoch 184/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6378 - mse: 0.1532 - val_loss: 1.3926 - val_mse: 0.9078\n",
      "Epoch 185/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6376 - mse: 0.1530 - val_loss: 1.3926 - val_mse: 0.9077\n",
      "Epoch 186/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6373 - mse: 0.1528 - val_loss: 1.3924 - val_mse: 0.9077\n",
      "Epoch 187/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6372 - mse: 0.1530 - val_loss: 1.3924 - val_mse: 0.9076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6370 - mse: 0.1526 - val_loss: 1.3923 - val_mse: 0.9075\n",
      "Epoch 189/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6368 - mse: 0.1525 - val_loss: 1.3922 - val_mse: 0.9074\n",
      "Epoch 190/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6367 - mse: 0.1524 - val_loss: 1.3919 - val_mse: 0.9071\n",
      "Epoch 191/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6365 - mse: 0.1520 - val_loss: 1.3919 - val_mse: 0.9071\n",
      "Epoch 192/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6364 - mse: 0.1520 - val_loss: 1.3916 - val_mse: 0.9070\n",
      "Epoch 193/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6362 - mse: 0.1520 - val_loss: 1.3917 - val_mse: 0.9071\n",
      "Epoch 194/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6360 - mse: 0.1519 - val_loss: 1.3915 - val_mse: 0.9070\n",
      "Epoch 195/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6359 - mse: 0.1517 - val_loss: 1.3914 - val_mse: 0.9069\n",
      "Epoch 196/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6358 - mse: 0.1517 - val_loss: 1.3911 - val_mse: 0.9064\n",
      "Epoch 197/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6356 - mse: 0.1514 - val_loss: 1.3910 - val_mse: 0.9065\n",
      "Epoch 198/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6355 - mse: 0.1514 - val_loss: 1.3907 - val_mse: 0.9061\n",
      "Epoch 199/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6353 - mse: 0.1512 - val_loss: 1.3911 - val_mse: 0.9067\n",
      "Epoch 200/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6352 - mse: 0.1512 - val_loss: 1.3909 - val_mse: 0.9064\n",
      "Epoch 201/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6351 - mse: 0.1510 - val_loss: 1.3905 - val_mse: 0.9060\n",
      "Epoch 202/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6350 - mse: 0.1511 - val_loss: 1.3903 - val_mse: 0.9058\n",
      "Epoch 203/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6348 - mse: 0.1508 - val_loss: 1.3909 - val_mse: 0.9064\n",
      "Epoch 204/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6347 - mse: 0.1508 - val_loss: 1.3909 - val_mse: 0.9065\n",
      "Epoch 205/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6346 - mse: 0.1505 - val_loss: 1.3901 - val_mse: 0.9058\n",
      "Epoch 206/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6345 - mse: 0.1505 - val_loss: 1.3900 - val_mse: 0.9057\n",
      "Epoch 207/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6344 - mse: 0.1504 - val_loss: 1.3901 - val_mse: 0.9057\n",
      "Epoch 208/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6343 - mse: 0.1505 - val_loss: 1.3901 - val_mse: 0.9058\n",
      "Epoch 209/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6341 - mse: 0.1502 - val_loss: 1.3906 - val_mse: 0.9061\n",
      "Epoch 210/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6340 - mse: 0.1503 - val_loss: 1.3903 - val_mse: 0.9059\n",
      "Epoch 211/500\n",
      "144/160 [==========================>...] - ETA: 0s - loss: 0.6327 - mse: 0.1488Restoring model weights from the end of the best epoch: 206.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.6339 - mse: 0.1499 - val_loss: 1.3903 - val_mse: 0.9060\n",
      "Epoch 211: early stopping\n",
      "k=30, lambda_=5e-05, val_mse=[11.982768058776855, 8.071343421936035, 3.576066493988037, 2.2998299598693848, 1.8353016376495361, 1.5920283794403076, 1.449804663658142, 1.3575754165649414, 1.295601725578308, 1.2528189420700073, 1.2209607362747192, 1.1960138082504272, 1.181051254272461, 1.1664729118347168, 1.1556698083877563, 1.1481231451034546, 1.1418514251708984, 1.1375584602355957, 1.1324516534805298, 1.1287976503372192, 1.1240475177764893, 1.1231886148452759, 1.1193602085113525, 1.1144460439682007, 1.1135557889938354, 1.1107087135314941, 1.107589840888977, 1.1042119264602661, 1.1021662950515747, 1.0974928140640259, 1.0937591791152954, 1.090470314025879, 1.086674451828003, 1.0835245847702026, 1.0802747011184692, 1.0761995315551758, 1.0728470087051392, 1.068161129951477, 1.064499020576477, 1.0606378316879272, 1.0551859140396118, 1.0526913404464722, 1.047871708869934, 1.0432919263839722, 1.0405453443527222, 1.0366630554199219, 1.0314338207244873, 1.030825138092041, 1.0249327421188354, 1.0215952396392822, 1.0192391872406006, 1.0144357681274414, 1.0137168169021606, 1.0103278160095215, 1.0052855014801025, 1.0028681755065918, 1.0002330541610718, 0.9979672431945801, 0.9944105744361877, 0.9915074706077576, 0.9913502931594849, 0.9885517358779907, 0.9859221577644348, 0.9838383197784424, 0.9820224642753601, 0.979732871055603, 0.9786854982376099, 0.9760392308235168, 0.9751899838447571, 0.9732202887535095, 0.9717186093330383, 0.9706987142562866, 0.9684551954269409, 0.9665607810020447, 0.9648175835609436, 0.9644772410392761, 0.9632769823074341, 0.9619458913803101, 0.9603161215782166, 0.9593846201896667, 0.958704948425293, 0.95787513256073, 0.9563015103340149, 0.9561124444007874, 0.9540136456489563, 0.9531755447387695, 0.9522128701210022, 0.9509449601173401, 0.9508082866668701, 0.9494944214820862, 0.9488407373428345, 0.9484851360321045, 0.9481275081634521, 0.9461896419525146, 0.945217490196228, 0.944414496421814, 0.9440219402313232, 0.9429795742034912, 0.9424179792404175, 0.9419229626655579, 0.9402841925621033, 0.9401371479034424, 0.9398302435874939, 0.9385493993759155, 0.9376980662345886, 0.9364789128303528, 0.9366459846496582, 0.9357592463493347, 0.934495210647583, 0.9337180256843567, 0.9342848658561707, 0.9329349398612976, 0.9319102764129639, 0.9316359162330627, 0.9304458498954773, 0.9295485019683838, 0.9284387230873108, 0.9284621477127075, 0.9285892248153687, 0.9275308847427368, 0.9272248148918152, 0.9263938069343567, 0.9259385466575623, 0.9250350594520569, 0.9252772331237793, 0.9240657091140747, 0.9237308502197266, 0.9230256676673889, 0.9223925471305847, 0.9224967956542969, 0.9225661158561707, 0.9216827154159546, 0.9209616184234619, 0.9203736782073975, 0.9194261431694031, 0.9199269413948059, 0.918942391872406, 0.9184457063674927, 0.9183887243270874, 0.9177975654602051, 0.9179934859275818, 0.9172260165214539, 0.9166626930236816, 0.9168363809585571, 0.9161352515220642, 0.9160486459732056, 0.9155886769294739, 0.9151228666305542, 0.914772629737854, 0.9141740798950195, 0.913404643535614, 0.9136325120925903, 0.9130041599273682, 0.9128691554069519, 0.9134058356285095, 0.9126631617546082, 0.9126085042953491, 0.9119831323623657, 0.9126031398773193, 0.9122380614280701, 0.9121787548065186, 0.9116905927658081, 0.9108576774597168, 0.9112677574157715, 0.9106258153915405, 0.9104660153388977, 0.9111294746398926, 0.9107033014297485, 0.9099675416946411, 0.9098740220069885, 0.9097533822059631, 0.909348726272583, 0.9091873168945312, 0.909485936164856, 0.9088752865791321, 0.9086988568305969, 0.9091389179229736, 0.908413827419281, 0.9086912870407104, 0.9087507724761963, 0.9084185361862183, 0.9083206057548523, 0.9076969623565674, 0.9077544212341309, 0.9076685905456543, 0.9076923727989197, 0.9075590372085571, 0.9074501395225525, 0.9073789119720459, 0.9071308970451355, 0.9071089625358582, 0.9070091843605042, 0.907134473323822, 0.9069602489471436, 0.9068785905838013, 0.9064408540725708, 0.9064978957176208, 0.9061267971992493, 0.9066933989524841, 0.9064035415649414, 0.906003475189209, 0.9058172106742859, 0.9064497947692871, 0.9064798355102539, 0.9058265089988708, 0.9057247042655945, 0.9057448506355286, 0.9058485627174377, 0.9061154723167419, 0.9058869481086731, 0.9060300588607788]\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°\n",
      "TESTING... 30 | 2e-05\n",
      "\n",
      "Epoch 1/500\n",
      "160/160 [==============================] - 1s 3ms/step - loss: 12.7160 - mse: 12.7132 - val_loss: 11.9851 - val_mse: 11.9819\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 2ms/step - loss: 10.4848 - mse: 10.4747 - val_loss: 8.1951 - val_mse: 8.1716\n",
      "Epoch 3/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 5.5187 - mse: 5.4710 - val_loss: 3.6831 - val_mse: 3.6096\n",
      "Epoch 4/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 2.6481 - mse: 2.5535 - val_loss: 2.3691 - val_mse: 2.2559\n",
      "Epoch 5/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.8249 - mse: 1.6982 - val_loss: 1.9183 - val_mse: 1.7790\n",
      "Epoch 6/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.4695 - mse: 1.3198 - val_loss: 1.6935 - val_mse: 1.5341\n",
      "Epoch 7/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.2728 - mse: 1.1052 - val_loss: 1.5676 - val_mse: 1.3921\n",
      "Epoch 8/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.1500 - mse: 0.9676 - val_loss: 1.4911 - val_mse: 1.3023\n",
      "Epoch 9/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0673 - mse: 0.8729 - val_loss: 1.4415 - val_mse: 1.2418\n",
      "Epoch 10/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.0077 - mse: 0.8033 - val_loss: 1.4076 - val_mse: 1.1985\n",
      "Epoch 11/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9627 - mse: 0.7497 - val_loss: 1.3857 - val_mse: 1.1688\n",
      "Epoch 12/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9273 - mse: 0.7071 - val_loss: 1.3715 - val_mse: 1.1482\n",
      "Epoch 13/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8986 - mse: 0.6725 - val_loss: 1.3632 - val_mse: 1.1344\n",
      "Epoch 14/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8746 - mse: 0.6435 - val_loss: 1.3569 - val_mse: 1.1235\n",
      "Epoch 15/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8539 - mse: 0.6185 - val_loss: 1.3532 - val_mse: 1.1159\n",
      "Epoch 16/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8354 - mse: 0.5965 - val_loss: 1.3505 - val_mse: 1.1098\n",
      "Epoch 17/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8197 - mse: 0.5776 - val_loss: 1.3485 - val_mse: 1.1054\n",
      "Epoch 18/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.8053 - mse: 0.5610 - val_loss: 1.3487 - val_mse: 1.1031\n",
      "Epoch 19/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7915 - mse: 0.5451 - val_loss: 1.3498 - val_mse: 1.1024\n",
      "Epoch 20/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7794 - mse: 0.5312 - val_loss: 1.3505 - val_mse: 1.1018\n",
      "Epoch 21/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7680 - mse: 0.5187 - val_loss: 1.3513 - val_mse: 1.1011\n",
      "Epoch 22/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7573 - mse: 0.5066 - val_loss: 1.3539 - val_mse: 1.1029\n",
      "Epoch 23/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7470 - mse: 0.4954 - val_loss: 1.3565 - val_mse: 1.1046\n",
      "Epoch 24/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7370 - mse: 0.4848 - val_loss: 1.3578 - val_mse: 1.1052\n",
      "Epoch 25/500\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7276 - mse: 0.4748 - val_loss: 1.3610 - val_mse: 1.1079\n",
      "Epoch 26/500\n",
      "145/160 [==========================>...] - ETA: 0s - loss: 0.7165 - mse: 0.4633Restoring model weights from the end of the best epoch: 21.\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7183 - mse: 0.4651 - val_loss: 1.3615 - val_mse: 1.1078\n",
      "Epoch 26: early stopping\n",
      "k=30, lambda_=2e-05, val_mse=[11.981945991516113, 8.171574592590332, 3.609586477279663, 2.2559497356414795, 1.7789840698242188, 1.5341113805770874, 1.3920648097991943, 1.302349328994751, 1.2417579889297485, 1.1984691619873047, 1.168841004371643, 1.1482298374176025, 1.1344220638275146, 1.123545527458191, 1.1158778667449951, 1.1098077297210693, 1.105370044708252, 1.1031147241592407, 1.102356195449829, 1.1017550230026245, 1.1010684967041016, 1.1028752326965332, 1.104615569114685, 1.1051931381225586, 1.1078909635543823, 1.107826590538025]\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score, best_model = grid_search(\n",
    "    train, \n",
    "    param_grid, \n",
    "    get_mf_bias_l2_reg_model,\n",
    "    nb_users, \n",
    "    nb_movies, \n",
    "    validation_size = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyper-parameters : {'k': 30, 'lambda': 0.0002}\n",
      "Best validation RMSE : 0.8920504450798035\n"
     ]
    }
   ],
   "source": [
    "print('Best hyper-parameters : ' + str(best_params))\n",
    "print('Best validation RMSE : ' + str(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([324, 473, 300, ..., 572, 303, 201]),\n",
       " array([2075, 2226,  587, ..., 1042,  989, 4915])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s 460us/step\n",
      "Best model test RMSE : 0.9400150259687583 \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain on all the dataset with the best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually other hyper-parameters such as the ones of SGD should also be grid-searched, like the number of epochs or the batch size. But that would be a bit long for this course. \n",
    "\n",
    "Now we want to do the best prediction possible, so retrain below your model on the whole dataset, including the test set, with the best values obtained from your grid search to make new predictions with our optimal parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "  7/178 [>.............................] - ETA: 1s - loss: 13.2909 - mse: 13.2440  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:14:59.088437: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 10ms/step - loss: 12.6486 - mse: 12.6312 - val_loss: 11.7563 - val_mse: 11.7303\n",
      "Epoch 2/500\n",
      "  1/178 [..............................] - ETA: 1s - loss: 12.4409 - mse: 12.4150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:15:00.872674: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 9ms/step - loss: 9.5454 - mse: 9.4028 - val_loss: 6.7211 - val_mse: 6.3835\n",
      "Epoch 3/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 4.8131 - mse: 4.2330 - val_loss: 3.8186 - val_mse: 3.0255\n",
      "Epoch 4/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 3.3361 - mse: 2.4125 - val_loss: 3.2578 - val_mse: 2.2291\n",
      "Epoch 5/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.9528 - mse: 1.8526 - val_loss: 3.0565 - val_mse: 1.8935\n",
      "Epoch 6/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.7869 - mse: 1.5802 - val_loss: 2.9553 - val_mse: 1.7098\n",
      "Epoch 7/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.6952 - mse: 1.4229 - val_loss: 2.8956 - val_mse: 1.5997\n",
      "Epoch 8/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.6345 - mse: 1.3229 - val_loss: 2.8504 - val_mse: 1.5235\n",
      "Epoch 9/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5892 - mse: 1.2537 - val_loss: 2.8142 - val_mse: 1.4716\n",
      "Epoch 10/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5513 - mse: 1.2048 - val_loss: 2.7799 - val_mse: 1.4300\n",
      "Epoch 11/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.5178 - mse: 1.1670 - val_loss: 2.7476 - val_mse: 1.3985\n",
      "Epoch 12/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 2.4865 - mse: 1.1397 - val_loss: 2.7161 - val_mse: 1.3707\n",
      "Epoch 13/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4562 - mse: 1.1146 - val_loss: 2.6844 - val_mse: 1.3467\n",
      "Epoch 14/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.4271 - mse: 1.0940 - val_loss: 2.6530 - val_mse: 1.3266\n",
      "Epoch 15/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.3983 - mse: 1.0786 - val_loss: 2.6215 - val_mse: 1.3080\n",
      "Epoch 16/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.3697 - mse: 1.0632 - val_loss: 2.5886 - val_mse: 1.2877\n",
      "Epoch 17/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.3413 - mse: 1.0483 - val_loss: 2.5571 - val_mse: 1.2728\n",
      "Epoch 18/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.3134 - mse: 1.0366 - val_loss: 2.5261 - val_mse: 1.2565\n",
      "Epoch 19/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2855 - mse: 1.0237 - val_loss: 2.4951 - val_mse: 1.2421\n",
      "Epoch 20/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2580 - mse: 1.0131 - val_loss: 2.4644 - val_mse: 1.2272\n",
      "Epoch 21/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2311 - mse: 1.0016 - val_loss: 2.4341 - val_mse: 1.2131\n",
      "Epoch 22/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.2050 - mse: 0.9922 - val_loss: 2.4041 - val_mse: 1.1993\n",
      "Epoch 23/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1793 - mse: 0.9828 - val_loss: 2.3763 - val_mse: 1.1870\n",
      "Epoch 24/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1540 - mse: 0.9716 - val_loss: 2.3483 - val_mse: 1.1735\n",
      "Epoch 25/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1303 - mse: 0.9636 - val_loss: 2.3230 - val_mse: 1.1633\n",
      "Epoch 26/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.1067 - mse: 0.9537 - val_loss: 2.2964 - val_mse: 1.1503\n",
      "Epoch 27/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0844 - mse: 0.9459 - val_loss: 2.2712 - val_mse: 1.1400\n",
      "Epoch 28/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0627 - mse: 0.9378 - val_loss: 2.2463 - val_mse: 1.1284\n",
      "Epoch 29/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0421 - mse: 0.9300 - val_loss: 2.2248 - val_mse: 1.1196\n",
      "Epoch 30/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0225 - mse: 0.9230 - val_loss: 2.2020 - val_mse: 1.1080\n",
      "Epoch 31/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 2.0036 - mse: 0.9156 - val_loss: 2.1814 - val_mse: 1.0991\n",
      "Epoch 32/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9855 - mse: 0.9085 - val_loss: 2.1614 - val_mse: 1.0906\n",
      "Epoch 33/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9685 - mse: 0.9021 - val_loss: 2.1425 - val_mse: 1.0818\n",
      "Epoch 34/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9518 - mse: 0.8958 - val_loss: 2.1238 - val_mse: 1.0719\n",
      "Epoch 35/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9359 - mse: 0.8889 - val_loss: 2.1068 - val_mse: 1.0637\n",
      "Epoch 36/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9210 - mse: 0.8831 - val_loss: 2.0899 - val_mse: 1.0558\n",
      "Epoch 37/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.9067 - mse: 0.8765 - val_loss: 2.0736 - val_mse: 1.0476\n",
      "Epoch 38/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8931 - mse: 0.8708 - val_loss: 2.0578 - val_mse: 1.0398\n",
      "Epoch 39/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8800 - mse: 0.8655 - val_loss: 2.0442 - val_mse: 1.0334\n",
      "Epoch 40/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8676 - mse: 0.8598 - val_loss: 2.0295 - val_mse: 1.0255\n",
      "Epoch 41/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8558 - mse: 0.8541 - val_loss: 2.0165 - val_mse: 1.0186\n",
      "Epoch 42/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8446 - mse: 0.8492 - val_loss: 2.0042 - val_mse: 1.0130\n",
      "Epoch 43/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8344 - mse: 0.8442 - val_loss: 1.9933 - val_mse: 1.0072\n",
      "Epoch 44/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8242 - mse: 0.8398 - val_loss: 1.9825 - val_mse: 1.0011\n",
      "Epoch 45/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8147 - mse: 0.8343 - val_loss: 1.9720 - val_mse: 0.9950\n",
      "Epoch 46/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.8058 - mse: 0.8303 - val_loss: 1.9617 - val_mse: 0.9888\n",
      "Epoch 47/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7974 - mse: 0.8259 - val_loss: 1.9531 - val_mse: 0.9838\n",
      "Epoch 48/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7894 - mse: 0.8218 - val_loss: 1.9448 - val_mse: 0.9793\n",
      "Epoch 49/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7819 - mse: 0.8172 - val_loss: 1.9366 - val_mse: 0.9736\n",
      "Epoch 50/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7748 - mse: 0.8128 - val_loss: 1.9299 - val_mse: 0.9708\n",
      "Epoch 51/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7681 - mse: 0.8100 - val_loss: 1.9224 - val_mse: 0.9652\n",
      "Epoch 52/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7617 - mse: 0.8055 - val_loss: 1.9160 - val_mse: 0.9612\n",
      "Epoch 53/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7558 - mse: 0.8015 - val_loss: 1.9107 - val_mse: 0.9579\n",
      "Epoch 54/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7503 - mse: 0.7979 - val_loss: 1.9051 - val_mse: 0.9541\n",
      "Epoch 55/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7449 - mse: 0.7942 - val_loss: 1.8995 - val_mse: 0.9498\n",
      "Epoch 56/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7399 - mse: 0.7911 - val_loss: 1.8940 - val_mse: 0.9455\n",
      "Epoch 57/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.7354 - mse: 0.7881 - val_loss: 1.8901 - val_mse: 0.9439\n",
      "Epoch 58/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7309 - mse: 0.7852 - val_loss: 1.8853 - val_mse: 0.9403\n",
      "Epoch 59/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7267 - mse: 0.7815 - val_loss: 1.8817 - val_mse: 0.9372\n",
      "Epoch 60/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7231 - mse: 0.7790 - val_loss: 1.8782 - val_mse: 0.9351\n",
      "Epoch 61/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7192 - mse: 0.7761 - val_loss: 1.8752 - val_mse: 0.9322\n",
      "Epoch 62/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7161 - mse: 0.7733 - val_loss: 1.8716 - val_mse: 0.9299\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7126 - mse: 0.7704 - val_loss: 1.8694 - val_mse: 0.9289\n",
      "Epoch 64/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7097 - mse: 0.7688 - val_loss: 1.8663 - val_mse: 0.9257\n",
      "Epoch 65/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7071 - mse: 0.7664 - val_loss: 1.8632 - val_mse: 0.9227\n",
      "Epoch 66/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7044 - mse: 0.7636 - val_loss: 1.8615 - val_mse: 0.9214\n",
      "Epoch 67/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.7018 - mse: 0.7616 - val_loss: 1.8591 - val_mse: 0.9187\n",
      "Epoch 68/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6998 - mse: 0.7588 - val_loss: 1.8563 - val_mse: 0.9164\n",
      "Epoch 69/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6973 - mse: 0.7571 - val_loss: 1.8542 - val_mse: 0.9148\n",
      "Epoch 70/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6955 - mse: 0.7556 - val_loss: 1.8524 - val_mse: 0.9129\n",
      "Epoch 71/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6936 - mse: 0.7535 - val_loss: 1.8509 - val_mse: 0.9115\n",
      "Epoch 72/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6917 - mse: 0.7518 - val_loss: 1.8490 - val_mse: 0.9083\n",
      "Epoch 73/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6900 - mse: 0.7495 - val_loss: 1.8479 - val_mse: 0.9079\n",
      "Epoch 74/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6885 - mse: 0.7479 - val_loss: 1.8470 - val_mse: 0.9070\n",
      "Epoch 75/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6869 - mse: 0.7473 - val_loss: 1.8464 - val_mse: 0.9054\n",
      "Epoch 76/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6856 - mse: 0.7445 - val_loss: 1.8448 - val_mse: 0.9043\n",
      "Epoch 77/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6842 - mse: 0.7430 - val_loss: 1.8441 - val_mse: 0.9039\n",
      "Epoch 78/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6831 - mse: 0.7416 - val_loss: 1.8426 - val_mse: 0.9014\n",
      "Epoch 79/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6819 - mse: 0.7409 - val_loss: 1.8418 - val_mse: 0.9006\n",
      "Epoch 80/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6808 - mse: 0.7390 - val_loss: 1.8405 - val_mse: 0.8986\n",
      "Epoch 81/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6798 - mse: 0.7375 - val_loss: 1.8410 - val_mse: 0.8984\n",
      "Epoch 82/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6789 - mse: 0.7359 - val_loss: 1.8394 - val_mse: 0.8966\n",
      "Epoch 83/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6780 - mse: 0.7346 - val_loss: 1.8395 - val_mse: 0.8968\n",
      "Epoch 84/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6771 - mse: 0.7336 - val_loss: 1.8389 - val_mse: 0.8960\n",
      "Epoch 85/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6764 - mse: 0.7324 - val_loss: 1.8385 - val_mse: 0.8948\n",
      "Epoch 86/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6756 - mse: 0.7311 - val_loss: 1.8380 - val_mse: 0.8939\n",
      "Epoch 87/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6748 - mse: 0.7302 - val_loss: 1.8373 - val_mse: 0.8931\n",
      "Epoch 88/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6742 - mse: 0.7286 - val_loss: 1.8372 - val_mse: 0.8923\n",
      "Epoch 89/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6737 - mse: 0.7280 - val_loss: 1.8368 - val_mse: 0.8915\n",
      "Epoch 90/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6730 - mse: 0.7269 - val_loss: 1.8368 - val_mse: 0.8909\n",
      "Epoch 91/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6725 - mse: 0.7253 - val_loss: 1.8365 - val_mse: 0.8904\n",
      "Epoch 92/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6719 - mse: 0.7253 - val_loss: 1.8362 - val_mse: 0.8897\n",
      "Epoch 93/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6714 - mse: 0.7245 - val_loss: 1.8352 - val_mse: 0.8880\n",
      "Epoch 94/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6710 - mse: 0.7232 - val_loss: 1.8353 - val_mse: 0.8877\n",
      "Epoch 95/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6705 - mse: 0.7222 - val_loss: 1.8361 - val_mse: 0.8882\n",
      "Epoch 96/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6702 - mse: 0.7217 - val_loss: 1.8355 - val_mse: 0.8869\n",
      "Epoch 97/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6697 - mse: 0.7204 - val_loss: 1.8351 - val_mse: 0.8864\n",
      "Epoch 98/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6695 - mse: 0.7202 - val_loss: 1.8354 - val_mse: 0.8858\n",
      "Epoch 99/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6690 - mse: 0.7180 - val_loss: 1.8354 - val_mse: 0.8855\n",
      "Epoch 100/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6688 - mse: 0.7187 - val_loss: 1.8354 - val_mse: 0.8848\n",
      "Epoch 101/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6684 - mse: 0.7169 - val_loss: 1.8354 - val_mse: 0.8847\n",
      "Epoch 102/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6681 - mse: 0.7169 - val_loss: 1.8353 - val_mse: 0.8844\n",
      "Epoch 103/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6679 - mse: 0.7159 - val_loss: 1.8354 - val_mse: 0.8836\n",
      "Epoch 104/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6676 - mse: 0.7152 - val_loss: 1.8352 - val_mse: 0.8832\n",
      "Epoch 105/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6674 - mse: 0.7145 - val_loss: 1.8351 - val_mse: 0.8828\n",
      "Epoch 106/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6672 - mse: 0.7142 - val_loss: 1.8354 - val_mse: 0.8827\n",
      "Epoch 107/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6671 - mse: 0.7135 - val_loss: 1.8356 - val_mse: 0.8826\n",
      "Epoch 108/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6668 - mse: 0.7126 - val_loss: 1.8360 - val_mse: 0.8823\n",
      "Epoch 109/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6666 - mse: 0.7123 - val_loss: 1.8355 - val_mse: 0.8816\n",
      "Epoch 110/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6664 - mse: 0.7120 - val_loss: 1.8357 - val_mse: 0.8813\n",
      "Epoch 111/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6662 - mse: 0.7111 - val_loss: 1.8357 - val_mse: 0.8810\n",
      "Epoch 112/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6661 - mse: 0.7109 - val_loss: 1.8360 - val_mse: 0.8806\n",
      "Epoch 113/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6660 - mse: 0.7103 - val_loss: 1.8367 - val_mse: 0.8813\n",
      "Epoch 114/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6657 - mse: 0.7095 - val_loss: 1.8358 - val_mse: 0.8800\n",
      "Epoch 115/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6656 - mse: 0.7092 - val_loss: 1.8361 - val_mse: 0.8802\n",
      "Epoch 116/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6654 - mse: 0.7086 - val_loss: 1.8364 - val_mse: 0.8797\n",
      "Epoch 117/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6654 - mse: 0.7081 - val_loss: 1.8367 - val_mse: 0.8800\n",
      "Epoch 118/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6653 - mse: 0.7079 - val_loss: 1.8362 - val_mse: 0.8790\n",
      "Epoch 119/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6651 - mse: 0.7074 - val_loss: 1.8362 - val_mse: 0.8792\n",
      "Epoch 120/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6651 - mse: 0.7071 - val_loss: 1.8368 - val_mse: 0.8792\n",
      "Epoch 121/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6649 - mse: 0.7072 - val_loss: 1.8370 - val_mse: 0.8788\n",
      "Epoch 122/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6649 - mse: 0.7061 - val_loss: 1.8371 - val_mse: 0.8787\n",
      "Epoch 123/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6648 - mse: 0.7059 - val_loss: 1.8374 - val_mse: 0.8789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6647 - mse: 0.7057 - val_loss: 1.8377 - val_mse: 0.8789\n",
      "Epoch 125/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6646 - mse: 0.7049 - val_loss: 1.8376 - val_mse: 0.8782\n",
      "Epoch 126/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6645 - mse: 0.7051 - val_loss: 1.8373 - val_mse: 0.8781\n",
      "Epoch 127/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6644 - mse: 0.7045 - val_loss: 1.8377 - val_mse: 0.8780\n",
      "Epoch 128/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6644 - mse: 0.7040 - val_loss: 1.8379 - val_mse: 0.8780\n",
      "Epoch 129/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6643 - mse: 0.7037 - val_loss: 1.8382 - val_mse: 0.8781\n",
      "Epoch 130/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6642 - mse: 0.7033 - val_loss: 1.8384 - val_mse: 0.8780\n",
      "Epoch 131/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6642 - mse: 0.7033 - val_loss: 1.8387 - val_mse: 0.8782\n",
      "Epoch 132/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6641 - mse: 0.7033 - val_loss: 1.8381 - val_mse: 0.8770\n",
      "Epoch 133/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6641 - mse: 0.7022 - val_loss: 1.8388 - val_mse: 0.8777\n",
      "Epoch 134/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6640 - mse: 0.7022 - val_loss: 1.8388 - val_mse: 0.8777\n",
      "Epoch 135/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6640 - mse: 0.7027 - val_loss: 1.8395 - val_mse: 0.8779\n",
      "Epoch 136/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6639 - mse: 0.7015 - val_loss: 1.8395 - val_mse: 0.8777\n",
      "Epoch 137/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6638 - mse: 0.7017 - val_loss: 1.8395 - val_mse: 0.8774\n",
      "Epoch 138/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6638 - mse: 0.7012 - val_loss: 1.8396 - val_mse: 0.8774\n",
      "Epoch 139/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6638 - mse: 0.7011 - val_loss: 1.8395 - val_mse: 0.8769\n",
      "Epoch 140/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6637 - mse: 0.7007 - val_loss: 1.8398 - val_mse: 0.8770\n",
      "Epoch 141/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6637 - mse: 0.7005 - val_loss: 1.8401 - val_mse: 0.8777\n",
      "Epoch 142/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6636 - mse: 0.7001 - val_loss: 1.8400 - val_mse: 0.8772\n",
      "Epoch 143/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6635 - mse: 0.7002 - val_loss: 1.8401 - val_mse: 0.8770\n",
      "Epoch 144/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6636 - mse: 0.7000 - val_loss: 1.8402 - val_mse: 0.8769\n",
      "Epoch 145/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6635 - mse: 0.6997 - val_loss: 1.8400 - val_mse: 0.8762\n",
      "Epoch 146/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6635 - mse: 0.6997 - val_loss: 1.8406 - val_mse: 0.8771\n",
      "Epoch 147/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6635 - mse: 0.6988 - val_loss: 1.8406 - val_mse: 0.8770\n",
      "Epoch 148/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6635 - mse: 0.6993 - val_loss: 1.8406 - val_mse: 0.8766\n",
      "Epoch 149/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6634 - mse: 0.6987 - val_loss: 1.8406 - val_mse: 0.8763\n",
      "Epoch 150/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6634 - mse: 0.6986 - val_loss: 1.8413 - val_mse: 0.8766\n",
      "Epoch 151/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6633 - mse: 0.6984 - val_loss: 1.8409 - val_mse: 0.8763\n",
      "Epoch 152/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6633 - mse: 0.6980 - val_loss: 1.8405 - val_mse: 0.8758\n",
      "Epoch 153/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6633 - mse: 0.6982 - val_loss: 1.8411 - val_mse: 0.8763\n",
      "Epoch 154/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6632 - mse: 0.6980 - val_loss: 1.8413 - val_mse: 0.8764\n",
      "Epoch 155/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6633 - mse: 0.6976 - val_loss: 1.8416 - val_mse: 0.8762\n",
      "Epoch 156/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6632 - mse: 0.6976 - val_loss: 1.8414 - val_mse: 0.8761\n",
      "Epoch 157/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6632 - mse: 0.6969 - val_loss: 1.8412 - val_mse: 0.8757\n",
      "Epoch 158/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6632 - mse: 0.6970 - val_loss: 1.8418 - val_mse: 0.8762\n",
      "Epoch 159/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6632 - mse: 0.6975 - val_loss: 1.8419 - val_mse: 0.8767\n",
      "Epoch 160/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6632 - mse: 0.6966 - val_loss: 1.8415 - val_mse: 0.8758\n",
      "Epoch 161/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6631 - mse: 0.6968 - val_loss: 1.8417 - val_mse: 0.8761\n",
      "Epoch 162/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6631 - mse: 0.6970 - val_loss: 1.8419 - val_mse: 0.8760\n",
      "Epoch 163/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6631 - mse: 0.6969 - val_loss: 1.8417 - val_mse: 0.8758\n",
      "Epoch 164/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6631 - mse: 0.6965 - val_loss: 1.8419 - val_mse: 0.8761\n",
      "Epoch 165/500\n",
      "178/178 [==============================] - 2s 10ms/step - loss: 1.6631 - mse: 0.6966 - val_loss: 1.8423 - val_mse: 0.8762\n",
      "Epoch 166/500\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6631 - mse: 0.6966 - val_loss: 1.8421 - val_mse: 0.8758\n",
      "Epoch 167/500\n",
      "175/178 [============================>.] - ETA: 0s - loss: 1.6629 - mse: 0.6959Restoring model weights from the end of the best epoch: 157.\n",
      "178/178 [==============================] - 2s 9ms/step - loss: 1.6631 - mse: 0.6960 - val_loss: 1.8423 - val_mse: 0.8759\n",
      "Epoch 167: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "X = [dataset[\"userId\"].to_numpy(), dataset[\"movieId\"].to_numpy()]\n",
    "y = dataset[\"rating\"].to_numpy()\n",
    "\n",
    "best_model=get_mf_bias_l2_reg_model(nb_users, nb_movies, k = best_params['k'], lambda_ = best_params[\"lambda\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    best_model.fit(X, y, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  44/3152 [..............................] - ETA: 11s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 12:19:39.865958: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3152/3152 [==============================] - 10s 3ms/step\n",
      "Best model test RMSE : 0.8345249200621202 \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    y_pred = best_model.predict(X)\n",
    "\n",
    "test_rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "print(\"Best model test RMSE : %s \" % test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend the top-5 movies for the 10 first users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your retrained best model with optimal hyper parameters, compute the predictions for all the ratings that are not in the `dataset` for the 10 first users (indexes from 0 to 9). That means all the movies $i$ that these users $u \\in 0,\\ldots,9$ haven't rated, thus all the $u,i$ combinations that are not in the `dataset` dataframe rows.\n",
    "\n",
    "Order these predicted ratings for these users by decreasing order, and print out the 5 first ones, i.e. the ones that have the highest predicted ratings. Use the *movies.csv* file to print the real titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9742, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title = pd.read_csv(\"../data/ml-latest-small/movies.csv\")\n",
    "data = pd.read_csv(\"../data/ml-latest-small/ratings.csv\")\n",
    "movie_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 3: 1,\n",
       " 6: 2,\n",
       " 47: 3,\n",
       " 50: 4,\n",
       " 70: 5,\n",
       " 101: 6,\n",
       " 110: 7,\n",
       " 151: 8,\n",
       " 157: 9,\n",
       " 163: 10,\n",
       " 216: 11,\n",
       " 223: 12,\n",
       " 231: 13,\n",
       " 235: 14,\n",
       " 260: 15,\n",
       " 296: 16,\n",
       " 316: 17,\n",
       " 333: 18,\n",
       " 349: 19,\n",
       " 356: 20,\n",
       " 362: 21,\n",
       " 367: 22,\n",
       " 423: 23,\n",
       " 441: 24,\n",
       " 457: 25,\n",
       " 480: 26,\n",
       " 500: 27,\n",
       " 527: 28,\n",
       " 543: 29,\n",
       " 552: 30,\n",
       " 553: 31,\n",
       " 590: 32,\n",
       " 592: 33,\n",
       " 593: 34,\n",
       " 596: 35,\n",
       " 608: 36,\n",
       " 648: 37,\n",
       " 661: 38,\n",
       " 673: 39,\n",
       " 733: 40,\n",
       " 736: 41,\n",
       " 780: 42,\n",
       " 804: 43,\n",
       " 919: 44,\n",
       " 923: 45,\n",
       " 940: 46,\n",
       " 943: 47,\n",
       " 954: 48,\n",
       " 1009: 49,\n",
       " 1023: 50,\n",
       " 1024: 51,\n",
       " 1025: 52,\n",
       " 1029: 53,\n",
       " 1030: 54,\n",
       " 1031: 55,\n",
       " 1032: 56,\n",
       " 1042: 57,\n",
       " 1049: 58,\n",
       " 1060: 59,\n",
       " 1073: 60,\n",
       " 1080: 61,\n",
       " 1089: 62,\n",
       " 1090: 63,\n",
       " 1092: 64,\n",
       " 1097: 65,\n",
       " 1127: 66,\n",
       " 1136: 67,\n",
       " 1196: 68,\n",
       " 1197: 69,\n",
       " 1198: 70,\n",
       " 1206: 71,\n",
       " 1208: 72,\n",
       " 1210: 73,\n",
       " 1213: 74,\n",
       " 1214: 75,\n",
       " 1219: 76,\n",
       " 1220: 77,\n",
       " 1222: 78,\n",
       " 1224: 79,\n",
       " 1226: 80,\n",
       " 1240: 81,\n",
       " 1256: 82,\n",
       " 1258: 83,\n",
       " 1265: 84,\n",
       " 1270: 85,\n",
       " 1275: 86,\n",
       " 1278: 87,\n",
       " 1282: 88,\n",
       " 1291: 89,\n",
       " 1298: 90,\n",
       " 1348: 91,\n",
       " 1377: 92,\n",
       " 1396: 93,\n",
       " 1408: 94,\n",
       " 1445: 95,\n",
       " 1473: 96,\n",
       " 1500: 97,\n",
       " 1517: 98,\n",
       " 1552: 99,\n",
       " 1573: 100,\n",
       " 1580: 101,\n",
       " 1587: 102,\n",
       " 1617: 103,\n",
       " 1620: 104,\n",
       " 1625: 105,\n",
       " 1644: 106,\n",
       " 1676: 107,\n",
       " 1732: 108,\n",
       " 1777: 109,\n",
       " 1793: 110,\n",
       " 1804: 111,\n",
       " 1805: 112,\n",
       " 1920: 113,\n",
       " 1927: 114,\n",
       " 1954: 115,\n",
       " 1967: 116,\n",
       " 2000: 117,\n",
       " 2005: 118,\n",
       " 2012: 119,\n",
       " 2018: 120,\n",
       " 2028: 121,\n",
       " 2033: 122,\n",
       " 2046: 123,\n",
       " 2048: 124,\n",
       " 2054: 125,\n",
       " 2058: 126,\n",
       " 2078: 127,\n",
       " 2090: 128,\n",
       " 2093: 129,\n",
       " 2094: 130,\n",
       " 2096: 131,\n",
       " 2099: 132,\n",
       " 2105: 133,\n",
       " 2115: 134,\n",
       " 2116: 135,\n",
       " 2137: 136,\n",
       " 2139: 137,\n",
       " 2141: 138,\n",
       " 2143: 139,\n",
       " 2161: 140,\n",
       " 2174: 141,\n",
       " 2193: 142,\n",
       " 2253: 143,\n",
       " 2268: 144,\n",
       " 2273: 145,\n",
       " 2291: 146,\n",
       " 2329: 147,\n",
       " 2338: 148,\n",
       " 2353: 149,\n",
       " 2366: 150,\n",
       " 2387: 151,\n",
       " 2389: 152,\n",
       " 2395: 153,\n",
       " 2406: 154,\n",
       " 2414: 155,\n",
       " 2427: 156,\n",
       " 2450: 157,\n",
       " 2459: 158,\n",
       " 2470: 159,\n",
       " 2478: 160,\n",
       " 2492: 161,\n",
       " 2502: 162,\n",
       " 2528: 163,\n",
       " 2529: 164,\n",
       " 2542: 165,\n",
       " 2571: 166,\n",
       " 2580: 167,\n",
       " 2596: 168,\n",
       " 2616: 169,\n",
       " 2617: 170,\n",
       " 2628: 171,\n",
       " 2640: 172,\n",
       " 2641: 173,\n",
       " 2644: 174,\n",
       " 2648: 175,\n",
       " 2654: 176,\n",
       " 2657: 177,\n",
       " 2692: 178,\n",
       " 2700: 179,\n",
       " 2716: 180,\n",
       " 2761: 181,\n",
       " 2797: 182,\n",
       " 2826: 183,\n",
       " 2858: 184,\n",
       " 2872: 185,\n",
       " 2899: 186,\n",
       " 2916: 187,\n",
       " 2944: 188,\n",
       " 2947: 189,\n",
       " 2948: 190,\n",
       " 2949: 191,\n",
       " 2959: 192,\n",
       " 2985: 193,\n",
       " 2987: 194,\n",
       " 2991: 195,\n",
       " 2993: 196,\n",
       " 2997: 197,\n",
       " 3033: 198,\n",
       " 3034: 199,\n",
       " 3052: 200,\n",
       " 3053: 201,\n",
       " 3062: 202,\n",
       " 3147: 203,\n",
       " 3168: 204,\n",
       " 3176: 205,\n",
       " 3243: 206,\n",
       " 3247: 207,\n",
       " 3253: 208,\n",
       " 3273: 209,\n",
       " 3386: 210,\n",
       " 3439: 211,\n",
       " 3440: 212,\n",
       " 3441: 213,\n",
       " 3448: 214,\n",
       " 3450: 215,\n",
       " 3479: 216,\n",
       " 3489: 217,\n",
       " 3527: 218,\n",
       " 3578: 219,\n",
       " 3617: 220,\n",
       " 3639: 221,\n",
       " 3671: 222,\n",
       " 3702: 223,\n",
       " 3703: 224,\n",
       " 3729: 225,\n",
       " 3740: 226,\n",
       " 3744: 227,\n",
       " 3793: 228,\n",
       " 3809: 229,\n",
       " 4006: 230,\n",
       " 5060: 231,\n",
       " 318: 232,\n",
       " 1704: 233,\n",
       " 6874: 234,\n",
       " 8798: 235,\n",
       " 46970: 236,\n",
       " 48516: 237,\n",
       " 58559: 238,\n",
       " 60756: 239,\n",
       " 68157: 240,\n",
       " 71535: 241,\n",
       " 74458: 242,\n",
       " 77455: 243,\n",
       " 79132: 244,\n",
       " 80489: 245,\n",
       " 80906: 246,\n",
       " 86345: 247,\n",
       " 89774: 248,\n",
       " 91529: 249,\n",
       " 91658: 250,\n",
       " 99114: 251,\n",
       " 106782: 252,\n",
       " 109487: 253,\n",
       " 112552: 254,\n",
       " 114060: 255,\n",
       " 115713: 256,\n",
       " 122882: 257,\n",
       " 131724: 258,\n",
       " 31: 259,\n",
       " 647: 260,\n",
       " 688: 261,\n",
       " 720: 262,\n",
       " 849: 263,\n",
       " 914: 264,\n",
       " 1093: 265,\n",
       " 1124: 266,\n",
       " 1263: 267,\n",
       " 1272: 268,\n",
       " 1302: 269,\n",
       " 1371: 270,\n",
       " 2080: 271,\n",
       " 2288: 272,\n",
       " 2424: 273,\n",
       " 2851: 274,\n",
       " 3024: 275,\n",
       " 3210: 276,\n",
       " 3949: 277,\n",
       " 4518: 278,\n",
       " 5048: 279,\n",
       " 5181: 280,\n",
       " 5746: 281,\n",
       " 5764: 282,\n",
       " 5919: 283,\n",
       " 6238: 284,\n",
       " 6835: 285,\n",
       " 7899: 286,\n",
       " 7991: 287,\n",
       " 26409: 288,\n",
       " 70946: 289,\n",
       " 72378: 290,\n",
       " 21: 291,\n",
       " 32: 292,\n",
       " 45: 293,\n",
       " 52: 294,\n",
       " 58: 295,\n",
       " 106: 296,\n",
       " 125: 297,\n",
       " 126: 298,\n",
       " 162: 299,\n",
       " 171: 300,\n",
       " 176: 301,\n",
       " 190: 302,\n",
       " 215: 303,\n",
       " 222: 304,\n",
       " 232: 305,\n",
       " 247: 306,\n",
       " 265: 307,\n",
       " 319: 308,\n",
       " 342: 309,\n",
       " 345: 310,\n",
       " 348: 311,\n",
       " 351: 312,\n",
       " 357: 313,\n",
       " 368: 314,\n",
       " 417: 315,\n",
       " 450: 316,\n",
       " 475: 317,\n",
       " 492: 318,\n",
       " 509: 319,\n",
       " 538: 320,\n",
       " 539: 321,\n",
       " 588: 322,\n",
       " 595: 323,\n",
       " 599: 324,\n",
       " 708: 325,\n",
       " 759: 326,\n",
       " 800: 327,\n",
       " 892: 328,\n",
       " 898: 329,\n",
       " 899: 330,\n",
       " 902: 331,\n",
       " 904: 332,\n",
       " 908: 333,\n",
       " 910: 334,\n",
       " 912: 335,\n",
       " 920: 336,\n",
       " 930: 337,\n",
       " 937: 338,\n",
       " 1046: 339,\n",
       " 1057: 340,\n",
       " 1077: 341,\n",
       " 1079: 342,\n",
       " 1084: 343,\n",
       " 1086: 344,\n",
       " 1094: 345,\n",
       " 1103: 346,\n",
       " 1179: 347,\n",
       " 1183: 348,\n",
       " 1188: 349,\n",
       " 1199: 350,\n",
       " 1203: 351,\n",
       " 1211: 352,\n",
       " 1225: 353,\n",
       " 1250: 354,\n",
       " 1259: 355,\n",
       " 1266: 356,\n",
       " 1279: 357,\n",
       " 1283: 358,\n",
       " 1288: 359,\n",
       " 1304: 360,\n",
       " 1391: 361,\n",
       " 1449: 362,\n",
       " 1466: 363,\n",
       " 1597: 364,\n",
       " 1641: 365,\n",
       " 1719: 366,\n",
       " 1733: 367,\n",
       " 1734: 368,\n",
       " 1834: 369,\n",
       " 1860: 370,\n",
       " 1883: 371,\n",
       " 1885: 372,\n",
       " 1892: 373,\n",
       " 1895: 374,\n",
       " 1907: 375,\n",
       " 1914: 376,\n",
       " 1916: 377,\n",
       " 1923: 378,\n",
       " 1947: 379,\n",
       " 1966: 380,\n",
       " 1968: 381,\n",
       " 2019: 382,\n",
       " 2076: 383,\n",
       " 2109: 384,\n",
       " 2145: 385,\n",
       " 2150: 386,\n",
       " 2186: 387,\n",
       " 2203: 388,\n",
       " 2204: 389,\n",
       " 2282: 390,\n",
       " 2324: 391,\n",
       " 2336: 392,\n",
       " 2351: 393,\n",
       " 2359: 394,\n",
       " 2390: 395,\n",
       " 2467: 396,\n",
       " 2583: 397,\n",
       " 2599: 398,\n",
       " 2683: 399,\n",
       " 2712: 400,\n",
       " 2762: 401,\n",
       " 2763: 402,\n",
       " 2770: 403,\n",
       " 2791: 404,\n",
       " 2843: 405,\n",
       " 2874: 406,\n",
       " 2921: 407,\n",
       " 2926: 408,\n",
       " 2973: 409,\n",
       " 3044: 410,\n",
       " 3060: 411,\n",
       " 3079: 412,\n",
       " 3083: 413,\n",
       " 3160: 414,\n",
       " 3175: 415,\n",
       " 3204: 416,\n",
       " 3255: 417,\n",
       " 3317: 418,\n",
       " 3358: 419,\n",
       " 3365: 420,\n",
       " 3408: 421,\n",
       " 3481: 422,\n",
       " 3508: 423,\n",
       " 3538: 424,\n",
       " 3591: 425,\n",
       " 3788: 426,\n",
       " 3851: 427,\n",
       " 3897: 428,\n",
       " 3911: 429,\n",
       " 3967: 430,\n",
       " 3996: 431,\n",
       " 4002: 432,\n",
       " 4014: 433,\n",
       " 4020: 434,\n",
       " 4021: 435,\n",
       " 4027: 436,\n",
       " 4029: 437,\n",
       " 4033: 438,\n",
       " 4034: 439,\n",
       " 4074: 440,\n",
       " 4121: 441,\n",
       " 4144: 442,\n",
       " 4166: 443,\n",
       " 4226: 444,\n",
       " 4239: 445,\n",
       " 4246: 446,\n",
       " 4252: 447,\n",
       " 4260: 448,\n",
       " 4273: 449,\n",
       " 4308: 450,\n",
       " 4347: 451,\n",
       " 4381: 452,\n",
       " 4641: 453,\n",
       " 4741: 454,\n",
       " 4765: 455,\n",
       " 4881: 456,\n",
       " 4896: 457,\n",
       " 4902: 458,\n",
       " 4967: 459,\n",
       " 34: 460,\n",
       " 36: 461,\n",
       " 39: 462,\n",
       " 150: 463,\n",
       " 153: 464,\n",
       " 253: 465,\n",
       " 261: 466,\n",
       " 266: 467,\n",
       " 290: 468,\n",
       " 300: 469,\n",
       " 344: 470,\n",
       " 364: 471,\n",
       " 380: 472,\n",
       " 410: 473,\n",
       " 474: 474,\n",
       " 515: 475,\n",
       " 531: 476,\n",
       " 534: 477,\n",
       " 589: 478,\n",
       " 594: 479,\n",
       " 597: 480,\n",
       " 2: 481,\n",
       " 4: 482,\n",
       " 5: 483,\n",
       " 7: 484,\n",
       " 8: 485,\n",
       " 10: 486,\n",
       " 11: 487,\n",
       " 13: 488,\n",
       " 15: 489,\n",
       " 16: 490,\n",
       " 17: 491,\n",
       " 19: 492,\n",
       " 22: 493,\n",
       " 24: 494,\n",
       " 25: 495,\n",
       " 26: 496,\n",
       " 27: 497,\n",
       " 41: 498,\n",
       " 43: 499,\n",
       " 46: 500,\n",
       " 54: 501,\n",
       " 60: 502,\n",
       " 61: 503,\n",
       " 62: 504,\n",
       " 65: 505,\n",
       " 66: 506,\n",
       " 76: 507,\n",
       " 79: 508,\n",
       " 86: 509,\n",
       " 87: 510,\n",
       " 88: 511,\n",
       " 89: 512,\n",
       " 92: 513,\n",
       " 93: 514,\n",
       " 95: 515,\n",
       " 100: 516,\n",
       " 102: 517,\n",
       " 104: 518,\n",
       " 105: 519,\n",
       " 112: 520,\n",
       " 113: 521,\n",
       " 135: 522,\n",
       " 140: 523,\n",
       " 141: 524,\n",
       " 145: 525,\n",
       " 146: 526,\n",
       " 158: 527,\n",
       " 159: 528,\n",
       " 160: 529,\n",
       " 161: 530,\n",
       " 165: 531,\n",
       " 168: 532,\n",
       " 170: 533,\n",
       " 174: 534,\n",
       " 177: 535,\n",
       " 179: 536,\n",
       " 180: 537,\n",
       " 181: 538,\n",
       " 185: 539,\n",
       " 186: 540,\n",
       " 189: 541,\n",
       " 191: 542,\n",
       " 195: 543,\n",
       " 196: 544,\n",
       " 201: 545,\n",
       " 204: 546,\n",
       " 205: 547,\n",
       " 207: 548,\n",
       " 208: 549,\n",
       " 209: 550,\n",
       " 210: 551,\n",
       " 212: 552,\n",
       " 217: 553,\n",
       " 218: 554,\n",
       " 219: 555,\n",
       " 224: 556,\n",
       " 225: 557,\n",
       " 230: 558,\n",
       " 234: 559,\n",
       " 236: 560,\n",
       " 237: 561,\n",
       " 239: 562,\n",
       " 240: 563,\n",
       " 243: 564,\n",
       " 248: 565,\n",
       " 250: 566,\n",
       " 251: 567,\n",
       " 252: 568,\n",
       " 254: 569,\n",
       " 256: 570,\n",
       " 257: 571,\n",
       " 258: 572,\n",
       " 262: 573,\n",
       " 267: 574,\n",
       " 270: 575,\n",
       " 271: 576,\n",
       " 273: 577,\n",
       " 274: 578,\n",
       " 276: 579,\n",
       " 277: 580,\n",
       " 279: 581,\n",
       " 281: 582,\n",
       " 282: 583,\n",
       " 288: 584,\n",
       " 289: 585,\n",
       " 291: 586,\n",
       " 292: 587,\n",
       " 293: 588,\n",
       " 302: 589,\n",
       " 303: 590,\n",
       " 304: 591,\n",
       " 310: 592,\n",
       " 312: 593,\n",
       " 313: 594,\n",
       " 314: 595,\n",
       " 315: 596,\n",
       " 317: 597,\n",
       " 327: 598,\n",
       " 329: 599,\n",
       " 330: 600,\n",
       " 332: 601,\n",
       " 336: 602,\n",
       " 337: 603,\n",
       " 339: 604,\n",
       " 340: 605,\n",
       " 343: 606,\n",
       " 347: 607,\n",
       " 350: 608,\n",
       " 352: 609,\n",
       " 353: 610,\n",
       " 354: 611,\n",
       " 355: 612,\n",
       " 358: 613,\n",
       " 359: 614,\n",
       " 360: 615,\n",
       " 361: 616,\n",
       " 366: 617,\n",
       " 370: 618,\n",
       " 371: 619,\n",
       " 374: 620,\n",
       " 377: 621,\n",
       " 378: 622,\n",
       " 381: 623,\n",
       " 382: 624,\n",
       " 383: 625,\n",
       " 405: 626,\n",
       " 412: 627,\n",
       " 415: 628,\n",
       " 416: 629,\n",
       " 419: 630,\n",
       " 426: 631,\n",
       " 432: 632,\n",
       " 434: 633,\n",
       " 435: 634,\n",
       " 437: 635,\n",
       " 440: 636,\n",
       " 445: 637,\n",
       " 454: 638,\n",
       " 455: 639,\n",
       " 458: 640,\n",
       " 460: 641,\n",
       " 466: 642,\n",
       " 468: 643,\n",
       " 469: 644,\n",
       " 472: 645,\n",
       " 477: 646,\n",
       " 485: 647,\n",
       " 489: 648,\n",
       " 490: 649,\n",
       " 491: 650,\n",
       " 493: 651,\n",
       " 494: 652,\n",
       " 497: 653,\n",
       " 502: 654,\n",
       " 505: 655,\n",
       " 508: 656,\n",
       " 510: 657,\n",
       " 516: 658,\n",
       " 520: 659,\n",
       " 524: 660,\n",
       " 532: 661,\n",
       " 536: 662,\n",
       " 537: 663,\n",
       " 540: 664,\n",
       " 542: 665,\n",
       " 546: 666,\n",
       " 548: 667,\n",
       " 569: 668,\n",
       " 575: 669,\n",
       " 587: 670,\n",
       " 606: 671,\n",
       " 609: 672,\n",
       " 616: 673,\n",
       " 628: 674,\n",
       " 631: 675,\n",
       " 637: 676,\n",
       " 640: 677,\n",
       " 662: 678,\n",
       " 667: 679,\n",
       " 694: 680,\n",
       " 697: 681,\n",
       " 700: 682,\n",
       " 704: 683,\n",
       " 709: 684,\n",
       " 710: 685,\n",
       " 711: 686,\n",
       " 719: 687,\n",
       " 747: 688,\n",
       " 762: 689,\n",
       " 765: 690,\n",
       " 775: 691,\n",
       " 783: 692,\n",
       " 795: 693,\n",
       " 799: 694,\n",
       " 801: 695,\n",
       " 802: 696,\n",
       " 818: 697,\n",
       " 830: 698,\n",
       " 835: 699,\n",
       " 837: 700,\n",
       " 838: 701,\n",
       " 839: 702,\n",
       " 842: 703,\n",
       " 848: 704,\n",
       " 852: 705,\n",
       " 867: 706,\n",
       " 880: 707,\n",
       " 881: 708,\n",
       " 888: 709,\n",
       " 891: 710,\n",
       " 979: 711,\n",
       " 981: 712,\n",
       " 986: 713,\n",
       " 991: 714,\n",
       " 996: 715,\n",
       " 999: 716,\n",
       " 1004: 717,\n",
       " 1006: 718,\n",
       " 1061: 719,\n",
       " 1064: 720,\n",
       " 1082: 721,\n",
       " 750: 722,\n",
       " 924: 723,\n",
       " 1101: 724,\n",
       " 1246: 725,\n",
       " 1584: 726,\n",
       " 1610: 727,\n",
       " 1682: 728,\n",
       " 1784: 729,\n",
       " 1917: 730,\n",
       " 2671: 731,\n",
       " 2688: 732,\n",
       " 2701: 733,\n",
       " 2717: 734,\n",
       " 3114: 735,\n",
       " 3354: 736,\n",
       " 3623: 737,\n",
       " 3869: 738,\n",
       " 3916: 739,\n",
       " 3977: 740,\n",
       " 3994: 741,\n",
       " 4018: 742,\n",
       " 4223: 743,\n",
       " 4306: 744,\n",
       " 4310: 745,\n",
       " 4370: 746,\n",
       " 4643: 747,\n",
       " 4700: 748,\n",
       " 4844: 749,\n",
       " 4874: 750,\n",
       " 4886: 751,\n",
       " 4963: 752,\n",
       " 4993: 753,\n",
       " 4995: 754,\n",
       " 5218: 755,\n",
       " 5349: 756,\n",
       " 5378: 757,\n",
       " 5445: 758,\n",
       " 5459: 759,\n",
       " 5464: 760,\n",
       " 5502: 761,\n",
       " 5618: 762,\n",
       " 5816: 763,\n",
       " 5952: 764,\n",
       " 5989: 765,\n",
       " 5991: 766,\n",
       " 6333: 767,\n",
       " 6365: 768,\n",
       " 6534: 769,\n",
       " 6539: 770,\n",
       " 6863: 771,\n",
       " 6934: 772,\n",
       " 7143: 773,\n",
       " 7153: 774,\n",
       " 7155: 775,\n",
       " 7445: 776,\n",
       " 8207: 777,\n",
       " 8360: 778,\n",
       " 8368: 779,\n",
       " 8373: 780,\n",
       " 8528: 781,\n",
       " 8636: 782,\n",
       " 8665: 783,\n",
       " 8666: 784,\n",
       " 8783: 785,\n",
       " 8808: 786,\n",
       " 8865: 787,\n",
       " 8870: 788,\n",
       " 8907: 789,\n",
       " 8908: 790,\n",
       " 8949: 791,\n",
       " 8957: 792,\n",
       " 8958: 793,\n",
       " 8961: 794,\n",
       " 8965: 795,\n",
       " 8970: 796,\n",
       " 8972: 797,\n",
       " 8984: 798,\n",
       " 27741: 799,\n",
       " 30812: 800,\n",
       " 30816: 801,\n",
       " 31878: 802,\n",
       " 32029: 803,\n",
       " 32031: 804,\n",
       " 32296: 805,\n",
       " 32587: 806,\n",
       " 33162: 807,\n",
       " 33493: 808,\n",
       " 33794: 809,\n",
       " 33836: 810,\n",
       " 34048: 811,\n",
       " 34319: 812,\n",
       " 37741: 813,\n",
       " 38388: 814,\n",
       " 42002: 815,\n",
       " 45499: 816,\n",
       " 45517: 817,\n",
       " 45668: 818,\n",
       " 45730: 819,\n",
       " 46530: 820,\n",
       " 48783: 821,\n",
       " 48997: 822,\n",
       " 49272: 823,\n",
       " 49278: 824,\n",
       " 49286: 825,\n",
       " 49824: 826,\n",
       " 586: 827,\n",
       " 187: 828,\n",
       " 627: 829,\n",
       " 922: 830,\n",
       " 1037: 831,\n",
       " 1095: 832,\n",
       " 1674: 833,\n",
       " 1987: 834,\n",
       " 2011: 835,\n",
       " 2023: 836,\n",
       " 2300: 837,\n",
       " 2877: 838,\n",
       " 2901: 839,\n",
       " 3173: 840,\n",
       " 3328: 841,\n",
       " 3735: 842,\n",
       " 4131: 843,\n",
       " 4558: 844,\n",
       " 5447: 845,\n",
       " 5451: 846,\n",
       " 5481: 847,\n",
       " 5507: 848,\n",
       " 5841: 849,\n",
       " 5843: 850,\n",
       " 5872: 851,\n",
       " 5890: 852,\n",
       " 5891: 853,\n",
       " 5893: 854,\n",
       " 5902: 855,\n",
       " 5956: 856,\n",
       " 5962: 857,\n",
       " 5965: 858,\n",
       " 5988: 859,\n",
       " 6001: 860,\n",
       " 6044: 861,\n",
       " 1028: 862,\n",
       " 1088: 863,\n",
       " 1247: 864,\n",
       " 1307: 865,\n",
       " 3882: 866,\n",
       " 4447: 867,\n",
       " 5066: 868,\n",
       " 5377: 869,\n",
       " 5620: 870,\n",
       " 5943: 871,\n",
       " 5957: 872,\n",
       " 6155: 873,\n",
       " 6266: 874,\n",
       " 6377: 875,\n",
       " 6535: 876,\n",
       " 6942: 877,\n",
       " 7149: 878,\n",
       " 7151: 879,\n",
       " 7154: 880,\n",
       " 7169: 881,\n",
       " 7293: 882,\n",
       " 7375: 883,\n",
       " 7451: 884,\n",
       " 7458: 885,\n",
       " 8529: 886,\n",
       " 8533: 887,\n",
       " 8869: 888,\n",
       " 8969: 889,\n",
       " 30749: 890,\n",
       " 31433: 891,\n",
       " 31685: 892,\n",
       " 33145: 893,\n",
       " 33679: 894,\n",
       " 40629: 895,\n",
       " 40819: 896,\n",
       " 41285: 897,\n",
       " 47099: 898,\n",
       " 51662: 899,\n",
       " 51705: 900,\n",
       " 51834: 901,\n",
       " 54286: 902,\n",
       " 56367: 903,\n",
       " 56949: 904,\n",
       " 58047: 905,\n",
       " 59333: 906,\n",
       " 59421: 907,\n",
       " 60397: 908,\n",
       " 60950: 909,\n",
       " 61250: 910,\n",
       " 63113: 911,\n",
       " 63992: 912,\n",
       " 64969: 913,\n",
       " 66203: 914,\n",
       " 68954: 915,\n",
       " 69406: 916,\n",
       " 69844: 917,\n",
       " 70183: 918,\n",
       " 70293: 919,\n",
       " 71579: 920,\n",
       " 72011: 921,\n",
       " 72330: 922,\n",
       " 72407: 923,\n",
       " 72720: 924,\n",
       " 72737: 925,\n",
       " 72998: 926,\n",
       " 73017: 927,\n",
       " 74450: 928,\n",
       " 77841: 929,\n",
       " 78772: 930,\n",
       " 79091: 931,\n",
       " 80549: 932,\n",
       " 81784: 933,\n",
       " 81845: 934,\n",
       " 81847: 935,\n",
       " 82167: 936,\n",
       " 82499: 937,\n",
       " 84374: 938,\n",
       " 86548: 939,\n",
       " 87222: 940,\n",
       " 88163: 941,\n",
       " 88810: 942,\n",
       " 91104: 943,\n",
       " 92259: 944,\n",
       " 94070: 945,\n",
       " 95167: 946,\n",
       " 95449: 947,\n",
       " 95510: 948,\n",
       " 95543: 949,\n",
       " 96079: 950,\n",
       " 97024: 951,\n",
       " 97938: 952,\n",
       " 98203: 953,\n",
       " 103335: 954,\n",
       " 103339: 955,\n",
       " 104374: 956,\n",
       " 105211: 957,\n",
       " 106489: 958,\n",
       " 106696: 959,\n",
       " 107141: 960,\n",
       " 109374: 961,\n",
       " 109853: 962,\n",
       " 112006: 963,\n",
       " 113275: 964,\n",
       " 113394: 965,\n",
       " 119145: 966,\n",
       " 129428: 967,\n",
       " 136020: 968,\n",
       " 137595: 969,\n",
       " 140110: 970,\n",
       " 44: 971,\n",
       " 376: 972,\n",
       " 511: 973,\n",
       " 529: 974,\n",
       " 1100: 975,\n",
       " 1358: 976,\n",
       " 1370: 977,\n",
       " 1385: 978,\n",
       " 1438: 979,\n",
       " 1518: 980,\n",
       " 1586: 981,\n",
       " 1604: 982,\n",
       " 1608: 983,\n",
       " 1616: 984,\n",
       " 1687: 985,\n",
       " 1693: 986,\n",
       " 1721: 987,\n",
       " 1840: 988,\n",
       " 1882: 989,\n",
       " 1918: 990,\n",
       " 2002: 991,\n",
       " 2027: 992,\n",
       " 1357: 993,\n",
       " 1405: 994,\n",
       " 1876: 995,\n",
       " 2072: 996,\n",
       " 2100: 997,\n",
       " 2421: 998,\n",
       " 2485: 999,\n",
       " ...}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_ids_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                      title   \n",
       "0           1                           Toy Story (1995)  \\\n",
       "1           2                             Jumanji (1995)   \n",
       "2           3                    Grumpier Old Men (1995)   \n",
       "3           4                   Waiting to Exhale (1995)   \n",
       "4           5         Father of the Bride Part II (1995)   \n",
       "...       ...                                        ...   \n",
       "9737   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "9738   193583               No Game No Life: Zero (2017)   \n",
       "9739   193585                               Flint (2017)   \n",
       "9740   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "9741   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                           genres  \n",
       "0     Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                      Adventure|Children|Fantasy  \n",
       "2                                  Comedy|Romance  \n",
       "3                            Comedy|Drama|Romance  \n",
       "4                                          Comedy  \n",
       "...                                           ...  \n",
       "9737              Action|Animation|Comedy|Fantasy  \n",
       "9738                     Animation|Comedy|Fantasy  \n",
       "9739                                        Drama  \n",
       "9740                             Action|Animation  \n",
       "9741                                       Comedy  \n",
       "\n",
       "[9742 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId_x</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>new_movieId</th>\n",
       "      <th>movieId_y</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465</td>\n",
       "      <td>1236</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1439915474</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>5833</td>\n",
       "      <td>Dog Soldiers (2002)</td>\n",
       "      <td>Action|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>473</td>\n",
       "      <td>1236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1132423177</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>5833</td>\n",
       "      <td>Dog Soldiers (2002)</td>\n",
       "      <td>Action|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>488</td>\n",
       "      <td>1236</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1334170640</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>5833</td>\n",
       "      <td>Dog Soldiers (2002)</td>\n",
       "      <td>Action|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>386</td>\n",
       "      <td>1236</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1121743996</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>5833</td>\n",
       "      <td>Dog Soldiers (2002)</td>\n",
       "      <td>Action|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>605</td>\n",
       "      <td>1236</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1171361068</td>\n",
       "      <td>5833.0</td>\n",
       "      <td>5833</td>\n",
       "      <td>Dog Soldiers (2002)</td>\n",
       "      <td>Action|Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47160</th>\n",
       "      <td>124</td>\n",
       "      <td>4799</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1474414744</td>\n",
       "      <td>4946.0</td>\n",
       "      <td>4946</td>\n",
       "      <td>Eye for an Eye, An (1981)</td>\n",
       "      <td>Action|Crime|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47161</th>\n",
       "      <td>293</td>\n",
       "      <td>6483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>966597212</td>\n",
       "      <td>5212.0</td>\n",
       "      <td>5212</td>\n",
       "      <td>Octagon, The (1980)</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47162</th>\n",
       "      <td>317</td>\n",
       "      <td>6731</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1507742593</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>4126</td>\n",
       "      <td>Less Than Zero (1987)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47163</th>\n",
       "      <td>49</td>\n",
       "      <td>2843</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1514240116</td>\n",
       "      <td>405.0</td>\n",
       "      <td>405</td>\n",
       "      <td>Highlander III: The Sorcerer (a.k.a. Highlande...</td>\n",
       "      <td>Action|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47164</th>\n",
       "      <td>413</td>\n",
       "      <td>7649</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1273977735</td>\n",
       "      <td>2253.0</td>\n",
       "      <td>2253</td>\n",
       "      <td>Toys (1992)</td>\n",
       "      <td>Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47165 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId_x  rating   timestamp  new_movieId  movieId_y   \n",
       "0         465       1236     1.5  1439915474       5833.0       5833  \\\n",
       "1         473       1236     4.0  1132423177       5833.0       5833   \n",
       "2         488       1236     2.5  1334170640       5833.0       5833   \n",
       "3         386       1236     3.5  1121743996       5833.0       5833   \n",
       "4         605       1236     4.0  1171361068       5833.0       5833   \n",
       "...       ...        ...     ...         ...          ...        ...   \n",
       "47160     124       4799     3.0  1474414744       4946.0       4946   \n",
       "47161     293       6483     1.0   966597212       5212.0       5212   \n",
       "47162     317       6731     4.0  1507742593       4126.0       4126   \n",
       "47163      49       2843     3.0  1514240116        405.0        405   \n",
       "47164     413       7649     2.5  1273977735       2253.0       2253   \n",
       "\n",
       "                                                   title   \n",
       "0                                    Dog Soldiers (2002)  \\\n",
       "1                                    Dog Soldiers (2002)   \n",
       "2                                    Dog Soldiers (2002)   \n",
       "3                                    Dog Soldiers (2002)   \n",
       "4                                    Dog Soldiers (2002)   \n",
       "...                                                  ...   \n",
       "47160                          Eye for an Eye, An (1981)   \n",
       "47161                                Octagon, The (1980)   \n",
       "47162                              Less Than Zero (1987)   \n",
       "47163  Highlander III: The Sorcerer (a.k.a. Highlande...   \n",
       "47164                                        Toys (1992)   \n",
       "\n",
       "                      genres  \n",
       "0              Action|Horror  \n",
       "1              Action|Horror  \n",
       "2              Action|Horror  \n",
       "3              Action|Horror  \n",
       "4              Action|Horror  \n",
       "...                      ...  \n",
       "47160  Action|Crime|Thriller  \n",
       "47161                 Action  \n",
       "47162                  Drama  \n",
       "47163         Action|Fantasy  \n",
       "47164         Comedy|Fantasy  \n",
       "\n",
       "[47165 rows x 8 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = dataset.copy()\n",
    "df_test['new_movieId'] = df_test['movieId'].map(movie_ids_map)\n",
    "merged_df = pd.merge(df_test, movie_title, left_on='new_movieId', right_on='movieId')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1923076923076925"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df[\"userId\"] == 2][\"rating\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9714</th>\n",
       "      <th>9715</th>\n",
       "      <th>9716</th>\n",
       "      <th>9717</th>\n",
       "      <th>9718</th>\n",
       "      <th>9719</th>\n",
       "      <th>9720</th>\n",
       "      <th>9721</th>\n",
       "      <th>9722</th>\n",
       "      <th>9723</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  0     1     2     3     4     5     6     7     8     9     ...   \n",
       "userId                                                               ...   \n",
       "0         4.0   4.0   4.0   5.0   5.0   3.0   5.0   4.0   5.0   5.0  ...  \\\n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "3         NaN   NaN   NaN   2.0   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "4         4.0   NaN   NaN   NaN   4.0   NaN   NaN   4.0   NaN   NaN  ...   \n",
       "5         NaN   5.0   4.0   4.0   1.0   NaN   NaN   5.0   4.0   NaN  ...   \n",
       "6         4.5   NaN   NaN   NaN   4.5   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "7         NaN   NaN   NaN   4.0   5.0   NaN   NaN   3.0   NaN   NaN  ...   \n",
       "8         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "9         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  ...   \n",
       "\n",
       "movieId  9714  9715  9716  9717  9718  9719  9720  9721  9722  9723  \n",
       "userId                                                               \n",
       "0         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "1         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "2         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "3         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "4         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "5         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "6         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "7         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "8         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "9         NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[10 rows x 9724 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "df = pivoted_dataset[(pivoted_dataset.index >= 0) & (pivoted_dataset.index <= 9)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97235</th>\n",
       "      <td>9</td>\n",
       "      <td>9719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97236</th>\n",
       "      <td>9</td>\n",
       "      <td>9720</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97237</th>\n",
       "      <td>9</td>\n",
       "      <td>9721</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97238</th>\n",
       "      <td>9</td>\n",
       "      <td>9722</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97239</th>\n",
       "      <td>9</td>\n",
       "      <td>9723</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId    0\n",
       "0           0        0  4.0\n",
       "1           0        1  4.0\n",
       "2           0        2  4.0\n",
       "3           0        3  5.0\n",
       "4           0        4  5.0\n",
       "...       ...      ...  ...\n",
       "97235       9     9719  NaN\n",
       "97236       9     9720  NaN\n",
       "97237       9     9721  NaN\n",
       "97238       9     9722  NaN\n",
       "97239       9     9723  NaN\n",
       "\n",
       "[97240 rows x 3 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.stack(dropna=False).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 305., 4180.,  559., ...,   13., 2657.,  555.])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "df = pivoted_dataset[(pivoted_dataset.index >= 0) & (pivoted_dataset.index <= 9)]\n",
    "df = df.stack(dropna=False).reset_index()\n",
    "\n",
    "\n",
    "df = df.rename(columns={0:'rating'})\n",
    "df['new_movieId'] = df['movieId'].map(movie_ids_map)\n",
    "df = df[df['rating'].isna()]\n",
    "df[\"new_movieId\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_for_user(model, user_id, dataset):\n",
    "    \n",
    "    pivoted_dataset = dataset.pivot(index='userId', columns='movieId', values='rating')\n",
    "    \n",
    "    df = pivoted_dataset[(pivoted_dataset.index >= 0) & (pivoted_dataset.index <= 9)]\n",
    "    df = df.stack(dropna=False).reset_index()\n",
    "    \n",
    "    \n",
    "    df = df.rename(columns={0:'rating'})\n",
    "    \n",
    "    df['new_movieId'] = df['movieId'].map(movie_ids_map)\n",
    "    \n",
    "    df = df[df['rating'].isna()]\n",
    "    \n",
    "    \n",
    "    df = df[df[\"userId\"] == user_id]\n",
    "    \n",
    "    X = [df[\"userId\"].to_numpy(), df[\"movieId\"].to_numpy()]\n",
    "    \n",
    "    \n",
    "    y_pred = best_model.predict(X)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "    \n",
    "    # Get the indices of the 5 maximum values in the array\n",
    "    max_indices = np.argsort(y_pred)[-5:][::-1]\n",
    "    \n",
    "    max_rating = y_pred[max_indices]\n",
    "    corresponding_movies = X[1][max_indices]\n",
    "    \n",
    "    movies = movie_title[movie_title[\"movieId\"].isin(corresponding_movies)][\"title\"]\n",
    "   \n",
    "    return max_rating, corresponding_movies\n",
    "    # return five_best_movie_ids, five_best_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 0\n",
      "Max ratings :\n",
      "[5.112232  4.947984  4.926054  4.8938503 4.883753 ]\n",
      "Movies : \n",
      "[ 232 1125 1101  774 1027]\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 1\n",
      "Max ratings :\n",
      "[3.6812172 3.666768  3.6439555 3.640048  3.606445 ]\n",
      "Movies : \n",
      "[166  16  20   4  28]\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 2\n",
      "Max ratings :\n",
      "[2.3554478 2.3108006 2.214007  2.207431  2.1903381]\n",
      "Movies : \n",
      "[478  70 722  81  20]\n",
      "298/298 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 3\n",
      "Max ratings :\n",
      "[4.282547  4.1625557 4.1340566 4.122222  4.0928035]\n",
      "Movies : \n",
      "[2144 1110  762 2394   72]\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 4\n",
      "Max ratings :\n",
      "[3.7732048 3.7654536 3.6293979 3.51016   3.4286346]\n",
      "Movies : \n",
      "[ 34  20  15 166  70]\n",
      "295/295 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 5\n",
      "Max ratings :\n",
      "[4.2449856 4.2045627 4.088467  4.026827  3.992019 ]\n",
      "Movies : \n",
      "[  37    0   70   15 1165]\n",
      "300/300 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 6\n",
      "Max ratings :\n",
      "[4.3807716 4.3119802 4.231864  4.179313  4.1220565]\n",
      "Movies : \n",
      "[232  28 166  70  25]\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 7\n",
      "Max ratings :\n",
      "[3.6478424 3.6158202 3.5551996 3.5328603 3.3935065]\n",
      "Movies : \n",
      "[166   0 478  15 192]\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 8\n",
      "Max ratings :\n",
      "[4.1995015 4.0407124 3.993546  3.9836178 3.97218  ]\n",
      "Movies : \n",
      "[232  28  15   4  16]\n"
     ]
    }
   ],
   "source": [
    "for user in range(9):\n",
    "    max_rating, movies = get_top5_for_user(best_model, user, dataset)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(f\"User id --> {user}\")\n",
    "    print(f\"Max ratings :\")\n",
    "    print(max_rating)\n",
    "    print(f\"Movies : \")\n",
    "    print(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_for_user(model, user_id, dataset,df_movie):\n",
    "\n",
    "    df_unrated=dataset.copy()\n",
    "#     df_unrated = df_unrated.drop(df_unrated[df_unrated['userId'] == user_id].index)\n",
    "#     movies_to_pred = np.unique(df_unrated[\"movieId\"].to_numpy())\n",
    "    rated_movies = set(df[df['userId'] == user_id]['movieId'])\n",
    "    movies_to_pred = set(df_unrated['movieId']) - rated_movies\n",
    "    movies_to_pred = np.array(list(movies_to_pred))\n",
    "    movies_to_pred = np.unique(movies_to_pred)\n",
    "    user_array = np.full_like(movies_to_pred,user_id)\n",
    "    \n",
    "    X = (user_array,movies_to_pred)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    df2 = pd.DataFrame({'rating': y_pred.tolist(), 'movieId': movies_to_pred.tolist()})\n",
    "   # merged_df = pd.merge(df2, df_movie, on='movieId')\n",
    "    sorted_df = df2.sort_values(by='rating', ascending=False)\n",
    "    five_best_movie_ids = sorted_df.head(5).loc[:, 'movieId'].tolist()\n",
    "    selected_rows = movie_title.loc[movie_title['movieId'].isin(five_best_movie_ids)]\n",
    "    five_best_ratings= sorted_df.head(5)['rating']\n",
    "    titles = selected_rows['title']\n",
    "    \n",
    "    return titles,five_best_movie_ids, five_best_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 0\n",
      "Max ratings :\n",
      "15    [4.979307651519775]\n",
      "28    [4.958395957946777]\n",
      "67    [4.931294918060303]\n",
      "68    [4.925956726074219]\n",
      "69    [4.899869441986084]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[15, 28, 67, 68, 69]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 1\n",
      "Max ratings :\n",
      "2     [3.8510780334472656]\n",
      "8     [3.5270118713378906]\n",
      "14    [3.3680009841918945]\n",
      "7      [3.347370147705078]\n",
      "1      [3.342815399169922]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[232, 238, 244, 237, 219]\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 2\n",
      "Max ratings :\n",
      "6      [2.175940752029419]\n",
      "20     [2.099778652191162]\n",
      "1      [2.030557632446289]\n",
      "11    [1.9964357614517212]\n",
      "18      [1.85059654712677]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[224, 272, 86, 263, 270]\n",
      "7/7 [==============================] - 0s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 3\n",
      "Max ratings :\n",
      "91     [4.448208808898926]\n",
      "2       [4.29256010055542]\n",
      "107    [4.254478454589844]\n",
      "15     [4.205239772796631]\n",
      "89       [4.2005934715271]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[335, 15, 351, 67, 333]\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 4\n",
      "Max ratings :\n",
      "13    [4.0436224937438965]\n",
      "8      [3.901742696762085]\n",
      "1     [3.7336153984069824]\n",
      "3      [3.695828437805176]\n",
      "2      [3.608024835586548]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[232, 28, 4, 16, 7]\n",
      "10/10 [==============================] - 0s 5ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 5\n",
      "Max ratings :\n",
      "13    [4.607840538024902]\n",
      "16    [4.388584136962891]\n",
      "33    [4.299388885498047]\n",
      "64    [4.253209114074707]\n",
      "58    [4.243347644805908]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[20, 25, 232, 471, 463]\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 6\n",
      "Max ratings :\n",
      "6     [4.375675678253174]\n",
      "2     [4.330160140991211]\n",
      "1     [4.224390506744385]\n",
      "3     [4.160630226135254]\n",
      "78    [4.156128406524658]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[34, 15, 4, 20, 753]\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 7\n",
      "Max ratings :\n",
      "15     [4.126692771911621]\n",
      "6     [3.9027609825134277]\n",
      "14    [3.9021220207214355]\n",
      "11     [3.899874448776245]\n",
      "2       [3.85640025138855]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[232, 20, 34, 28, 7]\n",
      "2/2 [==============================] - 0s 6ms/step\n",
      "------------------------------------------------------------------------\n",
      "User id --> 8\n",
      "Max ratings :\n",
      "2      [3.879760265350342]\n",
      "7     [3.8793206214904785]\n",
      "11    [3.8245997428894043]\n",
      "3      [3.625420093536377]\n",
      "0     [3.3847360610961914]\n",
      "Name: rating, dtype: object\n",
      "Movies : \n",
      "[70, 753, 764, 85, 12]\n"
     ]
    }
   ],
   "source": [
    "for user in range(9):\n",
    "    titles,five_best_movie_ids, five_best_ratings = get_top5_for_user(best_model, user, dataset, movie_title)\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(f\"User id --> {user}\")\n",
    "    print(f\"Max ratings :\")\n",
    "    print(five_best_ratings)\n",
    "    print(f\"Movies : \")\n",
    "    print(five_best_movie_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look at what is going on in the embedding space of the movies that we learnt. Our brain cannot picture anything beyond 3 dimensions, and we learnt high dimensional embeddings (k=15 or 30), so we are going to project the movies embeddings on a 2D plane, first with PCA, and then with another algorithm made for visualizing high dimensional spaces called t-sne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already studied PCA, it is a useful technique for dimensionality reduction, but also simply for visualization. Don't forget to scale your embeddings first. To access the embeddings values of your keras model, have a look at the *get_weights()* function.\n",
    "\n",
    "Compute a PCA on all your movies embeddings, get the 2 first principal components, and do a scatter plot of all the movies on a 2D plane, where each movie is a point defined by the two values of the two first principal components of the PCA from its embedding. Add the titles of the movies to each point of the plot (use plotly to do so it will be clearer), and try to see if you can interpret the axes of the PCA through to different movie genres, like in Figure 3 from the article *Matrix Factorization Techniques for Recommender Systems*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#TOFILL\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/decomposition/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_incremental_pca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncrementalPCA\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kernel_pca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KernelPCA\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparse_pca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparsePCA, MiniBatchSparsePCA\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_truncated_svd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruncatedSVD\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fastica\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastICA, fastica\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/decomposition/_sparse_pca.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hidden, Interval, StrOptions\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_is_fitted\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ridge_regression\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dict_learning, MiniBatchDictionaryLearning\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/linear_model/__init__.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sgd_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hinge, Log, ModifiedHuber, SquaredLoss, Huber\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stochastic_gradient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGDClassifier, SGDRegressor, SGDOneClassSVM\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ridge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV, ridge_regression\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_logistic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, LogisticRegressionCV\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_omp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     orthogonal_mp,\n\u001b[1;32m     40\u001b[0m     orthogonal_mp_gram,\n\u001b[1;32m     41\u001b[0m     OrthogonalMatchingPursuit,\n\u001b[1;32m     42\u001b[0m     OrthogonalMatchingPursuitCV,\n\u001b[1;32m     43\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearClassifierMixin, LinearModel\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _preprocess_data, _rescale_data\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sag_solver\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputMixin, RegressorMixin, is_classifier\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_sparse_dot\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dataset\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sag_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sag32, sag64\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvergenceWarning\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:404\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with t-sne, an algorithm specialized for visualizing high dimensional spaces, you can read more about it there : https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#TOFILL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-sne in general tends to preserve local similarities better than PCA. In any case, it's always interesting to try both for visualizing high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can export your embedding and upload them on https://projector.tensorflow.org/ to visualize the embeddings in 3D. You can also use the movies genres from the *movies.csv* file to make one plot for each movie genre and try to see if some parts of the embedding space are representative of a movie genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL PARTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend movies to yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that ask you to rate 20 movies, then add your own ratings to the dataset, retrain the model, and compute your own top-5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rate_my_movies(my_user_id, dataset, nb_movies, nb_to_rate, movie_ids_map):\n",
    "    \"\"\"\n",
    "    Returns a dataframe in the same format as the dataset dataframe, with\n",
    "    ratings entered by the user for `nb_to_rate` random movies\n",
    "    \n",
    "    Input :\n",
    "        my_user_id : int : The user_id of the new ratings\n",
    "        dataset : DataFrame : The whole dataset \n",
    "        nb_movies : int : Number of unique movie ids\n",
    "        nb_to_rate : int : Number of movies to rate\n",
    "        movie_ids_map : dict : The mapping of original file userId to a new index starting at 0.\n",
    "    \n",
    "    Output : \n",
    "        my_ratings : DataFrame : A dataframe with the same column as `dataset` containing\n",
    "            the new ratings entered by the user\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    return my_ratings\n",
    "\n",
    "\n",
    "my_user_id = len(user_ids_map)\n",
    "\n",
    "my_ratings = rate_my_movies(my_user_id, dataset, nb_movies, 20, movie_ids_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_me = pd.concat([dataset, my_ratings], axis = 0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_with_me = [dataset_with_me[\"userId\"].to_numpy(), dataset_with_me[\"movieId\"].to_numpy()]\n",
    "y_with_me = dataset_with_me[\"rating\"].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model=get_mf_bias_l2_reg_model(nb_users + 1, nb_movies, k = best_params['k'], lambda_ = best_params['lambda_'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_mse', patience=10, verbose=1, restore_best_weights=True)\n",
    "\n",
    "best_model.fit(X_with_me, y_with_me, epochs=500, batch_size=512, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_best_movie_ids, five_best_ratings =  get_top5_for_user(best_model, my_user_id, dataset)\n",
    "for i in range(5):\n",
    "    print('\\t' + ml_movie_id_to_title[ inverse_movie_ids_map[ five_best_movie_ids[i] ] ] + \n",
    "          '; predicted rating : ' + str(five_best_ratings[i]) )\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the movie embeddings to predict the movies genre with multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the goal of predicting missing rating, the matrix factorization techniques also produces vectorial representation of movies and users: their embeddings, what we just visualized for the movies. With a big enough dataset, these embeddings actually are good abstract representations of the movies and of the users, and can be reused as features for other tasks, such as classification.\n",
    "\n",
    "In the *movies.csv*, there is a column that gives the genres of each movie. Let's try to predict the genres of the movies from the embeddings we learnt. As you can see, each movie can have more than one genre, so in classification terms, more than one class. We can achieve that with *multilabel classification*. You can read more about it there: https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "Load the movies genre, encode them as binary classes and use the classes imported below to train a multilabel classifier that uses the movie embeddings as features, and the movie genres as classes. Use the *OneVsRestClassifier* with a simple *LinearSVC* without any hyper-parameter tuning. Finally print the test accuracy, F1, precision and recall for each class, as well as the number of time each class appears in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rare classes, you should get a very high accuracy, with a very low F1. Indeed these classes are really imbalanced : there are a few positives, hence the classifier is largely biased toward the negatives, and rarely predict a positive for these classes. This is why accuracy is generally a bad measure with imbalanced dataset : the high number of true negatives makes the accuracy number high, while our model is actually barely capable of predicting true positives.\n",
    "\n",
    "Let's compare our classifier performance with a *DummyClassifier*, the dummy classifier takes the ratio $r = \\frac{nb\\_positives}{nb\\_positives + nb\\_negatives}$ as the probability to predict a positive, and then do it randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, simply respecting the class balance, even at random, produces better F1 on most classes. One way to compensate for class imbalance is to tell the classifier to weight more the true samples at training time, accordingly with the ratio $r$ between true and false samples. With scikit-learn SVM implementation, you can use the argument *class_weight* for setting the weight of the positive and negative samples at training time. See : https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html\n",
    "\n",
    "But if you just want to set the class weights accordingly with the ratio between positives and negatives, you can just set *class_weight = ‘balanced’*. Test it with the LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 is now much better than with the dummy classifier, however is is still not very convincing. This is quite normal given the size of the dataset we are using, which is pretty small to get really meaningful embeddings. But with bigger datasets, reusing embeddings as features for auxiliary tasks such as classification is actually a very effective way of doing so when there is no other informations about the items we try to classify. Here the items are the movies, the dataset doesn't provide more information about them, but one could imagine fetching from internet textual descriptions of the movies and use them as features alongside the embeddings to improve the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the different SGD algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the notebook we used the 'adam' `optimizer` to train our model, which is a variation of SGD. Keras proposes different variations of SGD: https://keras.io/optimizers/ . This article gif images gives an intuitive view of their different behavior : https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39\n",
    "\n",
    "Try a few ones with our model and see how the training and testing loss evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the global bias $\\mu$  parameter to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we didn't added the global bias $\\mu$ to our model yet (Equations (4-5) from Koren's paper). Use your best google skills to find a way to add an embedding layer that does that.\n",
    "\n",
    "Hint : Use a constant `Input` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own Stochastic Gradient Descent for Matrix Factorization with numpy instead of Keras (very optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know everything to implement your own matrix factorization SGD model, all with numpy arrays. Start without the biases again, and without mini-batches. The gradient update equations are described in page 4 of Koren's paper. Let's initialize your $p$ and $q$ embeddings with a gaussian sampling. Print the RMSE at the beginning of each epoch, and finally compute the RMSE of your model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal\n",
    "\n",
    "P = normal(size = (nb_users,k))\n",
    "Q = normal(size = (nb_movies,k))\n",
    "\n",
    "gamma = 0.1\n",
    "lambda_ = 0.00001\n",
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for j in range(train.shape[0]):\n",
    "        u = train['userId'].iloc[j]\n",
    "        i = train['movieId'].iloc[j]\n",
    "        r_ui = train['rating'].iloc[j]\n",
    "        \n",
    "        #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
