1) What are the necessary preprocessing steps regarding:
a) classes ?
--> Need to check if the target value are rougly equaly represented to advoid a bias

b) categorical features ? 
--> Need to create a column for each value of the categorical using pd.get_dummies()

c) continuous features ?
--> Standardization

2) Confusion matrix:
a)How many patient were incorrectly diagnosed with a Heart disease (false positives) ?
--> 5

b)How many patient were incorrectly diagnosed as being Healthy (false negatives)?
--> 12

3) Changing the threshold:
a)What is the precision if we change the threshold to have a 0.95 recall ? 

--> threshold = 0.06

--> Accuracy: 0.5164835164835165
--> recall: 0.9512195121951219
--> precision: 0.48148148148148145
--> f1_score: 0.6393442622950819
b) How many patient were incorrectly diagnosed as being Healthy (false negatives)?
--> 33 => For the same reason we miss a lot Positive because we are to strict


4) Choosing an overall metric:

a) If I can compute my test sample probabilities and care more about the positive class, which overall metric should I use to compare classifiers ? 
--> AUC-ROC is a widely used performance metric that takes into account both :
--> true positive rate (TPR) 
--> false positive rate (FPR)

across different probability thresholds.

b) And if I only have the class predictions and no probabilities ?
accuracy, precision, recall, and F1-score to compare classifiers