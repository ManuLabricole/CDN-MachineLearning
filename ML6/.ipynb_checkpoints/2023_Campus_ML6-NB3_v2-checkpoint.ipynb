{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with a Prophet ðŸ“‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://youtu.be/pOYAXv15r3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One piece of recent open software is facebook's `prophet`. This uses a model which is similar to a **generalised additive model** (GAM), a class of additive (linear) models with potentially non-linear components. \n",
    "\n",
    "The model is easily interpreted (as a sum of components) and simple to fit (parameters have a strightforward interpretation).\n",
    "But if the hypothesis of the parametric model are not respected, the fitted model may seriously underfit.\n",
    "\n",
    "You can read more about the underlying model [here](https://arxiv.org/abs/2111.15397).\n",
    "\n",
    "As you are beginning to see time series forecasting can sometimes be as much an art as a science, and we know it takes quite some practice to become a good artist. The authors of the software had this in mind when they wrote it.  One of their aims is to provide software which can be used easily by those who have a little timeseries knowledge. Let's see how this goes....We will apply this software to the airline data now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install prophet I advise you to install a completely clean virtualenvironment with conda (you will find the instructions to do this.\n",
    "\n",
    "Then you can install prophet with: `pip install neuralprophet`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.style as style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1 - airline data\n",
    "\n",
    "We setup the data as we had done so before in the ARIMA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.datasets import load_airpassengers\n",
    "\n",
    "START_DATE = '1949-01-01'\n",
    "airline = load_airpassengers(as_series=True)\n",
    "\n",
    "#there's no datetimeindex from the bundled dataset. So let's add one.\n",
    "airline.index= pd.date_range(START_DATE, \n",
    "                             periods=len(airline), \n",
    "                             freq='MS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cv need all of the training data joined together.\n",
    "airline_adj = airline / airline.index.days_in_month\n",
    "\n",
    "train, test = ts_train_test_split(airline_adj, '1960-01-01')\n",
    "train_log, test_log = np.log(train), np.log(test) # get logged values for arima model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with prophet\n",
    "\n",
    "Fitting a basic model and making predictions is very simple with `prophet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralprophet import NeuralProphet\n",
    "pd.plotting.register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is expected in this df format below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train).reset_index().rename(columns = {0:'y','index':'ds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet()\n",
    "\n",
    "metrics = model.fit(train_df) #you can play with freq parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = model.make_future_dataframe(periods=12, \n",
    "                                     freq='MS', \n",
    "                                     include_history=True) \n",
    "#create a dataframe with the prediction datetimes we want\n",
    "future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(future)\n",
    "y_pred = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].set_index('ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = model.plot(forecast)\n",
    "# test.plot(style='.r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating this baseline model error on test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    '''\n",
    "    MAPE\n",
    "\n",
    "    Parameters:\n",
    "    --------\n",
    "    y_true -- np.array actual observations from time series\n",
    "    y_pred -- the predictions to evaluate\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float, scalar value representing the MAPE (0-100)\n",
    "    '''\n",
    "    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(test.values , \n",
    "                               forecast['yhat'][-12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We at least see it beats the naive model scores  that we saw in the last notebook (from the unlogged data). Not bad for only several lines of code. But looking at the plot it is clearly not fitting well. We should use cross validation to find a better fitting model (before we evaluated in the test set!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation in prophet\n",
    "\n",
    "There are cross validation functions in `prophet` that take care of time series data and can perform adjustable rolling windows as we had previously seen in other time series cross validation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follwo the guide: https://neuralprophet.com/how-to-guides/feature-guides/test_and_crossvalidate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "365.25*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralProphet()\n",
    "model.fit(train_df)\n",
    "df_cv = cross_validation(model, \n",
    "                         initial='1461 days', \n",
    "                         period='365 days', \n",
    "                         horizon = '365 days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for performance metrics and plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = performance_metrics(df_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_cross_validation_metric(df_cv, metric='mape')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is the standard output plot of the CV from `prophet`. It is different to what we have looked at before. Each grey point represents a prediction, made on a particular month. Because we made predictions over several horizons each are plotted on the graph. At around horizon 30 for example we see each of the plots over the first month of each of the 6, 12 month validations. The blue line shows a rolling window of the mape scores 'averaged' over the cv predictions. More info [here](https://facebook.github.io/prophet/docs/diagnostics.html).\n",
    "\n",
    "We are normally interested in the mean and std of the MAPE scores found at each window. This can be calculated from the dataframe returned from the `cross_validation` function above, using the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_performance_calc(df_cv):\n",
    "    \" function to replace prophet cv 'performance metrics' function\"\n",
    "    df_cv['mape'] = abs(df_cv['y'] - df_cv['yhat']) /df_cv['y'] # make column with individual abs normalised values\n",
    "    results = df_cv.groupby(['cutoff']).mean() # finish mape calc for each of the folds (distinguised in 'cutoff' column of df_cv)\n",
    "    results = results.describe() # sumarise folds information\n",
    "    results = results['mape']# # return only mape column\n",
    "    return(results)\n",
    "\n",
    "mape_performance_calc(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a MAPE of ~7.2% with std ~2%. This would have estimated the error we saw on the test set to a good degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters\n",
    "\n",
    "The model plot we saw earlier did not fit very well to the train data. We can change the model assumptions (like hyper-parameters) to change how well it is fitting the train data.\n",
    "\n",
    "`Prophet(\n",
    "    model = NeuralProphet(growth='linear',\n",
    "    n_forecasts=60,\n",
    "    n_lags=60,\n",
    "    n_changepoints=50,\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    learning_rate=1.0,\n",
    "    impute_missing= \"true\",\n",
    "    normalize=\"auto\",\n",
    "    )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**\n",
    "\n",
    "* Try changing some of these. [This](https://towardsdatascience.com/implementing-facebook-prophet-efficiently-c241305405a3) blog is very useful for the fbprophet version explaining the parameters and what they do. [This one](https://neuralprophet.com/how-to-guides/feature-guides/hyperparameter-selection.html) may be usefull for neuralProphet.\n",
    "* Find at least one change which improves your model cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your solution here \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - energy data forecasting âš¡\n",
    "\n",
    "Forecasting energy demand is big business and very important in helping energy producers maintain the balance within the electrical grid. You can find forecasts of energy demands for France in realtime [here](https://www.rte-france.com/fr/eco2mix/eco2mix-consommation). The data was already downloaded in the `data/nats.csv` file.\n",
    "\n",
    "Your mission now is to imagine a useful use-case for an enterprise and use `Prophet` to make forecasts on the energy data you have already seen (in data viz module) for France! Feel free to choose :\n",
    "* the type of energy generation/consommation\n",
    "* france or regional,\n",
    "* frequency scale you wish to try!\n",
    "\n",
    "Try and consider a forecast horizon that seems useful for the frequency at which you are making your predictions e.g. predicting the hourly forecast for 12 months propbably cannot be used in any meaningful way - and is probably not going be very easy to forecast accurately.\n",
    "\n",
    "Consider that:\n",
    "* Reducing the scale of the data, i.e. to regional\n",
    "* Higher frequency predictions\n",
    "* longer periods of forecast\n",
    "\n",
    "...are likely to make it more difficult to make a good forecast.\n",
    "\n",
    "Consider also that you must pick the most suitable cross validation procedure for your problem. Consider the initial, step and horizon you will use carefully.\n",
    "\n",
    "Once you have made your model:\n",
    "1. Compare it to a naive model!! If it does not beat this then try reducing your forecast horizon to find at what scale you can make useful predictions.\n",
    "2. Try tuning the model in some way to improve performance (add holidays might be a good idea)\n",
    "3. If you have time compare your model to an ARIMA model. Why might there be differences in perfomance between the two approachecs to forecasting?\n",
    "\n",
    "(feel free to use a new notebook for this task!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66297518c761de5048a5a5e335fa1ec12dc7fe7bda3903e2a9c6a9b5d44c0277"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
